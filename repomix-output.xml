This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.env.example
.github/copilot-instructions.md
.github/workflows/secret-scan.yml
.gitignore
.pre-commit-config.yaml
.pre-commit-hooks/gitleaks-wrapper.py
.pre-commit-hooks/run-gitleaks.sh
companies/company_a.json
companies/company_b.json
companies/company.example.json
company_config.py
dataPandasLib/testEnv/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_frame_eval/pydevd_frame_evaluator.pyx
epos_playwright.py
load_env.py
qbo_upload.py
query_qbo_for_company.py
README.md
requirements-dev.txt
requirements.txt
run_all_companies.py
run_pipeline.py
run_test_upload.py
scripts/qbo_check_inventory_start_dates.py
scripts/qbo_debug_account_query.py
scripts/qbo_debug_item_by_id.py
scripts/qbo_debug_item_by_name.py
scripts/qbo_verify_mapping_accounts.py
slack_notify.py
store_tokens.py
token_manager.py
transform.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="run_test_upload.py">
"""
Test upload: run the same pipeline as run_pipeline (single-day) but skip Phase 1 (EPOS download).

Uses a specific raw CSV you provide instead of downloading from EPOS. Runs:
  Phase 2: Transform (raw CSV -> QuickBooks-ready CSV)
  Phase 3: QBO upload (create SalesReceipts; creates inventory items when enabled)
  Phase 4: Reconciliation (EPOS vs QBO totals)
  Phase 5: Archive — only if you pass --archive (default: skip so raw file stays for repeated testing)

Use this to test inventory item creation, 6270/blockers logic, and reconciliation
without running Playwright/EPOS download. Product names from the CSV are used
as QBO line items (or new inventory items are created when inventory mode is enabled).

Usage:
    python run_test_upload.py
    python run_test_upload.py --csv BookKeeping_2026_01_29_1911.csv --company company_a --target-date 2026-01-28
    python run_test_upload.py --archive   # move raw/processed/metadata to Uploaded/<date>/
"""

import argparse
import logging
import os
import subprocess
import sys
from pathlib import Path

from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from run_pipeline import reconcile_company, archive_files

# Load .env so reconciliation (QBO query) and in-process steps have env vars
load_env_file()

# Defaults match the sample CSV (BookKeeping_2026_01_29_1911.csv with date 28/01/2026)
DEFAULT_CSV = "BookKeeping_2026_01_29_1911.csv"
DEFAULT_COMPANY = "company_a"
DEFAULT_TARGET_DATE = "2026-01-28"


def run_step(cmd: list, cwd: str, label: str) -> None:
    """Run a command; log output and exit on failure."""
    logging.info(f"\n=== {label} ===")
    logging.info(f"Running: {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)
    if result.stdout:
        for line in result.stdout.splitlines():
            logging.info(f"  {line}")
    if result.stderr:
        for line in result.stderr.splitlines():
            logging.warning(f"  {line}")
    if result.returncode != 0:
        logging.error(f"[ERROR] {label} failed with exit code {result.returncode}")
        if result.stdout:
            logging.error(result.stdout)
        if result.stderr:
            logging.error(result.stderr)
        sys.exit(result.returncode)
    logging.info(f"[OK] {label} completed successfully.")


def main():
    # Prefer UTF-8 on Windows to avoid UnicodeEncodeError (e.g. Naira symbol)
    os.environ.setdefault("PYTHONUTF8", "1")

    parser = argparse.ArgumentParser(
        description="Run pipeline from transform through archive using a specific raw CSV (no EPOS download)."
    )
    parser.add_argument(
        "--csv",
        default=DEFAULT_CSV,
        help=f"Path to raw EPOS CSV (relative to repo root or absolute). Default: {DEFAULT_CSV}",
    )
    parser.add_argument(
        "--company",
        default=DEFAULT_COMPANY,
        choices=get_available_companies(),
        help=f"Company key. Default: {DEFAULT_COMPANY}",
    )
    parser.add_argument(
        "--target-date",
        default=DEFAULT_TARGET_DATE,
        help=f"Target business date YYYY-MM-DD (must match CSV rows). Default: {DEFAULT_TARGET_DATE}",
    )
    parser.add_argument(
        "--archive",
        action="store_true",
        help="Move raw/processed/metadata files to Uploaded/<date>/ (default: skip so you can keep testing).",
    )
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO, format="%(message)s")
    # Optional: force UTF-8 on root handler to avoid Windows cp1252 encoding errors
    try:
        import io
        if hasattr(sys.stdout, "buffer"):
            utf8_stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
            for h in logging.root.handlers[:]:
                if isinstance(h, logging.StreamHandler) and getattr(h, "stream", None) is sys.stdout:
                    h.setStream(utf8_stdout)
                    break
    except Exception:
        pass  # Leave default; NGN instead of Naira symbol avoids main crash
    repo_root = Path(__file__).resolve().parent

    # Resolve CSV path
    csv_path = Path(args.csv)
    if not csv_path.is_absolute():
        csv_path = repo_root / csv_path
    if not csv_path.exists():
        logging.error(f"[ERROR] CSV not found: {csv_path}")
        sys.exit(1)
    if not csv_path.is_file():
        logging.error(f"[ERROR] CSV path is not a file: {csv_path}")
        sys.exit(1)

    archive_mode = "on" if args.archive else "off"
    logging.info("")
    logging.info("=== Test upload (no EPOS download) ===")
    logging.info(f"  company:      {args.company}")
    logging.info(f"  target_date:  {args.target_date}")
    logging.info(f"  raw_csv_path: {csv_path}")
    logging.info(f"  archive:      {archive_mode}" + ("" if args.archive else " (use --archive to move files to Uploaded/<date>/)"))
    logging.info("")

    # Phase 2: Transform
    transform_cmd = [
        sys.executable,
        str(repo_root / "transform.py"),
        "--company",
        args.company,
        "--target-date",
        args.target_date,
        "--raw-file",
        str(csv_path),
    ]
    run_step(transform_cmd, str(repo_root), "Phase 2: Transform to single CSV (transform)")

    # Phase 3: QBO upload (same as pipeline: uses latest single_sales_receipts_*.csv)
    upload_cmd = [
        sys.executable,
        str(repo_root / "qbo_upload.py"),
        "--company",
        args.company,
        "--target-date",
        args.target_date,
    ]
    run_step(upload_cmd, str(repo_root), "Phase 3: Upload to QBO (qbo_upload)")

    # Phase 4: Reconciliation (EPOS vs QBO totals)
    logging.info("\n=== Phase 4: Reconciliation ===")
    config = load_company_config(args.company)
    reconcile_result = None
    try:
        reconcile_result = reconcile_company(args.company, args.target_date, config, repo_root)
        status = reconcile_result.get("status", "NOT RUN")
        if status == "MATCH":
            epos_total = reconcile_result.get("epos_total", 0)
            qbo_total = reconcile_result.get("qbo_total", 0)
            logging.info(f"[OK] Reconciliation: MATCH (EPOS: NGN {epos_total:,.2f}, QBO: NGN {qbo_total:,.2f})")
        elif status == "MISMATCH":
            diff = reconcile_result.get("difference", 0)
            logging.warning(f"[WARN] Reconciliation: MISMATCH (Difference: NGN {diff:,.2f})")
        else:
            reason = reconcile_result.get("reason", "unknown")
            logging.warning(f"[WARN] Reconciliation: NOT RUN ({reason})")
    except Exception as e:
        logging.error(f"[ERROR] Phase 4: Reconciliation failed: {e}")
        logging.warning("Continuing (upload was successful)")

    # Phase 5: Archive — skip by default so raw file stays in repo root for repeated testing
    if args.archive:
        logging.info("\n=== Phase 5: Archive Files ===")
        try:
            archive_files(repo_root, config)
        except Exception as e:
            logging.error(f"[ERROR] Phase 5: Archive failed: {e}")
            logging.warning("Continuing (upload was successful)")
    else:
        logging.info("\n=== Phase 5: Archive (skipped; use --archive to move files to Uploaded/<date>/) ===")

    logging.info("\nTest upload completed successfully.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/qbo_check_inventory_start_dates.py">
#!/usr/bin/env python3
"""
Standalone diagnostic: list QBO Inventory items whose InvStartDate is AFTER a cutoff date.
Uses the same auth/token and request utilities as qbo_upload.py (no OAuth reimplementation).

Example:
  python scripts/qbo_check_inventory_start_dates.py --company company_a --cutoff-date 2026-01-28
  python scripts/qbo_check_inventory_start_dates.py --company company_a --cutoff-date 2026-01-28 --export-csv reports/inv_start_date_issues_company_a_2026-01-28.csv
  python scripts/qbo_check_inventory_start_dates.py --company company_a --cutoff-date 2026-01-28 --no-include-inactive --maxresults 500
  python scripts/qbo_check_inventory_start_dates.py --company company_a --cutoff-date 2026-01-28 --first-page-only   # only first page (no pagination)
By default, the script paginates (startposition/maxresults) to fetch all Inventory items, not just the first 1000.
"""
from __future__ import annotations

import argparse
import csv
import sys
from pathlib import Path
from urllib.parse import quote

# Run from repo root (parent of scripts/); add repo root to path for imports
_REPO_ROOT = Path(__file__).resolve().parent.parent
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from token_manager import verify_realm_match
from qbo_upload import BASE_URL, _make_qbo_request, TokenManager

load_env_file()


def parse_cutoff_date(s: str) -> str:
    """Validate and return YYYY-MM-DD string."""
    if len(s) != 10 or s[4] != "-" or s[7] != "-":
        raise ValueError(f"cutoff-date must be YYYY-MM-DD, got {s!r}")
    y, m, d = s[:4], s[5:7], s[8:10]
    if not (y.isdigit() and m.isdigit() and d.isdigit()):
        raise ValueError(f"cutoff-date must be YYYY-MM-DD, got {s!r}")
    return s


def query_inventory_items(
    token_mgr: TokenManager,
    realm_id: str,
    include_inactive: bool,
    maxresults: int,
    fetch_all: bool = True,
) -> list[dict]:
    """
    Query QBO for Inventory items. Read-only.
    When fetch_all is True (default), paginate with startposition/maxresults until all pages are fetched.
    Returns list of raw item dicts (may have missing InvStartDate).
    """
    select_clause = "select Id, Name, Type, TrackQtyOnHand, InvStartDate, Active from Item"
    where = "where Type = 'Inventory'"
    if not include_inactive:
        where += " and Active = true"
    all_items: list[dict] = []
    startposition = 1
    page_size = maxresults

    while True:
        # QBO uses 1-based startposition; maxresults is page size
        query = f"{select_clause} {where} startposition {startposition} maxresults {page_size}"
        url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
        resp = _make_qbo_request("GET", url, token_mgr)
        if resp.status_code != 200:
            try:
                body = resp.json()
                fault = body.get("Fault") or body.get("fault")
                if fault:
                    errors = fault.get("Error") or fault.get("error") or []
                    msgs = [e.get("Message") or e.get("message") or str(e) for e in errors]
                    raise RuntimeError(f"QBO query failed: {'; '.join(msgs)}")
            except ValueError:
                pass
            raise RuntimeError(f"QBO query failed: HTTP {resp.status_code} - {resp.text[:500]}")
        data = resp.json()
        items = data.get("QueryResponse", {}).get("Item", [])
        if not isinstance(items, list):
            items = [items] if items else []
        all_items.extend(items)
        # If we got fewer than page_size, we've reached the end
        if not fetch_all or len(items) < page_size:
            break
        startposition += page_size
    return all_items


def filter_issues(items: list[dict], cutoff_date: str) -> list[dict]:
    """Return items where InvStartDate exists and InvStartDate > cutoff_date, sorted by InvStartDate ascending."""
    issues = []
    for item in items:
        inv_start = item.get("InvStartDate")
        if inv_start is None or inv_start == "":
            continue
        inv_date_str = inv_start[:10] if len(str(inv_start)) >= 10 else str(inv_start)
        try:
            if inv_date_str > cutoff_date:
                issues.append({
                    "Id": item.get("Id", ""),
                    "Name": item.get("Name", ""),
                    "Type": item.get("Type", ""),
                    "TrackQtyOnHand": item.get("TrackQtyOnHand", ""),
                    "InvStartDate": inv_date_str,
                    "Active": item.get("Active", ""),
                })
        except (TypeError, ValueError):
            continue
    issues.sort(key=lambda x: x.get("InvStartDate", ""))
    return issues


def count_with_inv_start_date(items: list[dict]) -> int:
    """Count items that have a non-empty InvStartDate."""
    return sum(1 for it in items if it.get("InvStartDate") not in (None, ""))


def items_missing_inv_start_date(items: list[dict]) -> list[dict]:
    """Return items that have no InvStartDate (for debugging / raw keys)."""
    return [it for it in items if it.get("InvStartDate") in (None, "")]


def main() -> int:
    parser = argparse.ArgumentParser(
        description="List QBO Inventory items whose InvStartDate is after a cutoff date (diagnostic for QBO 6270).",
    )
    parser.add_argument(
        "--company",
        required=True,
        choices=get_available_companies(),
        help="Company identifier",
    )
    parser.add_argument(
        "--cutoff-date",
        required=True,
        metavar="YYYY-MM-DD",
        help="Cutoff date (YYYY-MM-DD). Items with InvStartDate > this are reported.",
    )
    parser.add_argument(
        "--no-include-inactive",
        action="store_true",
        help="Exclude inactive items from the query (default: include both active and inactive)",
    )
    parser.add_argument(
        "--maxresults",
        type=int,
        default=1000,
        metavar="N",
        help="Max items to return from QBO query (default: 1000)",
    )
    parser.add_argument(
        "--export-csv",
        metavar="PATH",
        default=None,
        help="Optional path to write CSV of issues (columns: Id, Name, Type, TrackQtyOnHand, InvStartDate, Active)",
    )
    parser.add_argument(
        "--first-page-only",
        action="store_true",
        help="Only fetch the first page of results (no pagination); default is to fetch all pages.",
    )
    args = parser.parse_args()

    try:
        cutoff_date = parse_cutoff_date(args.cutoff_date)
    except ValueError as e:
        print(f"[ERROR] {e}")
        return 1

    if args.maxresults < 1 or args.maxresults > 1000:
        print("[ERROR] --maxresults must be between 1 and 1000")
        return 1

    config = load_company_config(args.company)
    try:
        verify_realm_match(config.company_key, config.realm_id)
    except RuntimeError as e:
        print(f"[ERROR] {e}")
        return 1

    token_mgr = TokenManager(config.company_key, config.realm_id)
    include_inactive = not args.no_include_inactive
    fetch_all = not args.first_page_only

    try:
        items = query_inventory_items(token_mgr, config.realm_id, include_inactive, args.maxresults, fetch_all=fetch_all)
    except RuntimeError as e:
        print(f"[ERROR] {e}")
        return 1

    total_returned = len(items)
    total_with_inv = count_with_inv_start_date(items)
    missing_inv = items_missing_inv_start_date(items)
    issues = filter_issues(items, cutoff_date)
    issues_count = len(issues)

    # Summary
    print("--- Summary ---")
    print(f"Total items returned: {total_returned}")
    print(f"Total inventory items with InvStartDate: {total_with_inv}")
    print(f"Issues (InvStartDate > {cutoff_date}): {issues_count}")
    if missing_inv:
        print(f"Items without InvStartDate: {len(missing_inv)} (Ids: {[it.get('Id') for it in missing_inv[:20]]}{'...' if len(missing_inv) > 20 else ''})")
        print(f"  Raw keys on first such item: {list(missing_inv[0].keys())}")

    # Table-like list of issues
    if issues:
        print("\n--- Items with InvStartDate after cutoff ---")
        col_id = "Id"
        col_name = "Name"
        col_inv = "InvStartDate"
        col_active = "Active"
        col_tqoh = "TrackQtyOnHand"
        widths = [
            max(len(col_id), max(len(str(it["Id"])) for it in issues)),
            max(len(col_name), min(50, max(len(str(it["Name"])) for it in issues))),
            max(len(col_inv), 10),
            max(len(col_active), 5),
            max(len(col_tqoh), 3),
        ]
        fmt = "  ".join(f"{{:<{w}}}" for w in widths)
        print(fmt.format(col_id, col_name[:widths[1]], col_inv, col_active, col_tqoh))
        print("-" * (sum(widths) + 2 * (len(widths) - 1)))
        for it in issues:
            name = str(it["Name"])[:widths[1]]
            print(fmt.format(
                str(it["Id"]),
                name,
                str(it["InvStartDate"]),
                str(it["Active"]),
                str(it["TrackQtyOnHand"]),
            ))
    else:
        print("\nNo issues (no Inventory items with InvStartDate > cutoff).")

    # Optional CSV export
    if args.export_csv and issues:
        out_path = Path(args.export_csv)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["Id", "Name", "Type", "TrackQtyOnHand", "InvStartDate", "Active"])
            w.writeheader()
            w.writerows(issues)
        print(f"\n[INFO] Wrote {issues_count} row(s) to {out_path}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/qbo_debug_account_query.py">
#!/usr/bin/env python3
"""
Diagnostic: run QBO Account queries using Name-based matching only (no AccountNumber).
Read-only; reuses auth and _make_qbo_request from qbo_upload.py.

Example:
  python scripts/qbo_debug_account_query.py --company company_a --account-name "120300 - Non - Food Items"
  python scripts/qbo_debug_account_query.py --company company_a --account-number 120000
  python scripts/qbo_debug_account_query.py --company company_a --account-number 120000 --sample-mapping "120000 - Inventory:120300 - Non - Food Items"
"""
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from urllib.parse import quote

# Run from repo root (parent of scripts/); add repo root and scripts for imports
_REPO_ROOT = Path(__file__).resolve().parent.parent
_SCRIPTS = Path(__file__).resolve().parent
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))
if str(_SCRIPTS) not in sys.path:
    sys.path.insert(0, str(_SCRIPTS))

from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from token_manager import verify_realm_match
from qbo_upload import BASE_URL, _make_qbo_request, TokenManager

load_env_file()

# Name-based only (no AccountNumber)
ACCOUNT_SELECT = "Id, Name, Active, AccountType"
MAX_RESULTS = 10


def run_query(
    token_mgr: TokenManager,
    realm_id: str,
    query: str,
) -> list[dict]:
    """Execute a QBO query. On non-200 prints diagnostics; on 200 prints QueryResponse keys and returns Account list."""
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    resp = _make_qbo_request("GET", url, token_mgr)

    if resp.status_code != 200:
        print(f"HTTP status: {resp.status_code}")
        print(f"Response (first 1000 chars): {resp.text[:1000]}")
        print(f"Query: {query}")
        return []

    data = resp.json()
    qr = data.get("QueryResponse", {})
    if "totalCount" in qr:
        print(f"QueryResponse.totalCount: {qr['totalCount']}")
    entity_keys = [k for k in qr.keys() if k != "totalCount"]
    if entity_keys:
        print(f"QueryResponse entities: {entity_keys}")

    accounts = qr.get("Account", [])
    if not isinstance(accounts, list):
        accounts = [accounts] if accounts else []
    return accounts


def print_results(accounts: list[dict], verbose: bool) -> None:
    """Print count and either full JSON (verbose) or compact table (Id, Name)."""
    print(f"Count: {len(accounts)}")
    if verbose:
        print(json.dumps(accounts, indent=2))
    else:
        for i, a in enumerate(accounts[:10]):
            aid = a.get("Id", "")
            name = a.get("Name", "")
            print(f"  {i+1}. Id={aid} Name={name!r}")
        if len(accounts) > 10:
            print(f"  ... and {len(accounts) - 10} more")


def find_sample_mapping_from_file(mapping_file: Path, account_number: str) -> str | None:
    """Load Product.Mapping.csv and return first account string starting with '{account_number} - '."""
    from qbo_verify_mapping_accounts import load_mapping_accounts_with_provenance

    rows = load_mapping_accounts_with_provenance(mapping_file)
    prefix = f"{account_number} - "
    for _cat, _col, acct in rows:
        if acct.startswith(prefix):
            return acct
    return None


def parse_sample_mapping(sample_mapping: str) -> tuple[str, str]:
    """Parse mapping string: leaf = substring after last ':' then strip. E.g. '120000 - Inventory:120300 - Non - Food Items' -> leaf '120300 - Non - Food Items'."""
    s = sample_mapping.strip()
    # Leaf = after last colon (full string if no colon)
    leaf = s.split(":")[-1].strip() if s else ""
    # Name part for C = everything after first ' - ' (optional, for C we use full fqn from mapping)
    if " - " not in s:
        name_part = s
    else:
        _num, name_part = s.split(" - ", 1)
        name_part = name_part.strip()
    return name_part, leaf


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Run QBO Account queries using Name-based matching only (no AccountNumber)."
    )
    parser.add_argument("--company", required=True, choices=get_available_companies(), help="Company key")
    parser.add_argument("--account-number", default=None, help="Optional: used to derive sample mapping from Product.Mapping.csv for B/C/D.")
    parser.add_argument(
        "--account-name",
        default=None,
        help="Leaf name for Query B (exact Name match). E.g. '120300 - Non - Food Items'. If omitted, derived from sample mapping.",
    )
    parser.add_argument(
        "--sample-mapping",
        default=None,
        help='Full mapping string for B/C/D (e.g. "120000 - Inventory:120300 - Non - Food Items"). If omitted, derived from Product.Mapping.csv when --account-number is set.',
    )
    parser.add_argument("--verbose", action="store_true", help="Print full JSON for each query result; otherwise count + compact table.")
    args = parser.parse_args()
    verbose = args.verbose

    config = load_company_config(args.company)
    verify_realm_match(args.company, config.realm_id)
    token_mgr = TokenManager(config.company_key, config.realm_id)
    realm_id = config.realm_id

    # Resolve sample mapping for B/C/D (needed for leaf when --account-name not provided)
    sample_mapping = args.sample_mapping
    account_number = (args.account_number or "").strip()
    if not sample_mapping and account_number and config.product_mapping_file.exists():
        sample_mapping = find_sample_mapping_from_file(config.product_mapping_file, account_number)

    # --- A: List a few accounts ---
    print("=== A) List a few accounts (minimal fields) ===")
    query_a = f"select {ACCOUNT_SELECT} from Account maxresults {MAX_RESULTS}"
    accounts_a = run_query(token_mgr, realm_id, query_a)
    print_results(accounts_a, verbose)
    print()

    # --- B: By Name exact (leaf) ---
    leaf_b = (args.account_name or "").strip() or None
    if not leaf_b and sample_mapping:
        _name_part, leaf_b = parse_sample_mapping(sample_mapping)
    if leaf_b:
        print("=== B) Query by Name exact (leaf) ===")
        print(f"leaf: {leaf_b!r}")
        safe_leaf_b = leaf_b.replace("'", "''")
        query_b = f"select {ACCOUNT_SELECT} from Account where Name = '{safe_leaf_b}' maxresults {MAX_RESULTS}"
        accounts_b = run_query(token_mgr, realm_id, query_b)
        print_results(accounts_b, verbose)
    else:
        print("=== B) Query by Name exact (leaf) ===")
        print("(Skipped: no --account-name and no sample mapping to derive leaf)")
    print()

    # --- C & D: need sample mapping ---
    if not sample_mapping:
        print("(C and D skipped: no --sample-mapping and no account string in Product.Mapping.csv for --account-number)")
    else:
        name_guess, leaf = parse_sample_mapping(sample_mapping)
        safe_name = name_guess.replace("'", "''")
        safe_leaf = leaf.replace("'", "''")[:80]

        # --- C: By Name exact (full name part after first " - ") ---
        print("=== C) Query by Name exact (full mapping name part) ===")
        print(f"name_guess: {name_guess!r}")
        query_c = f"select {ACCOUNT_SELECT} from Account where Name = '{safe_name}' maxresults {MAX_RESULTS}"
        accounts_c = run_query(token_mgr, realm_id, query_c)
        print_results(accounts_c, verbose)
        print()

        # --- D: By Name LIKE (leaf segment) ---
        print("=== D) Query by Name LIKE (leaf segment) ===")
        print(f"leaf: {leaf!r}")
        query_d = f"select {ACCOUNT_SELECT} from Account where Name like '%{safe_leaf}%' maxresults {MAX_RESULTS}"
        accounts_d = run_query(token_mgr, realm_id, query_d)
        print_results(accounts_d, verbose)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/qbo_debug_item_by_id.py">
#!/usr/bin/env python3
"""
Diagnostic: fetch a QuickBooks Item by ID (direct entity fetch) or search by Name to inspect full metadata
and identify hidden/system items that block inventory creation.
Read-only; reuses auth and _make_qbo_request from qbo_upload.py.

Example:
  python scripts/qbo_debug_item_by_id.py --company company_a --item-id 9109
  python scripts/qbo_debug_item_by_id.py --company company_a --name "MARY & MAY 12g"
"""
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from urllib.parse import quote

# Run from repo root (parent of scripts/); add repo root for imports
_REPO_ROOT = Path(__file__).resolve().parent.parent
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from token_manager import verify_realm_match
from qbo_upload import BASE_URL, _make_qbo_request, TokenManager

load_env_file()


def _ref_str(ref: dict | None) -> str:
    """Format a Ref dict as 'value - name' or empty string."""
    if not ref or not isinstance(ref, dict):
        return ""
    name = ref.get("name", "")
    value = ref.get("value", "")
    if name:
        return f"{value} - {name}" if value else name
    return value or ""


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Fetch a QBO Item by ID (direct entity fetch) or search by Name to inspect metadata and identify system/hidden items."
    )
    parser.add_argument("--company", required=True, choices=get_available_companies(), help="Company key")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--item-id", help="QBO Item Id (e.g. 9109)")
    group.add_argument("--name", help="Search Item by Name (e.g. 'MARY & MAY 12g')")
    args = parser.parse_args()

    config = load_company_config(args.company)
    verify_realm_match(args.company, config.realm_id)
    token_mgr = TokenManager(config.company_key, config.realm_id)
    realm_id = config.realm_id

    if args.name:
        # Search by Name
        safe_name = args.name.strip().replace("'", "''")
        query = f"select Id, Name, Type, Active from Item where Name = '{safe_name}' maxresults 10"
        url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
        resp = _make_qbo_request("GET", url, token_mgr)

        if resp.status_code != 200:
            print(f"HTTP status: {resp.status_code}")
            print(f"Response (first 1000 chars): {resp.text[:1000]}")
            sys.exit(1)

        data = resp.json()
        items = data.get("QueryResponse", {}).get("Item", [])
        if not isinstance(items, list):
            items = [items] if items else []

        if not items:
            print(f"No items found with Name = {args.name!r}")
            sys.exit(1)

        print(f"Found {len(items)} item(s) with Name = {args.name!r}:")
        print()
        for i, item in enumerate(items, 1):
            print(f"--- Match {i} ---")
            print("Item ID:", item.get("Id", ""))
            print("Name:", item.get("Name", ""))
            print("Type:", item.get("Type", ""))
            print("Active:", item.get("Active", ""))
            print()
    else:
        # Fetch by ID
        item_id = args.item_id.strip()
        url = f"{BASE_URL}/v3/company/{realm_id}/item/{item_id}?minorversion=70"
        resp = _make_qbo_request("GET", url, token_mgr)

        if resp.status_code != 200:
            print(f"HTTP status: {resp.status_code}")
            print(f"Response (first 1000 chars): {resp.text[:1000]}")
            sys.exit(1)

        data = resp.json()
        item = data.get("Item")
        if not item:
            print("No Item in response.")
            print(json.dumps(data, indent=2))
            sys.exit(1)

        # Clean summary
        print("Item ID:", item.get("Id", ""))
        print("Name:", item.get("Name", ""))
        print("Type:", item.get("Type", ""))
        print("Active:", item.get("Active", ""))
        print("TrackQtyOnHand:", item.get("TrackQtyOnHand", ""))
        print("UnitPrice:", item.get("UnitPrice", ""))
        print("PurchaseCost:", item.get("PurchaseCost", ""))
        print("SalesTaxIncluded:", item.get("SalesTaxIncluded", ""))
        print("PurchaseTaxIncluded:", item.get("PurchaseTaxIncluded", ""))
        print("Taxable:", item.get("Taxable", ""))
        sales_tax_ref = item.get("SalesTaxCodeRef")
        print("SalesTaxCodeRef:", _ref_str(sales_tax_ref) if sales_tax_ref else "")
        purchase_tax_ref = item.get("PurchaseTaxCodeRef")
        print("PurchaseTaxCodeRef:", _ref_str(purchase_tax_ref) if purchase_tax_ref else "")
        inc = item.get("IncomeAccountRef")
        print("IncomeAccount:", _ref_str(inc) if inc else "")
        exp = item.get("ExpenseAccountRef")
        if exp:
            print("ExpenseAccountRef:", _ref_str(exp))
        asset = item.get("AssetAccountRef")
        if asset:
            print("AssetAccountRef:", _ref_str(asset))
        print("Domain:", item.get("domain", ""))
        if "sparse" in item:
            print("Sparse:", item.get("sparse"))

        item_type = (item.get("Type") or "").strip()
        if item_type and item_type != "Inventory":
            print()
            print("This item is a non-inventory Service/item type and will block inventory creation with the same name.")

        print()
        print("--- Raw JSON ---")
        print(json.dumps(item, indent=2))


if __name__ == "__main__":
    main()
</file>

<file path="scripts/qbo_debug_item_by_name.py">
#!/usr/bin/env python3
"""
Diagnostic: query QBO Items by exact Name to inspect SubItem/ParentRef and parent Category.
Used to verify that newly created Inventory items have ParentRef/SubItem set and parent Type=Category.
Read-only; reuses auth and _make_qbo_request from qbo_upload.py.

Example:
  python scripts/qbo_debug_item_by_name.py --company company_a --name "MARY & MAY 12g"

Expected after run_test_upload (inventory + category):
  Item Type: Inventory
  SubItem: True
  ParentRef.value present
  Parent Type: Category
  Parent Name: COSMETICS AND TOILETRIES (or normalized equivalent)
"""
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from urllib.parse import quote

# Run from repo root (parent of scripts/); add repo root for imports
_REPO_ROOT = Path(__file__).resolve().parent.parent
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from token_manager import verify_realm_match
from qbo_upload import BASE_URL, _make_qbo_request, TokenManager

load_env_file()


def _ref_str(ref: dict | None) -> str:
    """Format a Ref dict as 'value - name' or empty string."""
    if not ref or not isinstance(ref, dict):
        return ""
    name = ref.get("name", "")
    value = ref.get("value", "")
    if name:
        return f"{value} - {name}" if value else name
    return value or ""


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Query QBO Items by Name to inspect SubItem/ParentRef and parent Category item."
    )
    parser.add_argument("--company", required=True, choices=get_available_companies(), help="Company key")
    parser.add_argument("--name", required=True, help="Item name (exact match), e.g. 'MARY & MAY 12g'")
    args = parser.parse_args()

    config = load_company_config(args.company)
    verify_realm_match(args.company, config.realm_id)
    token_mgr = TokenManager(config.company_key, config.realm_id)
    realm_id = config.realm_id

    # QBO Query API does not support SubItem in SELECT; use valid fields only
    safe_name = args.name.strip().replace("'", "''")
    query = (
        "select Id, Name, Type, Active, ParentRef, FullyQualifiedName from Item "
        f"where Name = '{safe_name}' maxresults 10"
    )
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    resp = _make_qbo_request("GET", url, token_mgr)

    if resp.status_code != 200:
        print(f"HTTP status: {resp.status_code}")
        print(f"Response (first 1000 chars): {resp.text[:1000]}")
        sys.exit(1)

    data = resp.json()
    items = data.get("QueryResponse", {}).get("Item", [])
    if not isinstance(items, list):
        items = [items] if items else []

    if not items:
        print(f"No items found with Name = {args.name!r}")
        sys.exit(1)

    print(f"Found {len(items)} item(s) with Name = {args.name!r}:")
    print()

    for i, item in enumerate(items, 1):
        print(f"--- Match {i} ---")
        item_id = item.get("Id", "")
        print("Item ID:", item_id)
        print("Name:", item.get("Name", ""))
        print("Type:", item.get("Type", ""))
        print("Active:", item.get("Active", ""))
        parent_ref = item.get("ParentRef")
        if parent_ref:
            print("ParentRef (from query):", _ref_str(parent_ref))
            parent_id = parent_ref.get("value") if isinstance(parent_ref, dict) else None
        else:
            parent_id = None
            print("ParentRef (from query):", "")
        if item.get("FullyQualifiedName"):
            print("FullyQualifiedName:", item.get("FullyQualifiedName", ""))

        # GET by Id to read SubItem and full ParentRef (not available in query)
        if item_id:
            get_url = f"{BASE_URL}/v3/company/{realm_id}/item/{item_id}?minorversion=70"
            get_resp = _make_qbo_request("GET", get_url, token_mgr)
            if get_resp.status_code == 200:
                full_item = get_resp.json().get("Item")
                if full_item:
                    print()
                    print("  (from GET by Id:)")
                    print("  SubItem:", full_item.get("SubItem", ""))
                    print("  ParentRef:", _ref_str(full_item.get("ParentRef")))
                    print("  Type:", full_item.get("Type", ""))
                    parent_ref = full_item.get("ParentRef")
                    parent_id = parent_ref.get("value") if isinstance(parent_ref, dict) and parent_ref else None
            else:
                print("  [WARN] GET item by Id failed:", get_resp.status_code)

        if parent_id:
            parent_url = f"{BASE_URL}/v3/company/{realm_id}/item/{parent_id}?minorversion=70"
            parent_resp = _make_qbo_request("GET", parent_url, token_mgr)
            if parent_resp.status_code == 200:
                parent_data = parent_resp.json()
                parent_item = parent_data.get("Item")
                if parent_item:
                    print()
                    print("  Parent item (Category):")
                    print("    Id:", parent_item.get("Id", ""))
                    print("    Name:", parent_item.get("Name", ""))
                    print("    Type:", parent_item.get("Type", ""))
                    print("    Active:", parent_item.get("Active", ""))
                    ptype = (parent_item.get("Type") or "").strip()
                    if ptype != "Category":
                        print("    [WARN] Parent Type is not 'Category'")
                else:
                    print("  [WARN] No Item in parent response")
            else:
                print("  [WARN] Failed to fetch parent item:", parent_resp.status_code)
        else:
            print("  (No ParentRef.value; parent not fetched)")

        print()
        print("--- Raw JSON (query result) ---")
        print(json.dumps(item, indent=2))
        print()


if __name__ == "__main__":
    main()
</file>

<file path="scripts/qbo_verify_mapping_accounts.py">
#!/usr/bin/env python3
"""
Standalone diagnostic: verify that all accounts referenced in mappings/Product.Mapping.csv
exist in QBO using Name-based (leaf) resolution only. Read-only; uses same auth and request helpers as qbo_upload.py.

Example:
  python scripts/qbo_verify_mapping_accounts.py --company company_a
  python scripts/qbo_verify_mapping_accounts.py --company company_a --export-csv reports/mapping_account_verification_company_a.csv
  python scripts/qbo_verify_mapping_accounts.py --company company_a --maxresults 1000 --no-include-inactive
"""
from __future__ import annotations

import argparse
import csv
import re
import sys
from pathlib import Path
from urllib.parse import quote

# Run from repo root (parent of scripts/); add repo root to path for imports
_REPO_ROOT = Path(__file__).resolve().parent.parent
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

import pandas as pd

from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from token_manager import verify_realm_match
from qbo_upload import BASE_URL, _make_qbo_request, TokenManager

load_env_file()

# Same canonical/synonym logic as qbo_upload.load_category_account_mapping
def _norm_col(col: str) -> str:
    return str(col).strip().lower()

SYNONYM_TO_CANONICAL = {
    "category": "category",
    "categories": "category",
    "inventory account": "inventory account",
    "revenue account": "revenue account",
    "cost of sale account": "cost of sale account",
    "cost of sale": "cost of sale account",
    "cogs": "cost of sale account",
}
REQUIRED_CANONICALS = {"category", "inventory account", "revenue account", "cost of sale account"}

SOURCE_COLUMN_NAMES = {
    "inventory account": "Inventory Account",
    "revenue account": "Revenue Account",
    "cost of sale account": "Cost of Sale Account",
}


def load_mapping_accounts_with_provenance(mapping_file: Path) -> list[tuple[str, str, str]]:
    """
    Read Product.Mapping.csv with same header normalization as qbo_upload.
    Returns list of (SourceCategory, SourceColumn, SourceAccountString) for each account reference.
    """
    if not mapping_file.exists():
        raise FileNotFoundError(f"Mapping file not found: {mapping_file}")
    df = pd.read_csv(mapping_file)

    def norm(col: str) -> str:
        return str(col).strip().lower()

    canonical_to_actual = {}
    for actual_col in df.columns:
        n = norm(actual_col)
        if not n or re.match(r"^unnamed", n):
            continue
        canonical = SYNONYM_TO_CANONICAL.get(n)
        if canonical and canonical not in canonical_to_actual:
            canonical_to_actual[canonical] = actual_col

    missing = REQUIRED_CANONICALS - set(canonical_to_actual.keys())
    if missing:
        raise ValueError(
            f"Product.Mapping.csv missing required columns (after normalization): {', '.join(sorted(missing))}. "
            f"Detected: {list(df.columns)}"
        )

    cat_col = canonical_to_actual["category"]
    inv_col = canonical_to_actual["inventory account"]
    rev_col = canonical_to_actual["revenue account"]
    cost_col = canonical_to_actual["cost of sale account"]

    rows: list[tuple[str, str, str]] = []
    for _, row in df.iterrows():
        category = str(row[cat_col]).strip()
        category = re.sub(r"\s+", " ", category)
        if not category or category.lower() in ("nan", "none", ""):
            continue
        for canonical, col_name in [("inventory account", inv_col), ("revenue account", rev_col), ("cost of sale account", cost_col)]:
            val = str(row[col_name]).strip()
            if not val or val.lower() in ("nan", "none", ""):
                continue
            source_col = SOURCE_COLUMN_NAMES.get(canonical, canonical)
            rows.append((category, source_col, val))
    return rows


ACCOUNT_SELECT = "Id, Name, Active, AccountType"


def parse_leaf(account_string: str) -> str:
    """Leaf = substring after last ':' then strip. E.g. '120000 - Inventory:120300 - Non - Food Items' -> '120300 - Non - Food Items'."""
    s = (account_string or "").strip()
    if not s:
        return ""
    return s.split(":")[-1].strip()


def query_account_by_name(
    token_mgr: TokenManager,
    realm_id: str,
    name_exact: str,
    include_inactive: bool,
) -> dict | None:
    """Query Account by exact Name. Returns full account dict or None."""
    if not name_exact:
        return None
    safe = name_exact.replace("'", "''")
    where = f"Name = '{safe}'"
    if not include_inactive:
        where += " and Active = true"
    query = f"select {ACCOUNT_SELECT} from Account where {where} maxresults 10"
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    resp = _make_qbo_request("GET", url, token_mgr)
    if resp.status_code != 200:
        return None
    data = resp.json()
    accounts = data.get("QueryResponse", {}).get("Account", [])
    if not accounts:
        return None
    return accounts[0] if isinstance(accounts[0], dict) else None


def query_accounts_by_name_like(
    token_mgr: TokenManager,
    realm_id: str,
    name_pattern: str,
    maxresults: int,
    include_inactive: bool,
) -> list[dict]:
    """Conservative LIKE query on Name. name_pattern escaped; no % or _ from input."""
    if not name_pattern or len(name_pattern) > 80:
        return []
    safe = str(name_pattern).replace("'", "''").replace("%", "").replace("_", "")[:50]
    if not safe:
        return []
    where = f"Name like '%{safe}%'"
    if not include_inactive:
        where += " and Active = true"
    query = f"select {ACCOUNT_SELECT} from Account where {where} maxresults {maxresults}"
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    resp = _make_qbo_request("GET", url, token_mgr)
    if resp.status_code != 200:
        return []
    data = resp.json()
    accounts = data.get("QueryResponse", {}).get("Account", [])
    if not isinstance(accounts, list):
        accounts = [accounts] if accounts else []
    return accounts[:maxresults]


def resolve_account(
    account_string: str,
    token_mgr: TokenManager,
    realm_id: str,
    cache: dict,
    include_inactive: bool,
) -> dict | None:
    """Resolve account string to QBO account dict using Name-based (leaf) resolution only."""
    if account_string in cache:
        return cache[account_string]
    leaf = parse_leaf(account_string)
    acc = query_account_by_name(token_mgr, realm_id, leaf, include_inactive)
    cache[account_string] = acc
    return acc


def suggest_accounts(
    account_string: str,
    token_mgr: TokenManager,
    realm_id: str,
    maxresults: int,
    include_inactive: bool,
) -> list[dict]:
    """Return up to maxresults suggestion account dicts using Name LIKE on first token of leaf."""
    leaf = parse_leaf(account_string)
    first_token = leaf.split()[0].strip() if leaf else ""
    if not first_token:
        return []
    return query_accounts_by_name_like(token_mgr, realm_id, first_token, maxresults, include_inactive)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Verify Product.Mapping.csv accounts exist in QBO using Name-based (leaf) resolution."
    )
    parser.add_argument("--company", required=True, choices=get_available_companies(), help="Company key")
    parser.add_argument("--export-csv", default=None, help="Write results to CSV (e.g. reports/mapping_account_verification_company_a.csv)")
    parser.add_argument("--maxresults", type=int, default=1000, help="Max results for suggestion queries (default 1000)")
    parser.add_argument("--include-inactive", action="store_true", default=True, help="Include inactive accounts (default True)")
    parser.add_argument("--no-include-inactive", action="store_false", dest="include_inactive", help="Exclude inactive accounts")
    args = parser.parse_args()

    config = load_company_config(args.company)
    mapping_file = config.product_mapping_file
    verify_realm_match(args.company, config.realm_id)
    token_mgr = TokenManager(config.company_key, config.realm_id)

    rows_with_provenance = load_mapping_accounts_with_provenance(mapping_file)
    # Unique account strings with first provenance
    seen: set[str] = set()
    unique_with_provenance: list[tuple[str, str, str]] = []
    for cat, col, acct in rows_with_provenance:
        if acct not in seen:
            seen.add(acct)
            unique_with_provenance.append((cat, col, acct))

    cache: dict = {}
    resolved: dict[str, dict] = {}
    for _cat, _col, acct in unique_with_provenance:
        acc = resolve_account(acct, token_mgr, config.realm_id, cache, args.include_inactive)
        if acc is not None:
            resolved[acct] = acc

    resolved_count = len(resolved)
    missing_count = len(unique_with_provenance) - resolved_count

    print(f"\n=== Mapping account verification ===")
    print(f"Total unique accounts: {len(unique_with_provenance)}")
    print(f"Resolved: {resolved_count}")
    print(f"Missing: {missing_count}")

    export_rows: list[dict] = []
    for source_category, source_column, source_account in unique_with_provenance:
        acc = resolved.get(source_account)
        if acc is not None:
            export_rows.append({
                "SourceCategory": source_category,
                "SourceColumn": source_column,
                "SourceAccountString": source_account,
                "Resolved": "true",
                "ResolvedId": acc.get("Id", ""),
                "ResolvedName": acc.get("Name", ""),
                "ResolvedActive": acc.get("Active", ""),
                "ResolvedAccountType": acc.get("AccountType", ""),
                "Suggestions": "",
            })
        else:
            suggestions = suggest_accounts(
                source_account,
                token_mgr,
                config.realm_id,
                min(5, args.maxresults),
                args.include_inactive,
            )
            suggestion_names = "; ".join(str(s.get("Name", "")) for s in suggestions)
            leaf = parse_leaf(source_account)
            print(f"\n--- Missing ---")
            print(f"  SourceCategory: {source_category}")
            print(f"  SourceColumn: {source_column}")
            print(f"  SourceAccountString: {source_account}")
            print(f"  Leaf used: {leaf!r}")
            print(f"  Suggestions: {suggestion_names or 'none'}")
            export_rows.append({
                "SourceCategory": source_category,
                "SourceColumn": source_column,
                "SourceAccountString": source_account,
                "Resolved": "false",
                "ResolvedId": "",
                "ResolvedName": "",
                "ResolvedActive": "",
                "ResolvedAccountType": "",
                "Suggestions": suggestion_names,
            })

    if args.export_csv:
        out_path = Path(args.export_csv)
        if not out_path.is_absolute():
            out_path = _REPO_ROOT / out_path
        out_path.parent.mkdir(parents=True, exist_ok=True)
        fieldnames = [
            "SourceCategory", "SourceColumn", "SourceAccountString",
            "Resolved", "ResolvedId", "ResolvedName",
            "ResolvedActive", "ResolvedAccountType",
            "Suggestions",
        ]
        with open(out_path, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=fieldnames)
            w.writeheader()
            w.writerows(export_rows)
        print(f"\nExported to {out_path}")


if __name__ == "__main__":
    main()
</file>

<file path=".github/copilot-instructions.md">
# Copilot Instructions for AI Coding Agents

## Project Overview
This codebase is a Python project focused on processing, transforming, and auditing CSV sales data, primarily using Flask for web-based file uploads and Pandas for data manipulation. The project is organized for rapid, script-driven workflows and includes custom logic for location-based sales processing and audit comparison.

## Major Components
- **Flask Web App (`app.py`)**: Handles CSV file uploads via a web interface (`templates/upload.html`). Uploaded files are processed and saved to the `uploads/` directory, then transformed and output to `updates/`.
- **CSV Processing Logic**: The filename format (`LOCATION-DATE.csv`) is critical for routing and transforming data. Location codes map to human-readable names. Data is cleaned, columns are renamed, and output is saved as `processed_<original>.csv` in `updates/`.
- **Audit Scripts (`audit-record/compare.py`)**: Contains logic for comparing sales data across sources, using hardcoded multiline strings and dictionary-based comparison. Useful for reconciling discrepancies between systems.

## Developer Workflows
- **Run the Flask App**: Execute `python app.py` (ensure the `myenv` virtual environment is activated).
- **Upload and Process Files**: Use the web interface at `/` to upload CSVs. Processed files are saved in `updates/`.
- **Audit/Compare Data**: Run `audit-record/compare.py` directly for sales data reconciliation.
- **Virtual Environment**: Activate with `source myenv/bin/activate` before running scripts.

## Project-Specific Conventions
- **Filename Format**: Uploaded CSVs must follow `LOCATION-DATE.csv` (e.g., `E31-01-2025.csv`). Location codes are mapped in `app.py`.
- **Column Mapping**: Data transformation in `app.py` uses explicit column renaming and value assignment. See `process_csv_files()` for details.
- **Error Handling**: User feedback is provided via Flask `flash()` messages, visible in the web UI.
- **Directory Structure**: All uploads go to `uploads/`, processed files to `updates/`. Audit scripts are in `audit-record/`.

## Integration Points & Dependencies
- **Flask**: Web server and routing.
- **Pandas**: Data manipulation and CSV I/O.
- **Jinja2**: HTML templating for upload UI.
- **No database integration**: All data is file-based.

## Examples
- **Uploading a file**: Use the web UI, ensure filename matches convention.
- **Processing logic**: See `process_csv_files()` in `app.py` for column mapping and output.
- **Audit comparison**: See `structure_sales_data()` and `compare_sales_values()` in `audit-record/compare.py`.

## Key Files & Directories
- `app.py`: Main Flask app and CSV processing logic
- `audit-record/compare.py`: Sales data audit/comparison
- `templates/upload.html`: File upload UI
- `uploads/`: Raw uploaded files
- `updates/`: Processed output files

## Quickstart
1. Activate the environment: `source myenv/bin/activate`
2. Run the app: `python app.py`
3. Upload a CSV via the web UI
4. Find processed files in `updates/`
5. Run audit scripts as needed

---
For questions or unclear conventions, review the referenced files or ask for clarification.
</file>

<file path=".github/workflows/secret-scan.yml">
name: Secret Scanning

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    # Scan pushes to all branches (including main)

jobs:
  gitleaks:
    name: Gitleaks Secret Scan
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch full history for better detection

      - name: Run Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          # Fail the workflow if leaks are found
          no-git: false
          # Use default rules (comprehensive secret detection)
          # config-path: .gitleaks.toml  # Uncomment only if config file exists
          exit-code: 1  # Fail on leaks
</file>

<file path=".pre-commit-config.yaml">
# Pre-commit hooks configuration
# Install: pip install pre-commit && pre-commit install
# Run manually: pre-commit run --all-files
#
# Note: gitleaks is automatically downloaded on first run (no manual installation required)

repos:
  - repo: local
    hooks:
      - id: gitleaks
        name: Gitleaks Secret Scan
        description: Detect hardcoded secrets and credentials (auto-downloads gitleaks binary)
        entry: python .pre-commit-hooks/gitleaks-wrapper.py
        language: python
        additional_dependencies: [certifi>=2023.0.0]
        pass_filenames: false
        always_run: true
</file>

<file path=".pre-commit-hooks/gitleaks-wrapper.py">
#!/usr/bin/env python3
"""
Pre-commit hook wrapper for gitleaks that automatically downloads the binary.

This hook is self-contained and does not require gitleaks to be installed globally.
It downloads the appropriate binary for the platform (macOS/Windows/Linux) on first run.
"""

import os
import sys
import platform
import subprocess
import urllib.request
import ssl
import zipfile
import tarfile
from pathlib import Path

# Try to import certifi for SSL certificate verification
try:
    import certifi
except ImportError:
    certifi = None

# Gitleaks version to use
GITLEAKS_VERSION = "v8.18.0"

# Directory to store the gitleaks binary (gitignored)
HOOKS_DIR = Path(__file__).resolve().parent
BIN_DIR = HOOKS_DIR / ".bin"
BIN_DIR.mkdir(exist_ok=True)


def get_platform_info():
    """Determine platform and architecture for gitleaks binary."""
    system = platform.system().lower()
    machine = platform.machine().lower()
    
    # Map platform to gitleaks release naming
    # GitHub releases use format: gitleaks_8.18.0_darwin_arm64.tar.gz (version without 'v' prefix)
    version_num = GITLEAKS_VERSION.lstrip('v')  # Remove 'v' prefix for filename
    
    if system == "darwin":
        if machine in ("arm64", "aarch64"):
            return "darwin", "arm64", "gitleaks_{}_darwin_arm64.tar.gz".format(version_num)
        else:
            return "darwin", "amd64", "gitleaks_{}_darwin_amd64.tar.gz".format(version_num)
    elif system == "windows":
        if machine in ("arm64", "aarch64"):
            return "windows", "arm64", "gitleaks_{}_windows_arm64.zip".format(version_num)
        else:
            return "windows", "amd64", "gitleaks_{}_windows_amd64.zip".format(version_num)
    elif system == "linux":
        if machine in ("arm64", "aarch64"):
            return "linux", "arm64", "gitleaks_{}_linux_arm64.tar.gz".format(version_num)
        else:
            return "linux", "amd64", "gitleaks_{}_linux_amd64.tar.gz".format(version_num)
    else:
        raise RuntimeError(f"Unsupported platform: {system}")


def download_gitleaks():
    """Download and extract gitleaks binary for the current platform."""
    platform_name, arch, filename = get_platform_info()
    binary_name = "gitleaks.exe" if platform_name == "windows" else "gitleaks"
    binary_path = BIN_DIR / binary_name
    
    # If binary already exists, use it
    if binary_path.exists():
        return binary_path
    
    # Download URL
    url = f"https://github.com/gitleaks/gitleaks/releases/download/{GITLEAKS_VERSION}/{filename}"
    
    print(f"Downloading gitleaks {GITLEAKS_VERSION} for {platform_name}/{arch}...")
    print(f"URL: {url}")
    
    # Create SSL context with certifi certificates if available
    ssl_context = ssl.create_default_context()
    if certifi:
        try:
            ssl_context.load_verify_locations(certifi.where())
        except Exception:
            # If certifi fails, use default context (may fail on some systems)
            pass
    
    # Download the release archive
    archive_path = BIN_DIR / filename
    try:
        # Use urlopen with SSL context for certificate verification
        with urllib.request.urlopen(url, context=ssl_context) as response:
            with open(archive_path, 'wb') as out_file:
                out_file.write(response.read())
    except Exception as e:
        print(f"Error downloading gitleaks: {e}", file=sys.stderr)
        if "CERTIFICATE_VERIFY_FAILED" in str(e):
            print("\nTip: Install certificates by running:", file=sys.stderr)
            print("  python3 -m pip install --upgrade certifi", file=sys.stderr)
            print("  Or on macOS: /Applications/Python\\ 3.*/Install\\ Certificates.command", file=sys.stderr)
        sys.exit(1)
    
    # Extract the archive
    try:
        if filename.endswith(".zip"):
            with zipfile.ZipFile(archive_path, "r") as zip_ref:
                zip_ref.extractall(BIN_DIR)
        elif filename.endswith(".tar.gz"):
            with tarfile.open(archive_path, "r:gz") as tar_ref:
                tar_ref.extractall(BIN_DIR)
        else:
            raise RuntimeError(f"Unknown archive format: {filename}")
        
        # Clean up archive
        archive_path.unlink()
        
        # Make binary executable on Unix-like systems
        if platform_name != "windows":
            binary_path.chmod(0o755)
        
        if not binary_path.exists():
            raise RuntimeError(f"Binary not found after extraction: {binary_path}")
        
        print(f"✓ gitleaks downloaded to {binary_path}")
        return binary_path
        
    except Exception as e:
        print(f"Error extracting gitleaks: {e}", file=sys.stderr)
        sys.exit(1)


def main():
    """Run gitleaks on staged files."""
    try:
        # Download gitleaks if needed
        gitleaks_bin = download_gitleaks()
        
        # Run gitleaks protect on staged files
        cmd = [
            str(gitleaks_bin),
            "protect",
            "--no-banner",
            "--staged",
            "--verbose"
        ]
        
        result = subprocess.run(cmd, capture_output=False)
        sys.exit(result.returncode)
        
    except Exception as e:
        print(f"Error running gitleaks: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path=".pre-commit-hooks/run-gitleaks.sh">
#!/bin/sh
# DEPRECATED: no longer used; Python wrapper is the active hook.
# This file is kept for reference but is not used by .pre-commit-config.yaml
# The active hook is: .pre-commit-hooks/gitleaks-wrapper.py (Python-only, cross-platform)

if command -v python3 >/dev/null 2>&1; then
    exec python3 .pre-commit-hooks/gitleaks-wrapper.py "$@"
elif command -v python >/dev/null 2>&1; then
    exec python .pre-commit-hooks/gitleaks-wrapper.py "$@"
else
    echo "Error: Neither python3 nor python found in PATH" >&2
    exit 1
fi
</file>

<file path="dataPandasLib/testEnv/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_frame_eval/pydevd_frame_evaluator.pyx">
from __future__ import print_function
from _pydev_bundle._pydev_saved_modules import threading, thread
from _pydevd_bundle.pydevd_constants import GlobalDebuggerHolder
import dis
import sys
from _pydevd_frame_eval.pydevd_frame_tracing import update_globals_dict, dummy_tracing_holder
from _pydevd_frame_eval.pydevd_modify_bytecode import DebugHelper, insert_pydevd_breaks
from pydevd_file_utils import get_abs_path_real_path_and_base_from_frame, NORM_PATHS_AND_BASE_CONTAINER
from _pydevd_bundle.pydevd_trace_dispatch import fix_top_level_trace_and_get_trace_func

from _pydevd_bundle.pydevd_additional_thread_info import _set_additional_thread_info_lock
from _pydevd_bundle.pydevd_cython cimport PyDBAdditionalThreadInfo
from pydevd_tracing import SetTrace

_get_ident = threading.get_ident  # Note this is py3 only, if py2 needed to be supported, _get_ident would be needed.
_thread_local_info = threading.local()
_thread_active = threading._active

def clear_thread_local_info():
    global _thread_local_info
    _thread_local_info = threading.local()


cdef class ThreadInfo:

    cdef public PyDBAdditionalThreadInfo additional_info
    cdef public bint is_pydevd_thread
    cdef public int inside_frame_eval
    cdef public bint fully_initialized
    cdef public object thread_trace_func
    cdef bint _can_create_dummy_thread

    # Note: whenever get_func_code_info is called, this value is reset (we're using
    # it as a thread-local value info).
    # If True the debugger should not go into trace mode even if the new
    # code for a function is None and there are breakpoints.
    cdef public bint force_stay_in_untraced_mode

    cdef initialize(self, PyFrameObject * frame_obj):
        # Places that create a ThreadInfo should verify that
        # a current Python frame is being executed!
        assert frame_obj != NULL

        self.additional_info = None
        self.is_pydevd_thread = False
        self.inside_frame_eval = 0
        self.fully_initialized = False
        self.thread_trace_func = None

        # Get the root (if it's not a Thread initialized from the threading
        # module, create the dummy thread entry so that we can debug it --
        # otherwise, we have to wait for the threading module itself to
        # create the Thread entry).
        while frame_obj.f_back != NULL:
            frame_obj = frame_obj.f_back

        basename = <str> frame_obj.f_code.co_filename
        i = basename.rfind('/')
        j = basename.rfind('\\')
        if j > i:
            i = j
        if i >= 0:
            basename = basename[i + 1:]
        # remove ext
        i = basename.rfind('.')
        if i >= 0:
            basename = basename[:i]

        co_name = <str> frame_obj.f_code.co_name

        # In these cases we cannot create a dummy thread (an actual
        # thread will be created later or tracing will already be set).
        if basename == 'threading' and co_name in ('__bootstrap', '_bootstrap', '__bootstrap_inner', '_bootstrap_inner'):
            self._can_create_dummy_thread = False
        elif basename == 'pydev_monkey' and co_name == '__call__':
            self._can_create_dummy_thread = False
        elif basename == 'pydevd' and co_name in ('run', 'main', '_exec'):
            self._can_create_dummy_thread = False
        elif basename == 'pydevd_tracing':
            self._can_create_dummy_thread = False
        else:
            self._can_create_dummy_thread = True

        # print('Can create dummy thread for thread started in: %s %s' % (basename, co_name))

    cdef initialize_if_possible(self):
        # Don't call threading.currentThread because if we're too early in the process
        # we may create a dummy thread.
        self.inside_frame_eval += 1

        try:
            thread_ident = _get_ident()
            t = _thread_active.get(thread_ident)
            if t is None:
                if self._can_create_dummy_thread:
                    # Initialize the dummy thread and set the tracing (both are needed to
                    # actually stop on breakpoints).
                    t = threading.current_thread()
                    SetTrace(dummy_trace_dispatch)
                else:
                    return  # Cannot initialize until thread becomes active.

            if getattr(t, 'is_pydev_daemon_thread', False):
                self.is_pydevd_thread = True
                self.fully_initialized = True
            else:
                try:
                    additional_info = t.additional_info
                    if additional_info is None:
                        raise AttributeError()
                except:
                    with _set_additional_thread_info_lock:
                        # If it's not there, set it within a lock to avoid any racing
                        # conditions.
                        additional_info = getattr(thread, 'additional_info', None)
                        if additional_info is None:
                            additional_info = PyDBAdditionalThreadInfo()
                        t.additional_info = additional_info
                self.additional_info = additional_info
                self.fully_initialized = True
        finally:
            self.inside_frame_eval -= 1


cdef class FuncCodeInfo:

    cdef public str co_filename
    cdef public str co_name
    cdef public str canonical_normalized_filename
    cdef bint always_skip_code
    cdef public bint breakpoint_found
    cdef public object new_code

    # When breakpoints_mtime != PyDb.mtime the validity of breakpoints have
    # to be re-evaluated (if invalid a new FuncCodeInfo must be created and
    # tracing can't be disabled for the related frames).
    cdef public int breakpoints_mtime

    def __init__(self):
        self.co_filename = ''
        self.canonical_normalized_filename = ''
        self.always_skip_code = False

        # If breakpoints are found but new_code is None,
        # this means we weren't able to actually add the code
        # where needed, so, fallback to tracing.
        self.breakpoint_found = False
        self.new_code = None
        self.breakpoints_mtime = -1


def dummy_trace_dispatch(frame, str event, arg):
    if event == 'call':
        if frame.f_trace is not None:
            return frame.f_trace(frame, event, arg)
    return None


def get_thread_info_py() -> ThreadInfo:
    return get_thread_info(PyEval_GetFrame())


cdef ThreadInfo get_thread_info(PyFrameObject * frame_obj):
    '''
    Provides thread-related info.

    May return None if the thread is still not active.
    '''
    cdef ThreadInfo thread_info
    try:
        # Note: changing to a `dict[thread.ident] = thread_info` had almost no
        # effect in the performance.
        thread_info = _thread_local_info.thread_info
    except:
        if frame_obj == NULL:
            return None
        thread_info = ThreadInfo()
        thread_info.initialize(frame_obj)
        thread_info.inside_frame_eval += 1
        try:
            _thread_local_info.thread_info = thread_info

            # Note: _code_extra_index is not actually thread-related,
            # but this is a good point to initialize it.
            global _code_extra_index
            if _code_extra_index == -1:
                _code_extra_index = <int> _PyEval_RequestCodeExtraIndex(release_co_extra)

            thread_info.initialize_if_possible()
        finally:
            thread_info.inside_frame_eval -= 1

    return thread_info


def decref_py(obj):
    '''
    Helper to be called from Python.
    '''
    Py_DECREF(obj)


def get_func_code_info_py(thread_info, frame, code_obj) -> FuncCodeInfo:
    '''
    Helper to be called from Python.
    '''
    return get_func_code_info(<ThreadInfo> thread_info, <PyFrameObject *> frame, <PyCodeObject *> code_obj)


cdef int _code_extra_index = -1

cdef FuncCodeInfo get_func_code_info(ThreadInfo thread_info, PyFrameObject * frame_obj, PyCodeObject * code_obj):
    '''
    Provides code-object related info.

    Stores the gathered info in a cache in the code object itself. Note that
    multiple threads can get the same info.

    get_thread_info() *must* be called at least once before get_func_code_info()
    to initialize _code_extra_index.

    '''
    # f_code = <object> code_obj
    # DEBUG = f_code.co_filename.endswith('_debugger_case_multiprocessing.py')
    # if DEBUG:
    #     print('get_func_code_info', f_code.co_name, f_code.co_filename)

    cdef object main_debugger = GlobalDebuggerHolder.global_dbg
    thread_info.force_stay_in_untraced_mode = False  # This is an output value of the function.

    cdef PyObject * extra
    _PyCode_GetExtra(<PyObject *> code_obj, _code_extra_index, & extra)
    if extra is not NULL:
        extra_obj = <PyObject *> extra
        if extra_obj is not NULL:
            func_code_info_obj = <FuncCodeInfo> extra_obj
            if func_code_info_obj.breakpoints_mtime == main_debugger.mtime:
                # if DEBUG:
                #     print('get_func_code_info: matched mtime', f_code.co_name, f_code.co_filename)

                return func_code_info_obj

    cdef str co_filename = <str> code_obj.co_filename
    cdef str co_name = <str> code_obj.co_name
    cdef dict cache_file_type
    cdef tuple cache_file_type_key

    func_code_info = FuncCodeInfo()
    func_code_info.breakpoints_mtime = main_debugger.mtime

    func_code_info.co_filename = co_filename
    func_code_info.co_name = co_name

    if not func_code_info.always_skip_code:
        try:
            abs_path_real_path_and_base = NORM_PATHS_AND_BASE_CONTAINER[co_filename]
        except:
            abs_path_real_path_and_base = get_abs_path_real_path_and_base_from_frame(<object>frame_obj)

        func_code_info.canonical_normalized_filename = abs_path_real_path_and_base[1]

        cache_file_type = main_debugger.get_cache_file_type()
        # Note: this cache key must be the same from PyDB.get_file_type() -- see it for comments
        # on the cache.
        cache_file_type_key = (frame_obj.f_code.co_firstlineno, abs_path_real_path_and_base[0], <object>frame_obj.f_code)
        try:
            file_type = cache_file_type[cache_file_type_key]  # Make it faster
        except:
            file_type = main_debugger.get_file_type(<object>frame_obj, abs_path_real_path_and_base)  # we don't want to debug anything related to pydevd

        if file_type is not None:
            func_code_info.always_skip_code = True

    if not func_code_info.always_skip_code:
        if main_debugger is not None:

            breakpoints: dict = main_debugger.breakpoints.get(func_code_info.canonical_normalized_filename)
            function_breakpoint: object = main_debugger.function_breakpoint_name_to_breakpoint.get(func_code_info.co_name)
            # print('\n---')
            # print(main_debugger.breakpoints)
            # print(func_code_info.canonical_normalized_filename)
            # print(main_debugger.breakpoints.get(func_code_info.canonical_normalized_filename))
            code_obj_py: object = <object> code_obj
            cached_code_obj_info: object = _cache.get(code_obj_py)
            if cached_code_obj_info:
                # The cache is for new code objects, so, in this case it's already
                # using the new code and we can't change it as this is a generator!
                # There's still a catch though: even though we don't replace the code,
                # we may not want to go into tracing mode (as would usually happen
                # when the new_code is None).
                func_code_info.new_code = None
                breakpoint_found, thread_info.force_stay_in_untraced_mode = \
                    cached_code_obj_info.compute_force_stay_in_untraced_mode(breakpoints)
                func_code_info.breakpoint_found = breakpoint_found

            elif function_breakpoint:
                # Go directly into tracing mode
                func_code_info.breakpoint_found = True
                func_code_info.new_code = None
                
            elif breakpoints:
                # if DEBUG:
                #    print('found breakpoints', code_obj_py.co_name, breakpoints)

                # Note: new_code can be None if unable to generate.
                # It should automatically put the new code object in the cache.
                breakpoint_found, func_code_info.new_code = generate_code_with_breakpoints(code_obj_py, breakpoints)
                func_code_info.breakpoint_found = breakpoint_found

    Py_INCREF(func_code_info)
    _PyCode_SetExtra(<PyObject *> code_obj, _code_extra_index, <PyObject *> func_code_info)

    return func_code_info


cdef class _CodeLineInfo:

    cdef public dict line_to_offset
    cdef public int first_line
    cdef public int last_line

    def __init__(self, dict line_to_offset,  int first_line,  int last_line):
        self.line_to_offset = line_to_offset
        self.first_line = first_line
        self.last_line = last_line


# Note: this method has a version in pure-python too.
def _get_code_line_info(code_obj):
    line_to_offset: dict = {}
    first_line: int = None
    last_line: int = None

    cdef int offset
    cdef int line

    for offset, line in dis.findlinestarts(code_obj):
        line_to_offset[line] = offset

    if line_to_offset:
        first_line = min(line_to_offset)
        last_line = max(line_to_offset)
    return _CodeLineInfo(line_to_offset, first_line, last_line)


# Note: this is a cache where the key is the code objects we create ourselves so that
# we always return the same code object for generators.
# (so, we don't have a cache from the old code to the new info -- that's actually
# handled by the cython side in `FuncCodeInfo get_func_code_info` by providing the
# same code info if the debugger mtime is still the same).
_cache: dict = {}

def get_cached_code_obj_info_py(code_obj_py):
    '''
    :return _CacheValue:
    :note: on cython use _cache.get(code_obj_py) directly.
    '''
    return _cache.get(code_obj_py)


cdef class _CacheValue(object):

    cdef public object code_obj_py
    cdef public _CodeLineInfo code_line_info
    cdef public set breakpoints_hit_at_lines
    cdef public set code_lines_as_set

    def __init__(self, object code_obj_py, _CodeLineInfo code_line_info, set breakpoints_hit_at_lines):
        '''
        :param code_obj_py:
        :param _CodeLineInfo code_line_info:
        :param set[int] breakpoints_hit_at_lines:
        '''
        self.code_obj_py = code_obj_py
        self.code_line_info = code_line_info
        self.breakpoints_hit_at_lines = breakpoints_hit_at_lines
        self.code_lines_as_set = set(code_line_info.line_to_offset)

    cpdef compute_force_stay_in_untraced_mode(self, breakpoints):
        '''
        :param breakpoints:
            set(breakpoint_lines) or dict(breakpoint_line->breakpoint info)
        :return tuple(breakpoint_found, force_stay_in_untraced_mode)
        '''
        cdef bint force_stay_in_untraced_mode
        cdef bint breakpoint_found
        cdef set target_breakpoints

        force_stay_in_untraced_mode = False

        target_breakpoints = self.code_lines_as_set.intersection(breakpoints)
        breakpoint_found = bool(target_breakpoints)

        if not breakpoint_found:
            force_stay_in_untraced_mode = True
        else:
            force_stay_in_untraced_mode = self.breakpoints_hit_at_lines.issuperset(set(breakpoints))

        return breakpoint_found, force_stay_in_untraced_mode

def generate_code_with_breakpoints_py(object code_obj_py, dict breakpoints):
    return generate_code_with_breakpoints(code_obj_py, breakpoints)

# DEBUG = True
# debug_helper = DebugHelper()

cdef generate_code_with_breakpoints(object code_obj_py, dict breakpoints):
    '''
    :param breakpoints:
        dict where the keys are the breakpoint lines.
    :return tuple(breakpoint_found, new_code)
    '''
    # The cache is needed for generator functions, because after each yield a new frame
    # is created but the former code object is used (so, check if code_to_modify is
    # already there and if not cache based on the new code generated).

    cdef bint success
    cdef int breakpoint_line
    cdef bint breakpoint_found
    cdef _CacheValue cache_value
    cdef set breakpoints_hit_at_lines
    cdef dict line_to_offset

    assert code_obj_py not in _cache, 'If a code object is cached, that same code object must be reused.'

#     if DEBUG:
#         initial_code_obj_py = code_obj_py

    code_line_info = _get_code_line_info(code_obj_py)

    success = True

    breakpoints_hit_at_lines = set()
    line_to_offset = code_line_info.line_to_offset

    for breakpoint_line in breakpoints:
        if breakpoint_line in line_to_offset:
            breakpoints_hit_at_lines.add(breakpoint_line)

    if breakpoints_hit_at_lines:
        success, new_code = insert_pydevd_breaks(
            code_obj_py,
            breakpoints_hit_at_lines,
            code_line_info
        )

        if not success:
            code_obj_py = None
        else:
            code_obj_py = new_code

    breakpoint_found = bool(breakpoints_hit_at_lines)
    if breakpoint_found and success:
#         if DEBUG:
#             op_number = debug_helper.write_dis(
#                 'inserting code, breaks at: %s' % (list(breakpoints),),
#                 initial_code_obj_py
#             )
#
#             debug_helper.write_dis(
#                 'after inserting code, breaks at: %s' % (list(breakpoints,)),
#                 code_obj_py,
#                 op_number=op_number,
#             )

        cache_value = _CacheValue(code_obj_py, code_line_info, breakpoints_hit_at_lines)
        _cache[code_obj_py] = cache_value

    return breakpoint_found, code_obj_py

import sys

cdef bint IS_PY_39_OWNARDS = sys.version_info[:2] >= (3, 9)

def frame_eval_func():
    cdef PyThreadState *state = PyThreadState_Get()
    if IS_PY_39_OWNARDS:
        state.interp.eval_frame = <_PyFrameEvalFunction *> get_bytecode_while_frame_eval_39
    else:
        state.interp.eval_frame = <_PyFrameEvalFunction *> get_bytecode_while_frame_eval_38
    dummy_tracing_holder.set_trace_func(dummy_trace_dispatch)


def stop_frame_eval():
    cdef PyThreadState *state = PyThreadState_Get()
    state.interp.eval_frame = _PyEval_EvalFrameDefault

# During the build we'll generate 2 versions of the code below so that we're compatible with
# Python 3.9, which receives a "PyThreadState* tstate" as the first parameter and Python 3.6-3.8
# which doesn't.
### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!
cdef PyObject * get_bytecode_while_frame_eval_38(PyFrameObject * frame_obj, int exc):
    '''
    This function makes the actual evaluation and changes the bytecode to a version
    where programmatic breakpoints are added.
    '''
    if GlobalDebuggerHolder is None or _thread_local_info is None or exc:
        # Sometimes during process shutdown these global variables become None
        return CALL_EvalFrameDefault_38(frame_obj, exc)

    # co_filename: str = <str>frame_obj.f_code.co_filename
    # if co_filename.endswith('threading.py'):
    #     return CALL_EvalFrameDefault_38(frame_obj, exc)

    cdef ThreadInfo thread_info
    cdef int STATE_SUSPEND = 2
    cdef int CMD_STEP_INTO = 107
    cdef int CMD_STEP_OVER = 108
    cdef int CMD_STEP_OVER_MY_CODE = 159
    cdef int CMD_STEP_INTO_MY_CODE = 144
    cdef int CMD_STEP_INTO_COROUTINE = 206
    cdef int CMD_SMART_STEP_INTO = 128
    cdef bint can_skip = True
    try:
        thread_info = _thread_local_info.thread_info
    except:
        thread_info = get_thread_info(frame_obj)
        if thread_info is None:
            return CALL_EvalFrameDefault_38(frame_obj, exc)

    if thread_info.inside_frame_eval:
        return CALL_EvalFrameDefault_38(frame_obj, exc)

    if not thread_info.fully_initialized:
        thread_info.initialize_if_possible()
        if not thread_info.fully_initialized:
            return CALL_EvalFrameDefault_38(frame_obj, exc)

    # Can only get additional_info when fully initialized.
    cdef PyDBAdditionalThreadInfo additional_info = thread_info.additional_info
    if thread_info.is_pydevd_thread or additional_info.is_tracing:
        # Make sure that we don't trace pydevd threads or inside our own calls.
        return CALL_EvalFrameDefault_38(frame_obj, exc)

    # frame = <object> frame_obj
    # DEBUG = frame.f_code.co_filename.endswith('_debugger_case_tracing.py')
    # if DEBUG:
    #     print('get_bytecode_while_frame_eval', frame.f_lineno, frame.f_code.co_name, frame.f_code.co_filename)

    thread_info.inside_frame_eval += 1
    additional_info.is_tracing = True
    try:
        main_debugger: object = GlobalDebuggerHolder.global_dbg
        if main_debugger is None:
            return CALL_EvalFrameDefault_38(frame_obj, exc)
        frame = <object> frame_obj

        if thread_info.thread_trace_func is None:
            trace_func, apply_to_global = fix_top_level_trace_and_get_trace_func(main_debugger, frame)
            if apply_to_global:
                thread_info.thread_trace_func = trace_func

        if additional_info.pydev_step_cmd in (CMD_STEP_INTO, CMD_STEP_INTO_MY_CODE, CMD_STEP_INTO_COROUTINE, CMD_SMART_STEP_INTO) or \
                main_debugger.break_on_caught_exceptions or \
                main_debugger.break_on_user_uncaught_exceptions or \
                main_debugger.has_plugin_exception_breaks or \
                main_debugger.signature_factory or \
                additional_info.pydev_step_cmd in (CMD_STEP_OVER, CMD_STEP_OVER_MY_CODE) and main_debugger.show_return_values and frame.f_back is additional_info.pydev_step_stop:

            # if DEBUG:
            #     print('get_bytecode_while_frame_eval enabled trace')
            if thread_info.thread_trace_func is not None:
                frame.f_trace = thread_info.thread_trace_func
            else:
                frame.f_trace = <object> main_debugger.trace_dispatch
        else:
            func_code_info: FuncCodeInfo = get_func_code_info(thread_info, frame_obj, frame_obj.f_code)
            # if DEBUG:
            #     print('get_bytecode_while_frame_eval always skip', func_code_info.always_skip_code)
            if not func_code_info.always_skip_code:

                if main_debugger.has_plugin_line_breaks or main_debugger.has_plugin_exception_breaks:
                    can_skip = main_debugger.plugin.can_skip(main_debugger, <object> frame_obj)

                    if not can_skip:
                        # if DEBUG:
                        #     print('get_bytecode_while_frame_eval not can_skip')
                        if thread_info.thread_trace_func is not None:
                            frame.f_trace = thread_info.thread_trace_func
                        else:
                            frame.f_trace = <object> main_debugger.trace_dispatch

                if can_skip and func_code_info.breakpoint_found:
                    # if DEBUG:
                    #     print('get_bytecode_while_frame_eval new_code', func_code_info.new_code)
                    if not thread_info.force_stay_in_untraced_mode:
                        # If breakpoints are found but new_code is None,
                        # this means we weren't able to actually add the code
                        # where needed, so, fallback to tracing.
                        if func_code_info.new_code is None:
                            if thread_info.thread_trace_func is not None:
                                frame.f_trace = thread_info.thread_trace_func
                            else:
                                frame.f_trace = <object> main_debugger.trace_dispatch
                        else:
                            # print('Using frame eval break for', <object> frame_obj.f_code.co_name)
                            update_globals_dict(<object> frame_obj.f_globals)
                            Py_INCREF(func_code_info.new_code)
                            old = <object> frame_obj.f_code
                            frame_obj.f_code = <PyCodeObject *> func_code_info.new_code
                            Py_DECREF(old)
                    else:
                        # When we're forcing to stay in traced mode we need to
                        # update the globals dict (because this means that we're reusing
                        # a previous code which had breakpoints added in a new frame).
                        update_globals_dict(<object> frame_obj.f_globals)

    finally:
        thread_info.inside_frame_eval -= 1
        additional_info.is_tracing = False

    return CALL_EvalFrameDefault_38(frame_obj, exc)
### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!


### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!
cdef PyObject * get_bytecode_while_frame_eval_39(PyThreadState* tstate, PyFrameObject * frame_obj, int exc):
    '''
    This function makes the actual evaluation and changes the bytecode to a version
    where programmatic breakpoints are added.
    '''
    if GlobalDebuggerHolder is None or _thread_local_info is None or exc:
        # Sometimes during process shutdown these global variables become None
        return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)

    # co_filename: str = <str>frame_obj.f_code.co_filename
    # if co_filename.endswith('threading.py'):
    #     return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)

    cdef ThreadInfo thread_info
    cdef int STATE_SUSPEND = 2
    cdef int CMD_STEP_INTO = 107
    cdef int CMD_STEP_OVER = 108
    cdef int CMD_STEP_OVER_MY_CODE = 159
    cdef int CMD_STEP_INTO_MY_CODE = 144
    cdef int CMD_STEP_INTO_COROUTINE = 206
    cdef int CMD_SMART_STEP_INTO = 128
    cdef bint can_skip = True
    try:
        thread_info = _thread_local_info.thread_info
    except:
        thread_info = get_thread_info(frame_obj)
        if thread_info is None:
            return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)

    if thread_info.inside_frame_eval:
        return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)

    if not thread_info.fully_initialized:
        thread_info.initialize_if_possible()
        if not thread_info.fully_initialized:
            return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)

    # Can only get additional_info when fully initialized.
    cdef PyDBAdditionalThreadInfo additional_info = thread_info.additional_info
    if thread_info.is_pydevd_thread or additional_info.is_tracing:
        # Make sure that we don't trace pydevd threads or inside our own calls.
        return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)

    # frame = <object> frame_obj
    # DEBUG = frame.f_code.co_filename.endswith('_debugger_case_tracing.py')
    # if DEBUG:
    #     print('get_bytecode_while_frame_eval', frame.f_lineno, frame.f_code.co_name, frame.f_code.co_filename)

    thread_info.inside_frame_eval += 1
    additional_info.is_tracing = True
    try:
        main_debugger: object = GlobalDebuggerHolder.global_dbg
        if main_debugger is None:
            return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)
        frame = <object> frame_obj

        if thread_info.thread_trace_func is None:
            trace_func, apply_to_global = fix_top_level_trace_and_get_trace_func(main_debugger, frame)
            if apply_to_global:
                thread_info.thread_trace_func = trace_func

        if additional_info.pydev_step_cmd in (CMD_STEP_INTO, CMD_STEP_INTO_MY_CODE, CMD_STEP_INTO_COROUTINE, CMD_SMART_STEP_INTO) or \
                main_debugger.break_on_caught_exceptions or \
                main_debugger.break_on_user_uncaught_exceptions or \
                main_debugger.has_plugin_exception_breaks or \
                main_debugger.signature_factory or \
                additional_info.pydev_step_cmd in (CMD_STEP_OVER, CMD_STEP_OVER_MY_CODE) and main_debugger.show_return_values and frame.f_back is additional_info.pydev_step_stop:

            # if DEBUG:
            #     print('get_bytecode_while_frame_eval enabled trace')
            if thread_info.thread_trace_func is not None:
                frame.f_trace = thread_info.thread_trace_func
            else:
                frame.f_trace = <object> main_debugger.trace_dispatch
        else:
            func_code_info: FuncCodeInfo = get_func_code_info(thread_info, frame_obj, frame_obj.f_code)
            # if DEBUG:
            #     print('get_bytecode_while_frame_eval always skip', func_code_info.always_skip_code)
            if not func_code_info.always_skip_code:

                if main_debugger.has_plugin_line_breaks or main_debugger.has_plugin_exception_breaks:
                    can_skip = main_debugger.plugin.can_skip(main_debugger, <object> frame_obj)

                    if not can_skip:
                        # if DEBUG:
                        #     print('get_bytecode_while_frame_eval not can_skip')
                        if thread_info.thread_trace_func is not None:
                            frame.f_trace = thread_info.thread_trace_func
                        else:
                            frame.f_trace = <object> main_debugger.trace_dispatch

                if can_skip and func_code_info.breakpoint_found:
                    # if DEBUG:
                    #     print('get_bytecode_while_frame_eval new_code', func_code_info.new_code)
                    if not thread_info.force_stay_in_untraced_mode:
                        # If breakpoints are found but new_code is None,
                        # this means we weren't able to actually add the code
                        # where needed, so, fallback to tracing.
                        if func_code_info.new_code is None:
                            if thread_info.thread_trace_func is not None:
                                frame.f_trace = thread_info.thread_trace_func
                            else:
                                frame.f_trace = <object> main_debugger.trace_dispatch
                        else:
                            # print('Using frame eval break for', <object> frame_obj.f_code.co_name)
                            update_globals_dict(<object> frame_obj.f_globals)
                            Py_INCREF(func_code_info.new_code)
                            old = <object> frame_obj.f_code
                            frame_obj.f_code = <PyCodeObject *> func_code_info.new_code
                            Py_DECREF(old)
                    else:
                        # When we're forcing to stay in traced mode we need to
                        # update the globals dict (because this means that we're reusing
                        # a previous code which had breakpoints added in a new frame).
                        update_globals_dict(<object> frame_obj.f_globals)

    finally:
        thread_info.inside_frame_eval -= 1
        additional_info.is_tracing = False

    return CALL_EvalFrameDefault_39(tstate, frame_obj, exc)
### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!
### WARNING: GENERATED CODE, DO NOT EDIT!
</file>

<file path="load_env.py">
"""
Utility to automatically load environment variables from .env file.
This makes it easier to manage credentials without modifying shell profiles.
"""
import os
from pathlib import Path


def load_env_file(env_file: str = ".env") -> None:
    """
    Load environment variables from a .env file in the repo root.
    
    The .env file should contain lines like:
        QBO_CLIENT_ID=your_client_id
        EPOS_USERNAME=your_username
    
    Lines starting with # are treated as comments and ignored.
    """
    repo_root = Path(__file__).resolve().parent
    env_path = repo_root / env_file
    
    if not env_path.exists():
        # .env file is optional - if it doesn't exist, use system env vars
        return
    
    try:
        with open(env_path, "r") as f:
            for line in f:
                line = line.strip()
                
                # Skip empty lines and comments
                if not line or line.startswith("#"):
                    continue
                
                # Parse KEY=VALUE format
                if "=" in line:
                    key, value = line.split("=", 1)  # Split on first = only
                    key = key.strip()
                    value = value.strip()
                    
                    # Remove quotes if present
                    if value.startswith('"') and value.endswith('"'):
                        value = value[1:-1]
                    elif value.startswith("'") and value.endswith("'"):
                        value = value[1:-1]
                    
                    # Only set if not already in environment (env vars take precedence)
                    if key and not os.environ.get(key):
                        os.environ[key] = value
    except Exception as e:
        # Silently fail - if .env can't be read, fall back to system env vars
        pass
</file>

<file path="query_qbo_for_company.py">
#!/usr/bin/env python3
"""
Helper script to query QBO for a specific company using the company config system.

Usage:
    python3 query_qbo_for_company.py --company company_b query "select Id, Name from Item where Type = 'Service' MAXRESULTS 5"
    python3 query_qbo_for_company.py --company company_a query "select Id, Name from PaymentMethod"
"""

import os
import sys
import argparse
import requests
from urllib.parse import quote

from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from token_manager import get_access_token

# Load .env for shared credentials
load_env_file()

BASE_URL = "https://quickbooks.api.intuit.com"
MINOR_VERSION = os.environ.get("QBO_MINOR_VERSION", "70")


def qbo_query_for_company(query: str, company_key: str) -> dict:
    """
    Execute a QBO SQL-like query for a specific company.
    
    Args:
        query: SQL query string
        company_key: Company identifier ('company_a' or 'company_b')
    
    Returns:
        JSON response from QBO API
    """
    # Load company config to get realm_id
    config = load_company_config(company_key)
    realm_id = config.realm_id
    
    # Get access token for this company
    access_token = get_access_token(company_key, realm_id)
    
    # Build URL
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion={MINOR_VERSION}"
    
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Accept": "application/json",
    }
    
    resp = requests.get(url, headers=headers)
    resp.raise_for_status()
    return resp.json()


def main():
    parser = argparse.ArgumentParser(
        description="Query QuickBooks Online for a specific company using company config.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Query Items for Company B
  python3 query_qbo_for_company.py --company company_b query "select Id, Name from Item where Type = 'Service' MAXRESULTS 5"
  
  # Query PaymentMethods for Company B
  python3 query_qbo_for_company.py --company company_b query "select Id, Name from PaymentMethod"
  
  # Query Accounts for Company B
  python3 query_qbo_for_company.py --company company_b query "select Id, Name, AccountType from Account where AccountType = 'Income' MAXRESULTS 10"
        """
    )
    
    parser.add_argument(
        "--company",
        required=True,
        choices=get_available_companies(),
        help="Company identifier (REQUIRED). Available: %(choices)s",
    )
    
    parser.add_argument(
        "command",
        choices=["query"],
        help="Command to execute (currently only 'query' is supported)",
    )
    
    parser.add_argument(
        "sql_query",
        help="SQL query string to execute (e.g., \"select Id, Name from Item\")",
    )
    
    args = parser.parse_args()
    
    try:
        result = qbo_query_for_company(args.sql_query, args.company)
        
        # Pretty print the result
        import json
        print(json.dumps(result, indent=2))
        
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="requirements-dev.txt">
# Development dependencies (optional)
# Install with: pip install -r requirements-dev.txt
#
# These are not required for running the pipeline, but are useful for development.

# Pre-commit hooks for secret scanning
pre-commit>=4.0.0
</file>

<file path="store_tokens.py">
#!/usr/bin/env python3
"""
Helper script to store QBO OAuth tokens into qbo_tokens.sqlite.

This script simplifies onboarding on a new machine by providing a CLI interface
for storing tokens without hardcoding secrets.

⚠️  WARNING: DO NOT HARDCODE TOKENS OR CREDENTIALS IN THIS FILE ⚠️

- Tokens must be passed via CLI arguments only
- Tokens are stored in qbo_tokens.sqlite (which is gitignored)
- Secret scanning runs in CI and will block PRs if secrets are detected
- Never commit access tokens, refresh tokens, or any credentials

Usage:
    # Store tokens for a company
    python store_tokens.py --company company_a --access-token "..." --refresh-token "..."

    # List stored tokens (safe fields only)
    python store_tokens.py --list
"""

import argparse
import sys
import sqlite3
from pathlib import Path
from datetime import datetime

from token_manager import store_tokens_from_oauth
from company_config import load_company_config

# SQLite database file (same as token_manager.py)
SCRIPT_DIR = Path(__file__).resolve().parent
DB_FILE = SCRIPT_DIR / "qbo_tokens.sqlite"


def redact_tokens(text: str, access_token: str = "", refresh_token: str = "") -> str:
    """Redact tokens from error messages (best-effort)."""
    if access_token:
        text = text.replace(access_token, "[REDACTED_ACCESS_TOKEN]")
    if refresh_token:
        text = text.replace(refresh_token, "[REDACTED_REFRESH_TOKEN]")
    return text


def list_stored_tokens() -> None:
    """List stored tokens (safe fields only)."""
    if not DB_FILE.exists():
        print("No database found. Tokens have not been stored yet.")
        print(f"Database location: {DB_FILE}")
        return

    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.execute(
            "SELECT company_key, realm_id, environment, updated_at "
            "FROM qbo_tokens "
            "ORDER BY company_key, realm_id"
        )
        rows = cursor.fetchall()
        conn.close()

        if not rows:
            print("No tokens stored in database.")
            return

        print("\nStored tokens:")
        print("-" * 80)
        print(f"{'Company':<20} {'Realm ID':<20} {'Environment':<12} {'Updated At':<20}")
        print("-" * 80)

        for row in rows:
            company_key, realm_id, environment, updated_at = row
            # Convert Unix timestamp to human-readable
            if updated_at:
                dt = datetime.fromtimestamp(updated_at)
                updated_str = dt.strftime("%Y-%m-%d %H:%M:%S")
            else:
                updated_str = "N/A"

            env_str = environment or "production"
            print(f"{company_key:<20} {realm_id:<20} {env_str:<12} {updated_str:<20}")

        print("-" * 80)
        print(f"\nTotal: {len(rows)} token record(s)")

    except Exception as e:
        print(f"Error reading database: {e}")
        sys.exit(1)


def store_tokens(
    company_key: str,
    access_token: str,
    refresh_token: str,
    expires_in: int = 3600,
    environment: str = "production"
) -> None:
    """Store tokens for a company."""
    try:
        # Load company config to get realm_id
        config = load_company_config(company_key)

        # Store tokens
        store_tokens_from_oauth(
            company_key=company_key,
            realm_id=config.realm_id,
            access_token=access_token,
            refresh_token=refresh_token,
            expires_in=expires_in,
            environment=environment
        )

        # Print confirmation (safe fields only)
        display_name = config.display_name
        print(f"\n✅ Tokens stored successfully!")
        print(f"   Company: {company_key} ({display_name})")
        print(f"   Realm ID: {config.realm_id}")
        print(f"   Environment: {environment}")
        print(f"   Expires in: {expires_in} seconds")

    except FileNotFoundError as e:
        error_msg = redact_tokens(str(e), access_token, refresh_token)
        print(f"❌ Error: {error_msg}", file=sys.stderr)
        sys.exit(1)
    except ValueError as e:
        error_msg = redact_tokens(str(e), access_token, refresh_token)
        print(f"❌ Error: {error_msg}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        error_msg = redact_tokens(str(e), access_token, refresh_token)
        print(f"❌ Unexpected error: {error_msg}", file=sys.stderr)
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="Store QBO OAuth tokens into qbo_tokens.sqlite",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Store tokens for company_a
  python store_tokens.py --company company_a --access-token "..." --refresh-token "..."

  # Store tokens with custom expires_in and environment
  python store_tokens.py --company company_b --access-token "..." --refresh-token "..." --expires-in 3600 --env sandbox

  # List all stored tokens
  python store_tokens.py --list
        """
    )

    parser.add_argument(
        "--company",
        type=str,
        help="Company key (e.g., company_a, company_b)"
    )
    parser.add_argument(
        "--access-token",
        type=str,
        help="OAuth access token"
    )
    parser.add_argument(
        "--refresh-token",
        type=str,
        help="OAuth refresh token"
    )
    parser.add_argument(
        "--expires-in",
        type=int,
        default=3600,
        help="Token expiration time in seconds (default: 3600)"
    )
    parser.add_argument(
        "--env",
        type=str,
        default="production",
        choices=["production", "sandbox"],
        help="Environment: production or sandbox (default: production)"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="List stored tokens (safe fields only). Ignores other arguments."
    )

    args = parser.parse_args()

    # Handle --list mode
    if args.list:
        list_stored_tokens()
        return

    # Validate required arguments for store mode
    if not args.company:
        print("❌ Error: --company is required", file=sys.stderr)
        parser.print_help()
        sys.exit(1)

    if not args.access_token:
        print("❌ Error: --access-token is required", file=sys.stderr)
        parser.print_help()
        sys.exit(1)

    if not args.refresh_token:
        print("❌ Error: --refresh-token is required", file=sys.stderr)
        parser.print_help()
        sys.exit(1)

    # Store tokens
    store_tokens(
        company_key=args.company,
        access_token=args.access_token,
        refresh_token=args.refresh_token,
        expires_in=args.expires_in,
        environment=args.env
    )


if __name__ == "__main__":
    main()
</file>

<file path="companies/company_a.json">
{
  "company_key": "company_a",
  "display_name": "AKPONORA VENTURES LTD.",
  "qbo": {
    "realm_id": "9341455406194328",
    "deposit_account": "100900 - Undeposited Funds",
    "tax_mode": "vat_inclusive_7_5",
    "tax_code_id": "2",
    "tax_rate_id": "2",
    "tax_rate": 0.075,
    "default_item_id": "1",
    "default_income_account_id": "1"
  },
  "epos": {
    "username_env_key": "EPOS_USERNAME_A",
    "password_env_key": "EPOS_PASSWORD_A"
  },
  "transform": {
    "group_by": ["date", "tender"],
    "date_format": "%Y-%m-%d",
    "receipt_prefix": "SR",
    "receipt_number_format": "date_tender_sequence",
    "location_mapping": {}
  },
  "output": {
    "csv_prefix": "single_sales_receipts",
    "metadata_file": "last_epos_transform.json",
    "uploaded_docnumbers_file": "uploaded_docnumbers.json"
  },
  "slack": {
    "webhook_url_env_key": "SLACK_WEBHOOK_URL_A"
  },
  "trading_day": {
    "enabled": true,
    "start_hour": 5,
    "start_minute": 0
  },
  "inventory": {
    "enable_inventory_items": true,
    "allow_negative_inventory": true,
    "inventory_start_date": "today",
    "default_qty_on_hand": 0,
    "auto_fix_wrong_type_items": true
  }
}
</file>

<file path="companies/company_b.json">
{
  "company_key": "company_b",
  "display_name": "GOLDPLATES FEASTHOUSE LTD.",
  "qbo": {
    "realm_id": "9130357766900456",
    "deposit_account": "1250000 - Undeposited Funds",
    "tax_mode": "tax_inclusive_composite",
    "tax_code_id": "22",
    "tax_code_name": "Sales tax",
    "tax_rate": 0.125,
    "tax_components": [
      {"name": "VAT", "rate": 0.075, "tax_rate_id": "17"},
      {"name": "Lagos State", "rate": 0.05, "tax_rate_id": "30"}
    ],
    "default_item_id": "39",
    "default_income_account_id": "421",
    "department_mapping": {
      "1004 (VI)": "10",
      "Bask Lounge (Chevron)": "25",
      "Hotel (Ayangbure)": "17",
      "Lounge (Ayangbure)": "18",
      "Main Restaurant (Ayangbure)": "13",
      "Main Restaurant (Dream Park)": "15",
      "Shawarma Stand (Ayangbure)": "12",
      "Shawarma Stand (Chevron)": "26",
      "Shawarma Stand (Dream Park)": "16",
      "TALEA MALL (MAIN)": "28",
      "Club (Ayangbure)": "14",
      "MAIN STORE (C.L.H)": "22",
      "MAIN STORE (HQ)": "20",
      "MAIN STORE (IKORODU)": "21",
      "Marble Garden Plaza": "11",
      "Pastry Pay Point (Chevron)": "27"
    }
  },
  "epos": {
    "username_env_key": "EPOS_USERNAME_B",
    "password_env_key": "EPOS_PASSWORD_B"
  },
  "transform": {
    "group_by": ["date", "location", "tender"],
    "date_format": "%d/%m/%Y",
    "receipt_prefix": "SR",
    "receipt_number_format": "date_location_sequence",
    "location_mapping": {
      "TALEA MALL REST (CHEVRON)": "TALE",
      "1004 (VI)": "VI04",
      "AYANGBURE (MAIN)": "AYMN",
      "AYANGBURE STAND": "AYST",
      "DREAM PARK (MAIN)": "DPKM",
      "DREAM PARK STAND": "DPKS",
      "CLUB (AYANGBURE)": "CLAY",
      "HOTEL (AYANGBURE)": "HTAY",
      "LOUNGE (AYANGBURE)": "LGAY",
      "BASK LOUNGE (CHEVRON)": "BSKC",
      "SHAWARMA STAND (CHEVRON)": "SHWC",
      "PASTRY PAY POINT (CHEVRON)": "PSTR"
    }
  },
  "output": {
    "csv_prefix": "gp_sales_receipts",
    "metadata_file": "last_gp_transform.json",
    "uploaded_docnumbers_file": "uploaded_docnumbers_gp.json"
  },
  "slack": {
    "webhook_url_env_key": "SLACK_WEBHOOK_URL_B"
  }, 
  "trading_day": {
    "enabled": true,
    "start_hour": 5,
    "start_minute": 0
  }
}
</file>

<file path="companies/company.example.json">
{
  "company_key": "company_example",
  "display_name": "Your Company Name Here",
  "qbo": {
    "realm_id": "REPLACE_WITH_YOUR_REALM_ID",
    "deposit_account": "REPLACE_WITH_DEPOSIT_ACCOUNT_NAME",
    "tax_mode": "vat_inclusive_7_5",
    "tax_rate": 0.075,
    "tax_code_id": "REPLACE_WITH_TAX_CODE_ID",
    "tax_rate_id": "REPLACE_WITH_TAX_RATE_ID",
    "tax_code_name": null,
    "tax_components": [],
    "default_item_id": "1",
    "default_income_account_id": "1",
    "department_mapping": {}
  },
  "epos": {
    "username_env_key": "EPOS_USERNAME_EXAMPLE",
    "password_env_key": "EPOS_PASSWORD_EXAMPLE"
  },
  "transform": {
    "group_by": ["date", "tender"],
    "date_format": "%Y-%m-%d",
    "receipt_prefix": "SR",
    "receipt_number_format": "date_tender_sequence",
    "location_mapping": {}
  },
  "output": {
    "csv_prefix": "sales_receipts",
    "metadata_file": "last_transform.json",
    "uploaded_docnumbers_file": "uploaded_docnumbers.json"
  },
  "slack": {
    "webhook_url_env_key": "SLACK_WEBHOOK_URL_EXAMPLE"
  },
  "trading_day": {
    "enabled": false,
    "start_hour": 5,
    "start_minute": 0
  }
}
</file>

<file path="company_config.py">
"""
Company Configuration Loader

Loads and validates company-specific configuration from JSON files.
Provides a single source of truth for company settings.
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime


class CompanyConfig:
    """Company configuration loaded from JSON file."""
    
    def __init__(self, config_path: Path):
        """Load and validate company configuration."""
        if not config_path.exists():
            raise FileNotFoundError(f"Company config not found: {config_path}")
        
        with open(config_path, "r") as f:
            self._data = json.load(f)
        
        self._validate()
    
    def _validate(self) -> None:
        """Validate required fields in config."""
        required = ["company_key", "qbo", "epos", "transform", "output"]
        for field in required:
            if field not in self._data:
                raise ValueError(f"Missing required field: {field}")
        
        if "realm_id" not in self._data["qbo"]:
            raise ValueError("Missing qbo.realm_id in config")
        
        if "username_env_key" not in self._data["epos"]:
            raise ValueError("Missing epos.username_env_key in config")
        
        if "password_env_key" not in self._data["epos"]:
            raise ValueError("Missing epos.password_env_key in config")
    
    @property
    def company_key(self) -> str:
        """Company identifier (e.g., 'company_a', 'company_b')."""
        return self._data["company_key"]
    
    @property
    def display_name(self) -> str:
        """Human-readable company name."""
        return self._data.get("display_name", self.company_key)
    
    @property
    def realm_id(self) -> str:
        """QBO Realm ID for this company."""
        realm_id = self._data["qbo"]["realm_id"]
        if realm_id.startswith("REPLACE_WITH_"):
            raise ValueError(
                f"Realm ID not configured for {self.display_name}. "
                f"Please update {self.company_key}.json with the actual realm_id."
            )
        return realm_id
    
    @property
    def deposit_account(self) -> str:
        """Deposit account name for this company."""
        return self._data["qbo"]["deposit_account"]
    
    @property
    def tax_mode(self) -> str:
        """Tax mode: 'vat_inclusive_7_5' or 'sales_tax_company_b'."""
        return self._data["qbo"].get("tax_mode", "vat_inclusive_7_5")
    
    @property
    def tax_code_id(self) -> Optional[str]:
        """Tax code ID (for Company A VAT mode)."""
        return self._data["qbo"].get("tax_code_id")
    
    @property
    def tax_code_name(self) -> Optional[str]:
        """Tax code name (for Company B sales tax mode)."""
        return self._data["qbo"].get("tax_code_name")
    
    @property
    def tax_rate(self) -> float:
        """Tax rate as decimal (e.g., 0.075 for 7.5%, 0.125 for 12.5%)."""
        return self._data["qbo"].get("tax_rate", 0.075)  # Default to 7.5% if not specified
    
    @property
    def epos_username(self) -> str:
        """EPOS username from environment variable."""
        env_key = self._data["epos"]["username_env_key"]
        username = os.environ.get(env_key)
        if not username:
            raise RuntimeError(
                f"EPOS username not found. Set {env_key} environment variable "
                f"or add it to .env file."
            )
        return username
    
    @property
    def epos_password(self) -> str:
        """EPOS password from environment variable."""
        env_key = self._data["epos"]["password_env_key"]
        password = os.environ.get(env_key)
        if not password:
            raise RuntimeError(
                f"EPOS password not found. Set {env_key} environment variable "
                f"or add it to .env file."
            )
        return password
    
    @property
    def group_by(self) -> list:
        """List of fields to group by: ['date', 'tender'] or ['date', 'location', 'tender']."""
        return self._data["transform"]["group_by"]
    
    @property
    def date_format(self) -> str:
        """Date format string for transform output."""
        return self._data["transform"]["date_format"]
    
    @property
    def receipt_prefix(self) -> str:
        """Prefix for receipt numbers (e.g., 'SR')."""
        return self._data["transform"]["receipt_prefix"]
    
    @property
    def receipt_number_format(self) -> str:
        """Receipt number format: 'date_tender_sequence' or 'date_location_sequence'."""
        return self._data["transform"]["receipt_number_format"]
    
    @property
    def location_mapping(self) -> Dict[str, str]:
        """Mapping from location names to location codes (for Company B)."""
        return self._data["transform"].get("location_mapping", {})
    
    @property
    def csv_prefix(self) -> str:
        """Prefix for output CSV files."""
        return self._data["output"]["csv_prefix"]
    
    @property
    def metadata_file(self) -> str:
        """Name of metadata JSON file."""
        return self._data["output"]["metadata_file"]
    
    @property
    def uploaded_docnumbers_file(self) -> str:
        """Name of uploaded docnumbers ledger file."""
        return self._data["output"]["uploaded_docnumbers_file"]
    
    @property
    def slack_webhook_url(self) -> Optional[str]:
        """
        Slack webhook URL (optional).
        
        Supports two formats:
        1. Direct URL in config: "webhook_url_env_key": "https://hooks.slack.com/..."
        2. Environment variable key: "webhook_url_env_key": "SLACK_WEBHOOK_URL_A"
        """
        slack_config = self._data.get("slack", {})
        webhook_value = slack_config.get("webhook_url_env_key")
        if not webhook_value:
            return None
        
        # If it looks like a URL (starts with http), use it directly
        if webhook_value.startswith("http://") or webhook_value.startswith("https://"):
            return webhook_value
        
        # Otherwise, treat it as an environment variable key
        return os.environ.get(webhook_value)
    
    @property
    def trading_day_enabled(self) -> bool:
        """Whether trading day mode is enabled (default: False)."""
        return self._data.get("trading_day", {}).get("enabled", False)
    
    @property
    def trading_day_start_hour(self) -> int:
        """Trading day start hour (default: 5)."""
        return self._data.get("trading_day", {}).get("start_hour", 5)
    
    @property
    def trading_day_start_minute(self) -> int:
        """Trading day start minute (default: 0)."""
        return self._data.get("trading_day", {}).get("start_minute", 0)
    
    def _get_env_or_config(self, env_key: str, config_key: str, default: Any) -> Any:
        """Get value from ENV (if set) or config, with fallback to default.
        
        Precedence: ENV → company JSON → default
        """
        env_value = os.environ.get(env_key)
        if env_value is not None:
            # Convert string ENV values to appropriate types
            if isinstance(default, bool):
                return env_value.lower() in ("true", "1", "yes", "on")
            elif isinstance(default, int):
                try:
                    return int(env_value)
                except ValueError:
                    return default
            else:
                return env_value
        return self._data.get("inventory", {}).get(config_key, default)
    
    @property
    def inventory_enabled(self) -> bool:
        """Whether inventory items are enabled (default: False).
        
        ENV override: {COMPANY_KEY}_ENABLE_INVENTORY_ITEMS
        """
        env_key = f"{self.company_key.upper()}_ENABLE_INVENTORY_ITEMS"
        return self._get_env_or_config(env_key, "enable_inventory_items", False)
    
    @property
    def allow_negative_inventory(self) -> bool:
        """Whether negative inventory is allowed (default: False).
        
        ENV override: {COMPANY_KEY}_ALLOW_NEGATIVE_INVENTORY
        """
        env_key = f"{self.company_key.upper()}_ALLOW_NEGATIVE_INVENTORY"
        return self._get_env_or_config(env_key, "allow_negative_inventory", False)
    
    @property
    def inventory_start_date(self) -> str:
        """Inventory start date as ISO string (default: "today").
        
        If "today", returns current date in YYYY-MM-DD format.
        ENV override: {COMPANY_KEY}_INVENTORY_START_DATE
        """
        env_key = f"{self.company_key.upper()}_INVENTORY_START_DATE"
        value = self._get_env_or_config(env_key, "inventory_start_date", "today")
        
        if value == "today":
            return datetime.now().strftime("%Y-%m-%d")
        return str(value)
    
    @property
    def default_qty_on_hand(self) -> int:
        """Default quantity on hand for new inventory items (default: 0).
        
        ENV override: {COMPANY_KEY}_DEFAULT_QTY_ON_HAND
        """
        env_key = f"{self.company_key.upper()}_DEFAULT_QTY_ON_HAND"
        return self._get_env_or_config(env_key, "default_qty_on_hand", 0)
    
    @property
    def auto_fix_wrong_type_items(self) -> bool:
        """Whether to automatically rename and inactivate wrong-type items to free names for inventory creation (default: False).
        
        ENV override: {COMPANY_KEY}_AUTO_FIX_WRONG_TYPE_ITEMS
        """
        env_key = f"{self.company_key.upper()}_AUTO_FIX_WRONG_TYPE_ITEMS"
        return self._get_env_or_config(env_key, "auto_fix_wrong_type_items", False)
    
    @property
    def product_mapping_file(self) -> Path:
        """Path to product category mapping CSV file (default: mappings/Product.Mapping.csv)."""
        mapping_file = self._data.get("inventory", {}).get("product_mapping_file", "mappings/Product.Mapping.csv")
        repo_root = Path(__file__).resolve().parent
        return repo_root / mapping_file
    
    def get_qbo_config(self) -> Dict[str, Any]:
        """Get QBO-specific configuration."""
        return self._data["qbo"].copy()
    
    def get_transform_config(self) -> Dict[str, Any]:
        """Get transform-specific configuration."""
        return self._data["transform"].copy()


def load_company_config(company_key: str) -> CompanyConfig:
    """
    Load company configuration by company key.
    
    Args:
        company_key: 'company_a' or 'company_b'
    
    Returns:
        CompanyConfig instance
    
    Raises:
        FileNotFoundError: If config file doesn't exist
        ValueError: If config is invalid
    """
    repo_root = Path(__file__).resolve().parent
    config_path = repo_root / "companies" / f"{company_key}.json"
    
    return CompanyConfig(config_path)


def get_available_companies() -> list:
    """Return list of available company keys."""
    repo_root = Path(__file__).resolve().parent
    companies_dir = repo_root / "companies"
    
    if not companies_dir.exists():
        return []
    
    companies = []
    for config_file in companies_dir.glob("*.json"):
        try:
            with open(config_file, "r") as f:
                data = json.load(f)
                if "company_key" in data:
                    companies.append(data["company_key"])
        except Exception:
            continue
    
    return sorted(companies)
</file>

<file path="requirements.txt">
# Python dependencies for EPOS → QuickBooks Automation Pipeline
# Install with: pip install -r requirements.txt

# Web automation
playwright>=1.40.0

# Data processing
pandas>=2.0.0

# HTTP client for QuickBooks API
requests>=2.31.0

# SSL certificate bundle (optional but recommended for Slack notifications)
certifi>=2023.0.0
</file>

<file path="token_manager.py">
"""
QBO Token Manager with SQLite Storage

Manages QBO access tokens and refresh tokens in SQLite database, isolated by company_key and realm_id.
Prevents token mixing between companies.
"""

import os
import json
import time
import sqlite3
import stat
import base64
from pathlib import Path
from typing import Optional, Dict, Any
import threading

import requests

from load_env import load_env_file

# Load .env for shared credentials
load_env_file()

# QBO OAuth token endpoint
TOKEN_URL = "https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer"

# These must be set via environment variables
CLIENT_ID = os.environ.get("QBO_CLIENT_ID")
CLIENT_SECRET = os.environ.get("QBO_CLIENT_SECRET")

# SQLite database file
SCRIPT_DIR = Path(__file__).resolve().parent
DB_FILE = SCRIPT_DIR / "qbo_tokens.sqlite"

# Thread lock for database operations
_db_lock = threading.Lock()


def _validate_credentials() -> None:
    """Validate that required credentials are set."""
    if not CLIENT_ID:
        raise RuntimeError(
            "QBO_CLIENT_ID environment variable is not set. "
            "Please set it in your .env file."
        )
    if not CLIENT_SECRET:
        raise RuntimeError(
            "QBO_CLIENT_SECRET environment variable is not set. "
            "Please set it in your .env file."
        )


def _init_database() -> None:
    """Initialize SQLite database with qbo_tokens table if it doesn't exist."""
    with _db_lock:
        conn = sqlite3.connect(DB_FILE)
        try:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS qbo_tokens (
                    company_key TEXT NOT NULL,
                    realm_id TEXT NOT NULL UNIQUE,
                    access_token TEXT,
                    refresh_token TEXT NOT NULL,
                    access_expires_at INTEGER,
                    updated_at INTEGER NOT NULL,
                    environment TEXT DEFAULT 'production',
                    PRIMARY KEY (company_key, realm_id)
                )
            """)
            conn.commit()
        finally:
            conn.close()
        
        # Restrict file permissions
        try:
            DB_FILE.chmod(stat.S_IRUSR | stat.S_IWUSR)  # 0o600
        except OSError as e:
            # On network shares (SMB) or certain filesystems, chmod may be unsupported or treated as read-only.
            # Token reads/writes can still work, so we treat chmod as best-effort.
            if getattr(e, "errno", None) in (1, 30, 95):  # EPERM, EROFS, EOPNOTSUPP
                pass
            else:
                raise


def load_tokens(company_key: str, realm_id: str) -> Optional[Dict[str, Any]]:
    """
    Load tokens from database for a specific company/realm.
    
    Returns:
        Dict with access_token, refresh_token, expires_at, or None if not found
    """
    _init_database()
    
    with _db_lock:
        conn = sqlite3.connect(DB_FILE)
        try:
            cursor = conn.execute(
                "SELECT access_token, refresh_token, access_expires_at, updated_at, environment "
                "FROM qbo_tokens WHERE company_key = ? AND realm_id = ?",
                (company_key, realm_id)
            )
            row = cursor.fetchone()
            
            if not row:
                return None
            
            return {
                "access_token": row[0],
                "refresh_token": row[1],
                "expires_at": row[2],
                "updated_at": row[3],
                "environment": row[4] or "production",
            }
        finally:
            conn.close()


def save_tokens(
    company_key: str,
    realm_id: str,
    access_token: str,
    refresh_token: str,
    expires_at: float,
    environment: str = "production"
) -> None:
    """
    Save tokens to database for a specific company/realm.
    
    Args:
        company_key: Company identifier (e.g., 'company_a', 'company_b')
        realm_id: QBO Realm ID
        access_token: Access token
        refresh_token: Refresh token
        expires_at: Unix timestamp when access token expires
        environment: 'production' or 'sandbox'
    """
    _init_database()
    _validate_credentials()
    
    updated_at = int(time.time())
    
    with _db_lock:
        conn = sqlite3.connect(DB_FILE)
        try:
            conn.execute("""
                INSERT OR REPLACE INTO qbo_tokens 
                (company_key, realm_id, access_token, refresh_token, access_expires_at, updated_at, environment)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (company_key, realm_id, access_token, refresh_token, int(expires_at), updated_at, environment))
            conn.commit()
        finally:
            conn.close()


def is_token_expired(tokens: Optional[Dict[str, Any]]) -> bool:
    """Return True if token is expired or missing (with 60s safety margin)."""
    if not tokens:
        return True
    
    access_token = tokens.get("access_token")
    expires_at = tokens.get("expires_at")
    
    if not access_token or not expires_at:
        return True
    
    # Safety margin: refresh 60 seconds before actual expiry
    return time.time() > (expires_at - 60)


def refresh_access_token(company_key: str, realm_id: str) -> Dict[str, Any]:
    """
    Refresh access token using refresh token from database.
    
    Returns:
        Updated tokens dict
    """
    _validate_credentials()
    
    tokens = load_tokens(company_key, realm_id)
    if not tokens:
        raise RuntimeError(
            f"No tokens found for {company_key} (realm_id: {realm_id}). "
            "You need to run the OAuth flow first and store tokens."
        )
    
    refresh_token = tokens.get("refresh_token")
    if not refresh_token:
        raise RuntimeError(
            f"No refresh_token found for {company_key} (realm_id: {realm_id}). "
            "You need to re-authenticate via OAuth flow."
        )
    
    # Basic auth header
    auth_str = f"{CLIENT_ID}:{CLIENT_SECRET}".encode("utf-8")
    auth_header = base64.b64encode(auth_str).decode("utf-8")
    
    headers = {
        "Authorization": f"Basic {auth_header}",
        "Content-Type": "application/x-www-form-urlencoded",
        "Accept": "application/json",
    }
    
    data = {
        "grant_type": "refresh_token",
        "refresh_token": refresh_token,
    }
    
    resp = requests.post(TOKEN_URL, headers=headers, data=data)
    if resp.status_code != 200:
        error_detail = resp.text
        if resp.status_code == 401:
            if "invalid_client" in error_detail:
                raise RuntimeError(
                    f"Invalid CLIENT_ID or CLIENT_SECRET (401 invalid_client).\n"
                    f"Please check your .env file credentials."
                )
            else:
                raise RuntimeError(
                    f"Authentication failed (401). Check your CLIENT_ID and CLIENT_SECRET.\n"
                    f"Response: {error_detail}"
                )
        elif resp.status_code == 400 and "invalid_grant" in error_detail:
            raise RuntimeError(
                f"Refresh token is invalid or expired (400 invalid_grant).\n"
                f"You need to re-authenticate via OAuth flow for {company_key}."
            )
        else:
            raise RuntimeError(
                f"Failed to refresh access token: {resp.status_code} {error_detail}"
            )
    
    body = resp.json()
    new_access_token = body.get("access_token")
    new_refresh_token = body.get("refresh_token", refresh_token)  # Use new if provided, else keep old
    expires_in = body.get("expires_in", 3600)
    
    if not new_access_token:
        raise RuntimeError("Token refresh response missing access_token")
    
    expires_at = time.time() + int(expires_in)
    
    # Save updated tokens
    save_tokens(
        company_key=company_key,
        realm_id=realm_id,
        access_token=new_access_token,
        refresh_token=new_refresh_token,
        expires_at=expires_at,
        environment=tokens.get("environment", "production")
    )
    
    return {
        "access_token": new_access_token,
        "refresh_token": new_refresh_token,
        "expires_at": expires_at,
    }


def get_access_token(company_key: str, realm_id: str) -> str:
    """
    Get a valid access token for the specified company/realm.
    Automatically refreshes if expired.
    
    Args:
        company_key: Company identifier
        realm_id: QBO Realm ID
    
    Returns:
        Valid access token
    """
    tokens = load_tokens(company_key, realm_id)
    
    if not tokens:
        raise RuntimeError(
            f"No tokens found for {company_key} (realm_id: {realm_id}). "
            "You need to run the OAuth flow first and store tokens using store_tokens_from_oauth()."
        )
    
    if is_token_expired(tokens):
        tokens = refresh_access_token(company_key, realm_id)
    
    return tokens["access_token"]


def store_tokens_from_oauth(
    company_key: str,
    realm_id: str,
    access_token: str,
    refresh_token: str,
    expires_in: int,
    environment: str = "production"
) -> None:
    """
    Store tokens from OAuth flow into database.
    
    Args:
        company_key: Company identifier
        realm_id: QBO Realm ID
        access_token: Access token from OAuth
        refresh_token: Refresh token from OAuth
        expires_in: Expires in seconds
        environment: 'production' or 'sandbox'
    """
    expires_at = time.time() + expires_in
    save_tokens(
        company_key=company_key,
        realm_id=realm_id,
        access_token=access_token,
        refresh_token=refresh_token,
        expires_at=expires_at,
        environment=environment
    )


def verify_realm_match(company_key: str, expected_realm_id: str) -> None:
    """
    Verify that tokens in database match expected realm_id.
    Safety check to prevent cross-posting.
    
    Raises:
        RuntimeError: If realm_id mismatch detected
    """
    tokens = load_tokens(company_key, expected_realm_id)
    if tokens:
        # If we can load tokens for this realm_id, they match
        return
    
    # Check if there are tokens for this company_key but different realm_id
    _init_database()
    with _db_lock:
        conn = sqlite3.connect(DB_FILE)
        try:
            cursor = conn.execute(
                "SELECT realm_id FROM qbo_tokens WHERE company_key = ?",
                (company_key,)
            )
            row = cursor.fetchone()
            if row and row[0] != expected_realm_id:
                raise RuntimeError(
                    f"REALM ID MISMATCH DETECTED!\n"
                    f"Company: {company_key}\n"
                    f"Expected realm_id: {expected_realm_id}\n"
                    f"Token database has realm_id: {row[0]}\n"
                    f"This is a safety check to prevent uploading to the wrong QBO company."
                )
        finally:
            conn.close()
</file>

<file path="transform.py">
"""
Unified EPOS Transform Script

Transforms EPOS CSV files to QuickBooks-ready format using company-specific configuration.
Supports both Company A (date+tender grouping) and Company B (date+location+tender grouping).

Usage:
    python transform_epos.py --company company_a --target-date 2025-12-25
    python transform_epos.py --company company_b --target-date 2025-12-25
"""

import os
import glob
import json
import sys
import argparse
from datetime import datetime, timezone, timedelta
from typing import Optional, Tuple, List, Dict
import pandas as pd
import re


from company_config import load_company_config, get_available_companies

# ------------------------------
# Local helpers (self-contained)
# ------------------------------

REQUIRED_COLUMNS = [
    "Customer Full Name",
    "Location Name",
    "Quantity",
    "Product",
    "Category",
    "Date/Time",
    "TOTAL Sales",
]


def ensure_required_columns(df: pd.DataFrame) -> None:
    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]
    if missing:
        raise ValueError(
            f"Missing required column(s): {', '.join(missing)}. Present: {', '.join(df.columns)}"
        )


def parse_date(value: str) -> Optional[datetime]:
    """Parse common date/time strings into a naive datetime (local to EPOS export).
    Returns None if empty/unparseable.
    """
    if value is None or (isinstance(value, float) and pd.isna(value)):
        return None
    s = str(value).strip()
    if s == "":
        return None
    for fmt in ("%d/%m/%Y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%d/%m/%Y", "%Y-%m-%d"):
        try:
            return datetime.strptime(s, fmt)
        except Exception:
            pass
    dt = pd.to_datetime(s, errors="coerce")
    if pd.isna(dt):
        return None
    return dt.to_pydatetime()


def sanitize_location_for_code(location_name: str) -> str:
    """Convert location name to a short code (max 4 chars) for DocNumber."""
    if not location_name or pd.isna(location_name):
        return "UNK"

    location_name = str(location_name).strip().upper()

    # NOTE: Prefer config.location_mapping for exact control.
    # This fallback is only used when the location is not mapped in config.
    location_map = {
        "MAIN RESTAURANT": "MAIN",
        "MAIN RESTAURANT (AYANGBURE)": "MAIN",
        "MAIN RESTAURANT (DREAM PARK)": "MDPK",
        "CLUB": "CLUB",
        "CLUB (AYANGBURE)": "CLUB",
        "HOTEL": "HTL",
        "HOTEL (AYANGBURE)": "HTL",
        "LOUNGE": "LNG",
        "LOUNGE (AYANGBURE)": "LNG",
        "BASK LOUNGE": "BSK",
        "BASK LOUNGE (CHEVRON)": "BSK",
        "SHAWARMA STAND": "SHW",
        "SHAWARMA STAND (CHEVRON)": "SHWC",
        "SHAWARMA STAND (AYANGBURE)": "SHWA",
        "SHAWARMA STAND (DREAM PARK)": "SHWD",
        "TALEA MALL": "TALE",
        "TALEA MALL (MAIN)": "TALE",
        "1004 (VI)": "VI",
        "1004": "VI",
    }

    if location_name in location_map:
        return location_map[location_name]

    for key, code in location_map.items():
        if key in location_name or location_name in key:
            return code

    words = re.sub(r"[()]", " ", location_name).split()
    for word in words:
        word_upper = word.upper()
        if word_upper not in ["THE", "AND", "OR", "OF", "IN", "AT", "ON"] and len(word_upper) >= 3:
            return word_upper[:4]

    return "UNK"


def generate_gp_receipt_no(date_obj: datetime, location_code: str, seq: int, prefix: str) -> str:
    """Generate DocNumber: PREFIX-YYYYMMDD-LOC-SEQ (<= 21 chars)."""
    date_str = date_obj.strftime("%Y%m%d")
    loc_code = (location_code or "UNK")[:4]
    receipt_no = f"{prefix}-{date_str}-{loc_code}-{seq:04d}"

    if len(receipt_no) > 21:
        max_loc_len = 21 - len(f"{prefix}-{date_str}--{seq:04d}")
        loc_code = (location_code or "X")[: max_loc_len if max_loc_len > 0 else 1]
        receipt_no = f"{prefix}-{date_str}-{loc_code}-{seq:04d}"

    return receipt_no


def get_repo_root() -> str:
    """Return the directory where this script lives (code-scripts)."""
    return os.path.dirname(os.path.abspath(__file__))


def find_latest_raw_file(repo_root: str) -> str:
    """
    Find the most recently modified CSV in repo root (excluding processed files).
    
    Excludes:
    - Processed files (single_sales_receipts_*, gp_sales_receipts_*)
    - Files in uploads/range_raw/** (range mode split files)
    - Files in Uploaded/** (archived files)
    
    Only searches repo root directory (single-day mode only).
    For range mode, use --raw-file to explicitly specify the split file.
    """
    pattern = os.path.join(repo_root, "*.csv")
    files = glob.glob(pattern)
    
    # Exclude processed files
    exclude_prefixes = ["single_sales_receipts_", "gp_sales_receipts_"]
    files = [f for f in files if not any(os.path.basename(f).startswith(prefix) for prefix in exclude_prefixes)]
    
    # Exclude files in uploads/range_raw/** and Uploaded/**
    # Convert to absolute paths for comparison
    repo_root_abs = os.path.abspath(repo_root)
    uploads_range_raw = os.path.join(repo_root_abs, "uploads", "range_raw")
    uploaded_dir = os.path.join(repo_root_abs, "Uploaded")
    
    filtered_files = []
    for f in files:
        f_abs = os.path.abspath(f)
        # Skip if file is in uploads/range_raw/** or Uploaded/**
        if (uploads_range_raw in f_abs) or (uploaded_dir in f_abs):
            continue
        filtered_files.append(f)
    
    files = filtered_files
    
    if not files:
        raise FileNotFoundError(
            f"No raw CSV files found in {repo_root}. "
            f"Note: Files in uploads/range_raw/** and Uploaded/** are excluded. "
            f"For range mode, use --raw-file to specify the split file explicitly."
        )
    return max(files, key=os.path.getmtime)


# WAT timezone (UTC+1)
WAT_TZ = timezone(timedelta(hours=1))


def filter_rows_by_target_date(
    df: pd.DataFrame,
    target_date: str,
    raw_file: str,
    config=None,
) -> Tuple[pd.DataFrame, pd.DataFrame, dict]:
    """
    Filter rows by target date and identify non-target rows.
    
    Note: Non-target rows are no longer written as spillover files.
    RAW spill handling is now managed by run_pipeline.py at the raw CSV level.
    
    Returns:
        (target_rows, non_target_rows, stats_dict)
    """
    target_dt = datetime.strptime(target_date, "%Y-%m-%d")
    target_date_str = target_dt.strftime("%Y-%m-%d")
    
    # Parse dates from Date/Time column
    dates_series = df["Date/Time"].apply(parse_date) if "Date/Time" in df.columns else pd.Series([None] * len(df))
    
    # Convert to WAT timezone and extract date portion (trading day aware if enabled)
    def get_date_in_wat(dt) -> Optional[str]:
        try:
            if dt is None or pd.isna(dt):
                return None
        except (TypeError, ValueError):
            return None

        try:
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=WAT_TZ)
            elif dt.tzinfo != WAT_TZ:
                dt = dt.astimezone(WAT_TZ)

            # If trading day mode is enabled, compute trading date using cutoff
            trading_enabled = bool(getattr(config, "trading_day_enabled", False))
            if trading_enabled:
                start_hour = int(getattr(config, "trading_day_start_hour", 5))
                start_minute = int(getattr(config, "trading_day_start_minute", 0))
                cutoff = datetime.combine(dt.date(), datetime.min.time()).replace(
                    hour=start_hour, minute=start_minute, tzinfo=WAT_TZ
                )
                # If dt is before cutoff on its calendar day, it belongs to previous trading day
                if dt < cutoff:
                    return (dt.date() - timedelta(days=1)).strftime("%Y-%m-%d")
                return dt.date().strftime("%Y-%m-%d")

            # Calendar day mode (existing behavior)
            return dt.date().strftime("%Y-%m-%d")
        except (AttributeError, ValueError, TypeError):
            return None
    
    date_strings = dates_series.apply(get_date_in_wat)
    
    # Filter: keep rows where date matches target_date
    target_mask = date_strings == target_date_str
    target_rows = df[target_mask].copy().reset_index(drop=True)
    non_target_rows = df[~target_mask].copy().reset_index(drop=True)
    
    # Collect statistics
    date_strings_clean = date_strings.dropna()
    dates_present_list = sorted(date_strings_clean.unique().tolist()) if len(date_strings_clean) > 0 else []
    
    # Show date distribution
    date_counts = date_strings.value_counts()
    print(f"\nDate distribution in CSV:")
    for date_val, count in date_counts.items():
        marker = " <-- TARGET" if date_val == target_date_str else ""
        print(f"  {date_val}: {count} rows{marker}")
    
    stats = {
        "rows_total": len(df),
        "rows_kept": len(target_rows),
        "rows_non_target": len(non_target_rows),
        "dates_present": dates_present_list,
        "min_dt": dates_present_list[0] if dates_present_list else None,
        "max_dt": dates_present_list[-1] if dates_present_list else None,
    }
    
    print(f"\nFiltering by target date: {target_date}")
    print(f"  Total rows: {stats['rows_total']}")
    print(f"  Rows kept (target date): {stats['rows_kept']}")
    print(f"  Rows ignored (non-target): {stats['rows_non_target']}")
    print(f"  Dates present in CSV: {', '.join(stats['dates_present'])}")
    
    # Warn if non-target rows exist (they should have been filtered at RAW level)
    if len(non_target_rows) > 0:
        non_target_dates = date_strings[~target_mask].value_counts()
        non_target_summary = ", ".join(f"{d}({c})" for d, c in non_target_dates.items())
        print(f"\n[WARNING] Raw file contains {len(non_target_rows)} row(s) not matching target_date (ignored).")
        print(f"          Dates present: {non_target_summary}")
        print(f"          Note: RAW spill handling should be done at pipeline level, not transform.")
    
    return target_rows, non_target_rows, stats


def transform_dataframe_unified(df: pd.DataFrame, config, target_date: Optional[str] = None) -> pd.DataFrame:
    """
    Transform dataframe using company-specific configuration.
    Handles both Company A (date+tender) and Company B (date+location+tender) grouping.
    
    Args:
        df: Input dataframe
        config: CompanyConfig object
        target_date: Optional target date in YYYY-MM-DD format (used when trading_day_enabled is True)
    """
    ensure_required_columns(df)
    
    # Normalize and parse dates
    dates_dt = df["Date/Time"].apply(parse_date) if "Date/Time" in df.columns else pd.Series([None] * len(df))
    dates_d = df["Date"].apply(parse_date) if "Date" in df.columns else None
    dates = dates_dt if dates_d is None else dates_dt.combine_first(dates_d)
    
    # Handle missing dates
    missing_mask = dates.isna()
    if missing_mask.any():
        skipped = int(missing_mask.sum())
        if skipped:
            print(f"Skipping {skipped} row(s) with missing date values.")
        df = df.loc[~missing_mask].reset_index(drop=True)
        dates = dates.loc[~missing_mask].reset_index(drop=True)
    
    # Build output columns
    out = pd.DataFrame()
    out["_parsed_date"] = dates
    out["_date_str"] = [d.strftime(config.date_format) for d in dates]
    out["Customer"] = df.get("Customer Full Name").fillna("")
    
    # CRITICAL: In trading-day mode, force _parsed_date to trading date for grouping/DocNumber generation
    # This must happen BEFORE grouping and DocNumber generation to ensure all DocNumbers use trading date
    if config.trading_day_enabled and target_date:
        try:
            target_dt = datetime.strptime(target_date, "%Y-%m-%d")
            # Force _parsed_date to trading date for all rows (used for grouping and DocNumber generation)
            # Create a Series with the same length as the dataframe, all set to trading date
            out["_parsed_date"] = pd.Series([target_dt] * len(out), index=out.index)
            print(f"[INFO] Trading day mode: forcing _parsed_date to trading_date={target_date} for all {len(out)} rows (grouping/DocNumber generation)")
        except ValueError:
            print(f"[WARN] Could not parse target_date {target_date}, using calendar dates for grouping")
    
    # Set *SalesReceiptDate: use target_date if trading_day_enabled, otherwise use parsed date
    if config.trading_day_enabled and target_date:
        # Trading day mode: use target_date (trading date) for all rows
        # Convert target_date (YYYY-MM-DD) to company's date_format
        try:
            target_dt = datetime.strptime(target_date, "%Y-%m-%d")
            target_date_formatted = target_dt.strftime(config.date_format)
            out["*SalesReceiptDate"] = target_date_formatted
            print(f"[INFO] Trading day mode: setting *SalesReceiptDate to target_date={target_date} (formatted as {target_date_formatted})")
        except ValueError:
            # Fallback: use as-is if format conversion fails
            print(f"[WARN] Could not format target_date {target_date} using {config.date_format}, using as-is")
            out["*SalesReceiptDate"] = target_date
    else:
        # Calendar day mode: use parsed date from CSV
        out["*SalesReceiptDate"] = out["_date_str"]
    out["*DepositAccount"] = config.deposit_account
    out["Location"] = df.get("Location Name").fillna("")
    
    # Use Tender column for Memo
    tender_col = df.get("Tender")
    if isinstance(tender_col, pd.Series):
        out["Memo"] = tender_col.fillna("")
    else:
        out["Memo"] = ""
    
    out["Item(Product/Service)"] = df.get("Product").fillna("")
    out["ItemDescription"] = df.get("Category").fillna("")
    out["ItemQuantity"] = df.get("Quantity").fillna(0)
    out["ItemRate"] = ""
    
    # Ensure numeric amounts
    def to_number(x):
        try:
            if isinstance(x, str):
                x = x.replace(",", "")
            return float(x)
        except Exception:
            return 0.0
    
    out["*ItemAmount"] = df.get("TOTAL Sales").apply(to_number)

    # Carry through EPOS totals needed for per-unit calculations downstream (uploader needs NET Sales, Cost Price)
    if "NET Sales" in df.columns:
        out["NET Sales"] = df.get("NET Sales").apply(to_number)
    else:
        out["NET Sales"] = 0.0

    cost_col = "Cost Price" if "Cost Price" in df.columns else ("Cost" if "Cost" in df.columns else None)
    if cost_col:
        out["Cost Price"] = df.get(cost_col).apply(to_number)
    else:
        out["Cost Price"] = 0.0
    
    # Tax code handling based on company config
    if config.tax_mode == "vat_inclusive_7_5":
        # Company A: infer tax code
        out["*ItemTaxCode"] = df.apply(
            lambda r: "No VAT" if 'delivery' in str(r.get("Product", "")).lower() or 'pack' in str(r.get("Product", "")).lower() else "Sales Tax",
            axis=1
        )
    else:
        # Company B: always use "Sales Tax"
        out["*ItemTaxCode"] = "Sales Tax"
    
    out["ItemTaxAmount"] = df.get("Tax", 0).apply(to_number)
    
    # Service Date
    if "Date" in df.columns:
        svc_dates = df["Date"].apply(parse_date)
        svc_dates = svc_dates.where(~svc_dates.isna(), dates)
        out["Service Date"] = [d.strftime(config.date_format) for d in svc_dates]
    else:
        out["Service Date"] = out["*SalesReceiptDate"]
    
    # Generate SalesReceiptNo based on company config
    if config.receipt_number_format == "date_location_sequence":
        # Company B: SR-YYYYMMDD-LOC-SEQ
        seq_by_date_location_tender: Dict[tuple, int] = {}
        receipt_numbers = []

        for idx, row in out.iterrows():
            date_obj = row["_parsed_date"]
            location_raw = str(row["Location"]).strip() if row["Location"] else "UNKNOWN"
            # Normalize EPOS location for mapping: uppercase, collapse spaces, strip trailing commas
            location_key = re.sub(r"\s+", " ", location_raw).strip().rstrip(",").upper()

            tender = str(row["Memo"]).strip() if row["Memo"] else "UNKNOWN"

            # Use location mapping from config (keys should be stored normalized in the same way)
            location_code = config.location_mapping.get(location_key, None)
            if not location_code:
                # Fallback to sanitize function
                location_code = sanitize_location_for_code(location_raw)

            # Group by (date, location, tender) but receipt number format is SR-YYYYMMDD-LOC-SEQ
            key = (date_obj.strftime("%Y%m%d"), location_raw, tender)
            if key not in seq_by_date_location_tender:
                seq_by_date_location_tender[key] = len(seq_by_date_location_tender) + 1
            seq = seq_by_date_location_tender[key]
            receipt_numbers.append(generate_gp_receipt_no(date_obj, location_code, seq, config.receipt_prefix))

        out["*SalesReceiptNo"] = receipt_numbers
    else:
        # Company A: SR-YYYYMMDD-SEQ (group by date+tender)
        seq_by_date_tender: Dict[tuple, int] = {}
        receipt_numbers = []
        
        for idx, row in out.iterrows():
            date_obj = row["_parsed_date"]
            tender = str(row["Memo"]).strip() if row["Memo"] else "UNKNOWN"
            
            key = (date_obj.strftime("%Y%m%d"), tender)
            if key not in seq_by_date_tender:
                seq_by_date_tender[key] = len(seq_by_date_tender) + 1
            seq = seq_by_date_tender[key]
            receipt_numbers.append(f"{config.receipt_prefix}-{date_obj.strftime('%Y%m%d')}-{seq:04d}")
        
        out["*SalesReceiptNo"] = receipt_numbers
    
    # Drop temporary columns
    out = out.drop(columns=["_parsed_date", "_date_str"])
    
    # Column order as required
    columns = [
        "*SalesReceiptNo",
        "Customer",
        "*SalesReceiptDate",
        "*DepositAccount",
        "Location",
        "Memo",
        "Item(Product/Service)",
        "ItemDescription",
        "ItemQuantity",
        "ItemRate",
        "*ItemAmount",
        "*ItemTaxCode",
        "ItemTaxAmount",
        "Service Date",
    ]
    return out[columns]


def main():
    parser = argparse.ArgumentParser(
        description="Transform EPOS CSV to QuickBooks format using company-specific configuration."
    )
    parser.add_argument(
        "--company",
        required=True,
        choices=get_available_companies(),
        help="Company identifier (REQUIRED). Available: %(choices)s",
    )
    parser.add_argument(
        "--target-date",
        help="Target business date in YYYY-MM-DD format (required for filtering)",
    )
    parser.add_argument(
        "--raw-file",
        help="Path to raw CSV file (overrides auto-detection of latest raw EPOS file)",
    )
    args = parser.parse_args()
    
    # Validate --raw-file if provided
    if args.raw_file:
        if not os.path.exists(args.raw_file):
            parser.error(f"--raw-file: file not found: {args.raw_file}")
        if not os.path.isfile(args.raw_file):
            parser.error(f"--raw-file: path is not a file: {args.raw_file}")
    
    # Load company configuration
    try:
        config = load_company_config(args.company)
    except Exception as e:
        print(f"Error: Failed to load company config for '{args.company}': {e}")
        sys.exit(1)
    
    repo_root = get_repo_root()
    
    # Get target_date
    if not args.target_date:
        print("Warning: No --target-date provided. Processing all rows without filtering.")
        print("  Usage: python transform_epos.py --company company_a --target-date YYYY-MM-DD")
        target_date = None
    else:
        target_date = args.target_date
    
    # Use provided raw_file or auto-detect latest
    if args.raw_file:
        raw_file = args.raw_file
        # Convert to absolute path if relative
        if not os.path.isabs(raw_file):
            raw_file = os.path.join(repo_root, raw_file)
        print(f"Using provided raw file: {raw_file}")
    else:
        raw_file = find_latest_raw_file(repo_root)
        print(f"Using auto-detected raw file: {raw_file}")
    
    # Load raw CSV
    df = pd.read_csv(raw_file)
    
    # Filter by target_date if provided
    stats = {
        "rows_total": len(df),
        "rows_kept": len(df),
        "rows_non_target": 0,
        "dates_present": [],
        "min_dt": None,
        "max_dt": None,
    }
    
    if target_date:
        target_df, non_target_rows, stats = filter_rows_by_target_date(df, target_date, raw_file, config=config)
        df = target_df
        
        if len(df) == 0:
            raise ValueError(f"No rows found for target date {target_date}. Cannot proceed with empty dataset.")
    else:
        # If no target_date, analyze dates present
        dates_series = df["Date/Time"].apply(parse_date) if "Date/Time" in df.columns else pd.Series([None] * len(df))
        def get_date_in_wat(dt) -> Optional[str]:
            try:
                if dt is None or pd.isna(dt):
                    return None
            except (TypeError, ValueError):
                return None
            try:
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=WAT_TZ)
                elif dt.tzinfo != WAT_TZ:
                    dt = dt.astimezone(WAT_TZ)
                return dt.date().strftime("%Y-%m-%d")
            except (AttributeError, ValueError, TypeError):
                return None
        date_strings = dates_series.apply(get_date_in_wat)
        date_strings_clean = date_strings.dropna()
        dates_present_list = sorted(date_strings_clean.unique().tolist()) if len(date_strings_clean) > 0 else []
        stats["dates_present"] = dates_present_list
        stats["min_dt"] = dates_present_list[0] if dates_present_list else None
        stats["max_dt"] = dates_present_list[-1] if dates_present_list else None
    
    # Transform using unified transform logic
    transformed = transform_dataframe_unified(df, config, target_date=target_date)
    
    # NOTE: Transformed spill system has been deprecated.
    # RAW spill handling is now managed by run_pipeline.py at the raw CSV level:
    # - Downloads are split by WAT date before transform
    # - Future rows become RAW spill files in uploads/spill_raw/
    # - RAW spill files are merged back when their date is processed
    # This prevents double-handling and keeps transform.py simple.
    
    # Extract normalized date for archiving
    if target_date:
        normalized_date = target_date
    else:
        if "*SalesReceiptDate" in transformed.columns:
            dates = transformed["*SalesReceiptDate"].dropna()
            if len(dates) > 0:
                try:
                    date_obj = datetime.strptime(dates.iloc[0], config.date_format)
                    normalized_date = date_obj.strftime("%Y-%m-%d")
                except ValueError:
                    normalized_date = datetime.now().strftime("%Y-%m-%d")
            else:
                normalized_date = datetime.now().strftime("%Y-%m-%d")
        else:
            normalized_date = datetime.now().strftime("%Y-%m-%d")
    
    # Write ONE QuickBooks-ready CSV in repo root
    base_name = os.path.splitext(os.path.basename(raw_file))[0]
    output_filename = f"{config.csv_prefix}_{base_name}.csv"
    output_path = os.path.join(repo_root, output_filename)
    
    transformed.to_csv(output_path, index=False)
    print(f"\nWrote combined QuickBooks file: {output_path}")
    print(f"Rows (including header): {len(transformed) + 1}")
    
    # Write metadata file for archiving
    # Determine source mode from raw file name (for diagnostics)
    raw_basename = os.path.basename(raw_file)
    if raw_basename.startswith("CombinedRaw_"):
        source_mode = "raw_combined"
    elif raw_basename.startswith("BookKeeping_") and "_to_" in raw_file:
        source_mode = "raw_split"
    else:
        source_mode = "raw_direct"
    
    metadata = {
        "raw_file": raw_basename,
        "raw_file_path": raw_file,
        "processed_files": [output_filename],
        "normalized_date": normalized_date,
        "target_date": target_date,
        "rows_total": stats["rows_total"],
        "rows_kept": stats["rows_kept"],
        "rows_non_target": stats["rows_non_target"],
        "dates_present": stats["dates_present"],
        "min_dt": stats["min_dt"],
        "max_dt": stats["max_dt"],
        "processed_at": datetime.now().isoformat(),
        "company_key": config.company_key,
        "grouping": config.group_by,
        "source_mode": source_mode,
    }
    
    metadata_path = os.path.join(repo_root, config.metadata_file)
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"Wrote metadata: {metadata_path}")


if __name__ == "__main__":
    main()
</file>

<file path="run_all_companies.py">
import argparse
import subprocess
import sys
from typing import Optional
from company_config import get_available_companies


def run(argv: Optional[list[str]] = None) -> int:
    parser = argparse.ArgumentParser(
        description="Run run_pipeline.py for all configured companies (sequentially)."
    )
    parser.add_argument(
        "--target-date",
        help="Target business date in YYYY-MM-DD format (for all companies). If omitted, each company run defaults to yesterday.",
    )
    parser.add_argument(
        "--from-date",
        help="Start date for range mode in YYYY-MM-DD format (must be used with --to-date).",
    )
    parser.add_argument(
        "--to-date",
        help="End date for range mode in YYYY-MM-DD format (must be used with --from-date).",
    )
    parser.add_argument(
        "--companies",
        nargs="*",
        help="Optional subset of companies to run (space-separated). Defaults to all configured companies.",
    )
    parser.add_argument(
        "--continue-on-failure",
        action="store_true",
        help="Continue running remaining companies even if one fails. Default is to stop on first failure.",
    )
    parser.add_argument(
        "--skip-download",
        action="store_true",
        help="Skip EPOS download and use existing split files in uploads/range_raw/ (range mode only).",
    )

    args = parser.parse_args(argv)

    # Validation: --from-date and --to-date must be provided together
    if (args.from_date is None) != (args.to_date is None):
        parser.error("--from-date and --to-date must be provided together")
    
    # Validation: --skip-download only works in range mode
    if args.skip_download and (args.from_date is None or args.to_date is None):
        parser.error("--skip-download can only be used with --from-date and --to-date (range mode)")

    all_companies = [c for c in get_available_companies() if not c.endswith("_example")]
    if not all_companies:
        print("No runnable companies found. Exiting.")
        return 1

    # Optional subset filtering
    if args.companies:
        requested = set(args.companies)
        companies = [c for c in all_companies if c in requested]
        missing = sorted(list(requested - set(companies)))
        if missing:
            print(f"[WARN] Ignoring unknown companies: {', '.join(missing)}")
    else:
        companies = all_companies

    if not companies:
        print("No runnable companies selected. Exiting.")
        return 1

    # Build common date args to forward to run_pipeline.py
    forwarded_date_args: list = []
    if args.from_date and args.to_date:
        forwarded_date_args.extend(["--from-date", args.from_date, "--to-date", args.to_date])
    elif args.target_date:
        forwarded_date_args.extend(["--target-date", args.target_date])
    
    # Forward --skip-download if provided
    if args.skip_download:
        forwarded_date_args.append("--skip-download")

    failures: list = []

    for company in companies:
        if args.from_date and args.to_date:
            print(f"\n=== Running pipeline for {company} (range {args.from_date} to {args.to_date}) ===")
        elif args.target_date:
            print(f"\n=== Running pipeline for {company} (target-date {args.target_date}) ===")
        else:
            print(f"\n=== Running pipeline for {company} (yesterday) ===")

        cmd = [sys.executable, "run_pipeline.py", "--company", company] + forwarded_date_args
        result = subprocess.run(cmd)

        if result.returncode != 0:
            msg = f"Pipeline failed for {company} (exit code {result.returncode})."
            print(f"[ERROR] {msg}")
            failures.append(company)
            if not args.continue_on_failure:
                return result.returncode

    if failures:
        print(f"\nCompleted with failures: {', '.join(failures)}")
        return 1

    print("\nAll company pipelines completed successfully ✅")
    return 0


if __name__ == "__main__":
    raise SystemExit(run())
</file>

<file path=".env.example">
# QuickBooks Online App Credentials (shared by both companies)
# Get these from your Intuit Developer app at https://developer.intuit.com
QBO_CLIENT_ID=your_client_id_here
QBO_CLIENT_SECRET=your_client_secret_here

# This section is for getting token info from the windows computer (optional)
# Only needed if you're using a token broker setup
QBO_TOKEN_BROKER_URL=http://127.0.0.1:8765/token
QBO_TOKEN_BROKER_KEY=your-broker-key-here

# Company A (AKPONORA VENTURES LTD.) EPOS Credentials
EPOS_USERNAME_A=your_company_a_epos_username
EPOS_PASSWORD_A=your_company_a_epos_password

# Company B (GOLDPLATES FEASTHOUSE LTD.) EPOS Credentials
EPOS_USERNAME_B=your_company_b_epos_username
EPOS_PASSWORD_B=your_company_b_epos_password

# Company A (AKPONORA VENTURES LTD.) Slack Webhook (optional)
SLACK_WEBHOOK_URL_A=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Company B (GOLDPLATES FEASTHOUSE LTD.) Slack Webhook (optional)
SLACK_WEBHOOK_URL_B=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Note: QBO_REALM_ID is no longer needed here - it's now in companies/company_a.json and companies/company_b.json
# Note: The .env.gp file is no longer needed - all credentials are now in this single .env file
</file>

<file path="epos_playwright.py">
import re
import sys
import argparse
from datetime import datetime, timedelta
from playwright.sync_api import Playwright, sync_playwright, expect
import os

# Load .env file if it exists (makes credential management easier)
from load_env import load_env_file
from company_config import load_company_config, get_available_companies

load_env_file()


def navigate_to_month(page, target_date: str) -> None:
    """Navigate calendar to the correct month if needed."""
    target_dt = datetime.strptime(target_date, "%Y-%m-%d")
    target_month_year = target_dt.strftime("%B %Y")
    target_dt_month = datetime.strptime(target_month_year, "%B %Y")
    
    page.wait_for_timeout(500)
    calendar_title = page.locator('.ajax__calendar_title:visible, td.title:visible, th.title:visible').first
    
    if calendar_title.count() == 0:
        return
    
    try:
        current_text = calendar_title.inner_text().strip().replace(",", "").strip()
        if target_month_year in current_text:
            return
        
        current_dt = datetime.strptime(current_text, "%B %Y")
        prev_btn = page.locator('.ajax__calendar_prev:visible, a.ajax__calendar_prev:visible, a[title*="Previous" i]:visible, a[title*="Prev" i]:visible').first
        next_btn = page.locator('.ajax__calendar_next:visible, a.ajax__calendar_next:visible, a[title*="Next" i]:visible').first
        
        for _ in range(24):  # Max 2 years
            current_text = calendar_title.inner_text().strip().replace(",", "").strip()
            if target_month_year in current_text:
                break
            
            try:
                current_dt = datetime.strptime(current_text, "%B %Y")
            except ValueError:
                break
            
            btn = prev_btn if current_dt > target_dt_month else next_btn
            if btn.count() > 0:
                btn.click()
                page.wait_for_timeout(400)
                prev_btn = page.locator('.ajax__calendar_prev:visible, a.ajax__calendar_prev:visible, a[title*="Previous" i]:visible, a[title*="Prev" i]:visible').first
                next_btn = page.locator('.ajax__calendar_next:visible, a.ajax__calendar_next:visible, a[title*="Next" i]:visible').first
            else:
                break
    except Exception:
        pass


def click_date_simple(page, target_date: str) -> None:
    """Click a date in the calendar - navigate to correct month first, then find by title."""
    target_dt = datetime.strptime(target_date, "%Y-%m-%d")
    day_number = str(target_dt.day)
    target_titles = [
        target_dt.strftime("%d %B %Y"),      # "10 November 2025"
        target_dt.strftime("%d %B, %Y"),     # "10 November, 2025"
    ]
    
    navigate_to_month(page, target_date)
    page.wait_for_timeout(500)
    
    # Try to find by title attribute (most reliable)
    for title in target_titles:
        day = page.locator(f'[id*="day"][title="{title}"]:visible').first
        if day.count() > 0:
            day.click()
            page.wait_for_timeout(200)
            return
    
    # Fallback: find by day number and verify title matches
    for day_elem in page.locator(f'[id*="day"]:visible').all():
        try:
            if day_elem.inner_text().strip() == day_number:
                day_title = day_elem.get_attribute("title") or ""
                if any(title in day_title for title in target_titles):
                    day_elem.click()
                    page.wait_for_timeout(200)
                    return
        except:
            continue
    
    raise RuntimeError(f"Could not find calendar day for date {target_date}")


def get_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Download EPOS CSV for a specific company and date."
    )
    parser.add_argument(
        "--company",
        required=True,
        choices=get_available_companies(),
        help="Company identifier (REQUIRED). Available: %(choices)s",
    )
    parser.add_argument(
        "--target-date",
        help="Target business date in YYYY-MM-DD format (default: yesterday, ignored if --from-date and --to-date are provided)",
    )
    parser.add_argument(
        "--from-date",
        help="Start date for range in YYYY-MM-DD format (must be used with --to-date)",
    )
    parser.add_argument(
        "--to-date",
        help="End date for range in YYYY-MM-DD format (must be used with --from-date)",
    )
    args = parser.parse_args()
    
    # Validation: --from-date and --to-date must be provided together
    if (args.from_date is None) != (args.to_date is None):
        parser.error("--from-date and --to-date must be provided together")
    
    return args


def run(playwright: Playwright, config, from_date: str = None, to_date: str = None, target_date: str = None) -> None:
    # Get credentials from company config
    try:
        epos_username = config.epos_username
        epos_password = config.epos_password
    except RuntimeError as e:
        raise RuntimeError(
            f"Failed to get EPOS credentials for {config.display_name}: {e}\n"
            f"Please set {config._data['epos']['username_env_key']} and "
            f"{config._data['epos']['password_env_key']} in your .env file."
        )
    
    # Determine date range: prefer from_date/to_date if provided, else use target_date
    if from_date and to_date:
        date_from = from_date
        date_to = to_date
        print(f"Downloading EPOS CSV for {config.display_name} (range: {date_from} to {date_to})")
    elif target_date:
        date_from = target_date
        date_to = target_date
        print(f"Downloading EPOS CSV for {config.display_name} (date: {target_date})")
    else:
        # Default to yesterday
        date_from = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        date_to = date_from
        print(f"Downloading EPOS CSV for {config.display_name} (date: {date_from})")
    
    browser = playwright.chromium.launch(headless=True)
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://www.eposnowhq.com/Pages/Reporting/SageReport.aspx")
    page.get_by_role("textbox", name="Username or email address").click()
    page.get_by_role("textbox", name="Username or email address").fill(epos_username)
    page.get_by_role("textbox", name="Password").click()
    page.get_by_role("textbox", name="Password").fill(epos_password)
    page.get_by_role("button", name="Log in").click()
    
    # Select Custom date range
    page.get_by_label("Show data from").select_option("Custom")
    
    # FROM date
    page.locator("#MainContent_timeControl_btnFromDate").click()
    page.wait_for_timeout(500)
    click_date_simple(page, date_from)
    
    # TO date
    page.locator("#MainContent_timeControl_btnToDate").click()
    page.wait_for_timeout(500)
    click_date_simple(page, date_to)
    
    # Apply date range
    page.locator("#MainContent_timeControl_btnApplyDate").click()
    page.wait_for_timeout(500)
    
    # Download CSV
    # For large date ranges, downloads can take longer - increase timeout and don't wait for navigation
    with page.expect_download(timeout=150000) as download_info:  # 2 minute timeout for large downloads
        page.get_by_role("button", name="Export to .csv").click(timeout=30000, no_wait_after=True)
    download = download_info.value

    # Determine repo root by using the current script's directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    repo_root = os.path.abspath(script_dir)

    filename = download.suggested_filename
    save_path = os.path.join(repo_root, filename)
    download.save_as(save_path)

    # ---------------------
    context.close()
    browser.close()


if __name__ == "__main__":
    args = get_args()
    
    # Load company configuration
    try:
        config = load_company_config(args.company)
    except Exception as e:
        print(f"Error: Failed to load company config for '{args.company}': {e}")
        sys.exit(1)
    
    # Determine date parameters: if both from_date and to_date are provided, use them and ignore target_date
    from_date = args.from_date
    to_date = args.to_date
    target_date = None
    
    if from_date and to_date:
        # Range mode: ignore target_date
        target_date = None
    elif args.target_date:
        # Single day mode
        target_date = args.target_date
    else:
        # Default to yesterday
        target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        print(f"No --target-date provided, using yesterday: {target_date}")
    
    with sync_playwright() as playwright:
        run(playwright, config, from_date=from_date, to_date=to_date, target_date=target_date)
</file>

<file path="slack_notify.py">
import os
import re
import json
import csv
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List
import urllib.request
import ssl

try:
    import certifi
except ImportError:  # pragma: no cover - best effort
    certifi = None


def send_slack_success(message: str, webhook_url: str = None) -> None:
    """
    Send a Slack notification.
    
    Args:
        message: Message to send
        webhook_url: Optional webhook URL. If not provided, falls back to SLACK_WEBHOOK_URL env var.
    """
    if not webhook_url:
        webhook_url = os.getenv("SLACK_WEBHOOK_URL")

    if not webhook_url:
        logging.info("Slack webhook URL not set, skipping Slack notification.")
        return

    payload = {
        "text": message
    }

    data = json.dumps(payload).encode("utf-8")

    # Build SSL context with system certs; fall back to certifi if available.
    context = ssl.create_default_context()
    if certifi:
        try:
            context.load_verify_locations(certifi.where())
        except Exception as e:  # pragma: no cover
            logging.warning(f"Could not load certifi certs: {e}")

    req = urllib.request.Request(
        webhook_url,
        data=data,
        headers={"Content-Type": "application/json"},
    )

    try:
        with urllib.request.urlopen(req, context=context) as resp:
            logging.info(f"Slack message sent (status {resp.status})")
    except Exception as e:
        logging.error(f"Failed to send Slack message: {e}")


def notify_pipeline_update(
    pipeline_name: str,
    log_file: Path,
    summary: Dict[str, Any],
    webhook_url: str = None
) -> None:
    """
    Send a state-based watchdog/update message (sent ONCE if noteworthy).
    
    This is NOT a timer/heartbeat. Only call when there are warnings/anomalies
    that warrant mid-run notification (e.g., spill files, duplicates, partial failures).
    
    Args:
        pipeline_name: Name of the pipeline
        log_file: Path to log file
        summary: Dictionary containing update information (phase, warnings, etc.)
        webhook_url: Optional webhook URL
    """
    message = format_run_summary(pipeline_name, log_file, summary, status="update")
    send_slack_success(message, webhook_url)


def notify_pipeline_success(
    pipeline_name: str,
    log_file: Path,
    date_range: str = None,
    metadata: dict = None,
    webhook_url: str = None
) -> None:
    """
    Send a Slack notification when the pipeline completes successfully.
    
    Args:
        pipeline_name: Name of the pipeline
        log_file: Path to log file
        date_range: Optional date range string (for backward compatibility)
        metadata: Optional metadata dict with summary information
        webhook_url: Optional webhook URL
    """
    # Build summary from metadata and date_range
    summary = {}
    if metadata:
        summary.update(metadata)
    if date_range and not summary.get("target_date") and not summary.get("date_range"):
        summary["date_range"] = date_range
    
    message = format_run_summary(pipeline_name, log_file, summary, status="success")
    send_slack_success(message, webhook_url)


def _summarize_blockers_csv(repo_root: Path, company_key: str, target_date: str) -> Optional[str]:
    """
    If the inventory_start_date_blockers CSV exists for this run, read it and return
    a short summary (row count + sample items). Used for Slack when 6270 rejections occurred.
    """
    if not company_key or not target_date:
        return None
    safe_key = re.sub(r"[^\w-]", "_", company_key)
    filename = f"inventory_start_date_blockers_{safe_key}_{target_date}.csv"
    filepath = repo_root / "reports" / filename
    if not filepath.exists():
        return None
    try:
        with open(filepath, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        if not rows:
            return None
        # Unique (DocNumber, ItemName, InvStartDate) for a concise summary
        seen = set()
        sample_items: List[str] = []
        for r in rows:
            name = (r.get("ItemName") or "").strip()
            inv_date = (r.get("InvStartDate") or "").strip()
            key = (name, inv_date)
            if key not in seen and inv_date and inv_date != "(missing)":
                seen.add(key)
                sample_items.append(f"{name} ({inv_date})")
                if len(sample_items) >= 5:
                    break
        count = len(rows)
        if count == 0:
            return None
        summary = f"{count} blocker row(s)"
        if sample_items:
            summary += f" — e.g. {', '.join(sample_items)}"
        summary += f"\n  Report: `reports/{filename}`"
        return summary
    except Exception as e:
        logging.warning(f"Could not read blockers CSV for summary: {e}")
        return None


def format_run_summary(
    pipeline_name: str,
    log_file: Path,
    summary: Dict[str, Any],
    status: str,
    error: Optional[str] = None
) -> str:
    """
    Format a consolidated run summary message for Slack.
    
    Args:
        pipeline_name: Name of the pipeline
        log_file: Path to log file
        summary: Dictionary containing run summary data
        status: One of "success", "failure", "update"
        error: Error message (for failure status)
    
    Returns:
        Formatted Slack message string
    """
    # Status headers
    if status == "success":
        header = f"✅ *{pipeline_name} completed*"
    elif status == "failure":
        header = f"❌ *{pipeline_name} failed*"
    elif status == "update":
        header = f"⚠️ *{pipeline_name} update*"
    else:
        header = f"*{pipeline_name}*"
    
    message = f"{header}\n"
    message += f"• Time: {datetime.now().isoformat(timespec='seconds')}\n"
    message += f"• Log: `{log_file.name}`\n"
    
    # Date information
    if summary.get("target_date"):
        message += f"• Target Date: {summary['target_date']}\n"
    elif summary.get("date_range"):
        message += f"• Date Range: {summary['date_range']}\n"
    
    # Phase information (for update/failure)
    if status in ("update", "failure"):
        if summary.get("phase"):
            message += f"• Phase: {summary['phase']}\n"
        if summary.get("phase_status"):
            message += f"• Status: {summary['phase_status']}\n"
        if status == "failure" and summary.get("phase_failed"):
            message += f"• Phase Failed: {summary['phase_failed']}\n"
    
    # Failure reason
    if status == "failure" and error:
        reason = extract_error_reason(error)
        message += f"• Reason: {reason}\n"
    
    # Row statistics
    rows_kept = summary.get("rows_kept")
    rows_spilled = summary.get("rows_spilled")
    rows_total = summary.get("rows_total")
    if rows_total is not None:
        message += f"• Rows: {rows_kept} kept, {rows_spilled} spilled (total: {rows_total})\n"
    
    # Spill files
    spill_files = summary.get("spill_files", [])
    if spill_files:
        message += f"• Spill Files: {len(spill_files)} file(s)\n"
    
    # Upload statistics
    upload_stats = summary.get("upload_stats")
    if upload_stats:
        attempted = upload_stats.get("attempted", 0)
        uploaded = upload_stats.get("uploaded", 0)
        skipped = upload_stats.get("skipped", 0)
        failed = upload_stats.get("failed", 0)
        stale_ledger = upload_stats.get("stale_ledger_entries_detected", 0)
        message += f"• Upload: {uploaded} uploaded, {skipped} skipped, {failed} failed (attempted: {attempted})\n"
        if stale_ledger > 0:
            message += f"• Stale ledger entries detected: {stale_ledger} (healed by uploading)\n"
        
        # Inventory statistics
        items_created = upload_stats.get("items_created_count", 0)
        inventory_warnings = upload_stats.get("inventory_warnings_count", 0)
        inventory_rejections = upload_stats.get("inventory_rejections_count", 0)
        inventory_start_date_issues = upload_stats.get("inventory_start_date_issues_count", 0)
        target_date = summary.get("target_date", "")
        if items_created > 0 or inventory_warnings > 0 or inventory_rejections > 0:
            message += f"• Inventory: {items_created} items created"
            if inventory_warnings > 0:
                message += f", {inventory_warnings} warnings"
            if inventory_rejections > 0:
                message += f", {inventory_rejections} rejections"
            message += "\n"
        if inventory_start_date_issues > 0 and target_date:
            message += f"• Inventory StartDate: {inventory_start_date_issues} items have InvStartDate after {target_date}\n"
        # If we had rejections or failures (e.g. 6270 InvStartDate), include blockers CSV summary when present
        if (inventory_rejections > 0 or failed > 0) and target_date:
            repo_root = Path(__file__).resolve().parent
            company_key = summary.get("company_key", "")
            blockers_summary = _summarize_blockers_csv(repo_root, company_key, target_date)
            if blockers_summary:
                message += f"• InvStartDate blockers (6270): {blockers_summary}\n"
    
    # Reconciliation
    reconcile = summary.get("reconcile")
    if reconcile:
        reconcile_status = reconcile.get("status", "NOT RUN")
        if reconcile_status == "MATCH":
            message += f"• Reconciliation: MATCH\n"
        elif reconcile_status == "MISMATCH":
            message += f"• ⚠️ Reconciliation: MISMATCH\n"
        else:
            message += f"• Reconciliation: NOT RUN\n"
        
        if reconcile_status != "NOT RUN":
            epos_total = reconcile.get("epos_total", 0)
            epos_count = reconcile.get("epos_count", 0)
            qbo_total = reconcile.get("qbo_total", 0)
            qbo_count = reconcile.get("qbo_count", 0)
            difference = reconcile.get("difference", 0)
            
            message += f"  – EPOS: ₦{epos_total:,.2f} ({epos_count} receipts)\n"
            message += f"  – QBO: ₦{qbo_total:,.2f} ({qbo_count} receipts)\n"
            message += f"  – Difference: ₦{difference:,.2f}\n"
        else:
            reason_not_run = reconcile.get("reason", "upload incomplete")
            message += f"  – Reconciliation not run ({reason_not_run})\n"
    elif status == "failure":
        # If failure and no reconcile data, indicate it wasn't run
        message += f"• Reconciliation: NOT RUN\n"
        message += f"  – Reconciliation not run (upload incomplete)\n"
    
    # Trading day boundary stats (if available)
    trading_day_stats = summary.get("trading_day_stats")
    if trading_day_stats:
        cutoff = trading_day_stats.get("cutoff", "05:00")
        by_date = trading_day_stats.get("by_date", {})
        
        # For single-day or per-day summaries, show stats for the specific date
        target_date = summary.get("target_date")
        if target_date and target_date in by_date:
            day_stats = by_date[target_date]
            pre_cutoff = day_stats.get("pre_cutoff_reassigned", 0)
            if pre_cutoff > 0:
                message += f"• Trading-day adjustment: {pre_cutoff} row(s) from next calendar day (pre-cutoff) assigned to {target_date} (cutoff={cutoff} WAT)\n"
        # For range mode final summary, show aggregate or per-day stats
        elif by_date:
            # Show stats for all dates in range
            total_reassigned = sum(stats.get("pre_cutoff_reassigned", 0) for stats in by_date.values())
            if total_reassigned > 0:
                message += f"• Trading-day adjustment: {total_reassigned} total row(s) reassigned from next calendar day (cutoff={cutoff} WAT)\n"
                # Optionally show per-day breakdown (limit to 3 dates to avoid clutter)
                dates_with_reassigned = [
                    (date, stats.get("pre_cutoff_reassigned", 0))
                    for date, stats in by_date.items()
                    if stats.get("pre_cutoff_reassigned", 0) > 0
                ]
                if len(dates_with_reassigned) <= 3:
                    for date, count in dates_with_reassigned:
                        message += f"  – {date}: {count} row(s)\n"
    
    # Range Totals (only for range mode final summary)
    if status == "success" and summary.get("range_totals"):
        # Check if this is a range completion message (has from_date and to_date, or date_range contains "to")
        is_range_mode = (
            (summary.get("from_date") is not None and summary.get("to_date") is not None) or
            (summary.get("date_range") and " to " in str(summary.get("date_range")))
        )
        
        if is_range_mode:
            range_totals = summary["range_totals"]
            included_days = range_totals.get("included_days", 0)
            total_days = range_totals.get("total_days", 0)
            epos_total = range_totals.get("epos_total", 0)
            qbo_total = range_totals.get("qbo_total", 0)
            epos_count = range_totals.get("epos_count", 0)
            qbo_count = range_totals.get("qbo_count", 0)
            difference = range_totals.get("difference", 0)
            
            if included_days == total_days:
                message += f"• Range Totals (sum of per-day reconciliation):\n"
            else:
                message += f"• Range Totals (partial — {included_days}/{total_days} days included):\n"
            
            message += f"  – EPOS: ₦{epos_total:,.2f} ({epos_count} receipts)\n"
            message += f"  – QBO: ₦{qbo_total:,.2f} ({qbo_count} receipts)\n"
            message += f"  – Difference: ₦{difference:,.2f}\n"
    
    # Warnings/Notes (for update messages)
    warnings = summary.get("warnings", [])
    if warnings and len(warnings) > 0:
        message += f"• Notes:\n"
        for warning in warnings[:6]:  # Limit to 6 warnings
            message += f"  – {warning}\n"
    
    return message


def notify_pipeline_start(
    pipeline_name: str,
    log_file: Path,
    date_range: str = None,
    webhook_url: str = None,
    metadata: Dict[str, Any] = None
) -> None:
    """
    Send a Slack notification when the pipeline starts.
    
    Args:
        pipeline_name: Name of the pipeline
        log_file: Path to log file
        date_range: Optional date range string
        webhook_url: Optional webhook URL
        metadata: Optional metadata dict with target_date, company_key, etc.
    """
    summary = {}
    if metadata:
        summary.update(metadata)
    if date_range:
        summary["date_range"] = date_range
    
    message = (
        f"🚀 *{pipeline_name} started*\n"
        f"• Time: {datetime.now().isoformat(timespec='seconds')}\n"
        f"• Log: `{log_file.name}`"
    )
    
    if summary.get("target_date"):
        message += f"\n• Target Date: {summary['target_date']}"
    elif summary.get("date_range"):
        message += f"\n• Date Range: {summary['date_range']}"
    
    send_slack_success(message, webhook_url)


def extract_error_reason(error: str) -> str:
    """
    Extract a concise, user-friendly reason from an error message.
    Returns a professional summary of the error.
    Updated for multi-company + SQLite setup.
    """
    error_lower = error.lower()
    error_original = error  # Keep original for exact matches
    
    # Duplicate receipt errors
    if "duplicate" in error_lower and ("docnumber" in error_lower or "document number" in error_lower):
        return "Duplicate receipt detected (DocNumber already exists in QBO)."
    
    # Line validation errors
    if "amount must equal" in error_lower and ("unitprice" in error_lower or "qty" in error_lower):
        return "Line validation failed (Amount must equal UnitPrice × Qty)."
    
    # Department/location mapping errors
    if "department" in error_lower and ("not found" in error_lower or "invalid" in error_lower or "mapping" in error_lower):
        return "Missing/invalid Department mapping for this location."
    
    # Token-related errors (updated for SQLite)
    if "invalid_grant" in error_lower or "invalid refresh token" in error_lower:
        return "Invalid refresh token. Re-authenticate via OAuth flow and update qbo_tokens.sqlite."
    if "invalid_client" in error_lower or "qbo_client_id" in error_lower or "qbo_client_secret" in error_lower:
        return "Invalid QuickBooks credentials. Check QBO_CLIENT_ID and QBO_CLIENT_SECRET in .env file."
    if "qbo_tokens.sqlite" in error_lower and ("not found" in error_lower or "empty" in error_lower or "no tokens found" in error_lower):
        return "No tokens found in qbo_tokens.sqlite. Run OAuth flow first using --company selection."
    if "refresh token" in error_lower and ("expired" in error_lower or "invalid" in error_lower):
        return "Refresh token expired or invalid. Re-authenticate via OAuth flow for this company."
    if "company_key" in error_lower or "realm_id" in error_lower:
        if "not found" in error_lower or "missing" in error_lower:
            return "Company configuration error. Use --company selection (company_a or company_b)."
    
    # File-related errors
    if "file not found" in error_lower or "no such file" in error_lower:
        if "csv" in error_lower:
            return "Required CSV file not found. Check if EPOS download completed successfully."
        return "Required file not found. Check pipeline logs for details."
    if "single_sales_receipts" in error_lower or "gp_sales_receipts" in error_lower:
        return "Processed CSV file not found. Phase 2 (transformation) may have failed."
    
    # Network/API errors
    if "connection" in error_lower or "network" in error_lower or "timeout" in error_lower:
        return "Network connection error. Check internet connectivity and try again."
    if "401" in error or "unauthorized" in error_lower:
        return "Authentication failed. Check QuickBooks credentials and tokens."
    if "403" in error or "forbidden" in error_lower:
        return "Access forbidden. Check QuickBooks API permissions."
    if "429" in error or "rate limit" in error_lower:
        return "API rate limit exceeded. Please wait before retrying."
    
    # Phase-specific errors
    if "phase 1" in error_lower or "epos_playwright" in error_lower:
        return "EPOS download failed. Check EPOS credentials and website accessibility."
    if "phase 2" in error_lower or "transform" in error_lower:
        return "CSV transformation failed. Check input file format and data."
    if "phase 3" in error_lower or "qbo_upload" in error_lower:
        return "QuickBooks upload failed. Check API credentials and data format."
    
    # Generic fallback - extract first meaningful line
    lines = error.split('\n')
    for line in lines:
        line = line.strip()
        if line and not line.startswith('Traceback') and not line.startswith('File'):
            # Limit length
            if len(line) > 150:
                line = line[:147] + "..."
            return line
    
    return "Pipeline failed. Check logs for details."


def notify_pipeline_failure(
    pipeline_name: str,
    log_file: Path,
    error: str,
    date_range: str = None,
    webhook_url: str = None,
    metadata: dict = None
) -> None:
    """
    Send a Slack notification when the pipeline fails.
    
    Args:
        pipeline_name: Name of the pipeline
        log_file: Path to log file
        error: Error message or exception string
        date_range: Optional date range string (for backward compatibility)
        webhook_url: Optional webhook URL
        metadata: Optional metadata dict with summary information
    """
    # Build summary from metadata and date_range
    summary = {}
    if metadata:
        summary.update(metadata)
    if date_range and not summary.get("target_date") and not summary.get("date_range"):
        summary["date_range"] = date_range
    
    message = format_run_summary(pipeline_name, log_file, summary, status="failure", error=error)
    send_slack_success(message, webhook_url)
</file>

<file path="qbo_upload.py">
from __future__ import annotations

import os
import glob
import json
import argparse
import sys
import re
import csv
from typing import Optional, Dict, Callable, Any, Tuple, List
from urllib.parse import quote
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
import requests
from load_env import load_env_file
from company_config import load_company_config, get_available_companies
from token_manager import get_access_token, refresh_access_token, verify_realm_match

# Load .env if present so QBO_* vars are available (shared secrets only)
load_env_file()

BASE_URL = "https://quickbooks.api.intuit.com"

# Tax code id for your 7.5% VAT ("7.5% S")
TAX_CODE_ID = "2"

# Legacy PaymentMethod mapping (Company A) - kept for backward compatibility
# Note: PaymentMethods are now queried from QBO by name per company
LEGACY_PAYMENT_METHOD_BY_NAME = {
    "Card": "5",
    "Cash": "1",
    "Cash/Transfer": "8",
    "Cheque": "2",
    "Credit Card": "3",
    "Direct Debit": "4",
    "Transfer": "6",
    "Card/Transfer": "9",
    "Card/Cash": "7",
    "Card/Cash/Transfer": "10",
}

# CSV column names
AMOUNT_COL = "*ItemAmount"        # GROSS line amount (inclusive of tax) from EPOS
DATE_COL = "*SalesReceiptDate"
MEMO_COL = "Memo"
DOCNUM_COL = "*SalesReceiptNo"
GROUP_COL = "*SalesReceiptNo"
LOCATION_COL = "Location"         # Location name from CSV

# Detail columns
ITEM_NAME_COL = "Item(Product/Service)"  # Product/Service name in QBO
ITEM_DESC_COL = "ItemDescription"        # Line description
QTY_COL = "ItemQuantity"                 # Quantity sold
RATE_COL = "ItemRate"                    # Unit price (can be NaN)
SERVICE_DATE_COL = "Service Date"        # Per-line service date
TAX_AMOUNT_COL = "ItemTaxAmount"        # Per-line tax amount from EPOS (7.5% VAT)

# Item mapping / creation behaviour
DEFAULT_ITEM_ID = "1"           # Fallback generic item
DEFAULT_INCOME_ACCOUNT_ID = "1" # For auto-created items
AUTO_CREATE_ITEMS = True       # Flip to True if you ever want auto item creation


def get_repo_root() -> str:
    """Return the directory this script lives in (the repo root for our purposes)."""
    return os.path.dirname(os.path.abspath(__file__))


def load_uploaded_docnumbers(repo_root: str, config) -> set:
    """Load set of DocNumbers that have been successfully uploaded."""
    ledger_path = os.path.join(repo_root, config.uploaded_docnumbers_file)
    if not os.path.exists(ledger_path):
        return set()
    
    try:
        with open(ledger_path, "r") as f:
            data = json.load(f)
            return set(data.get("docnumbers", []))
    except Exception as e:
        print(f"[WARN] Failed to load {config.uploaded_docnumbers_file}: {e}")
        return set()


def save_uploaded_docnumber(repo_root: str, docnumber: str, config) -> None:
    """Add a DocNumber to the uploaded ledger."""
    ledger_path = os.path.join(repo_root, config.uploaded_docnumbers_file)
    
    # Load existing
    docnumbers = load_uploaded_docnumbers(repo_root, config)
    docnumbers.add(docnumber)
    
    # Save back
    data = {
        "docnumbers": sorted(list(docnumbers)),
        "last_updated": datetime.now().isoformat(),
    }
    
    try:
        with open(ledger_path, "w") as f:
            json.dump(data, f, indent=2)
    except Exception as e:
        print(f"[WARN] Failed to save {config.uploaded_docnumbers_file}: {e}")


def check_qbo_existing_docnumbers(
    docnumbers: list[str],
    token_mgr: TokenManager,
    realm_id: str,
    batch_size: int = 50,
    target_date: Optional[str] = None
) -> Tuple[set, dict]:
    """
    Check QBO for existing SalesReceipts by DocNumber.
    
    Args:
        docnumbers: List of DocNumbers to check
        token_mgr: TokenManager instance
        realm_id: QBO Realm ID
        batch_size: Batch size for queries
        target_date: Optional target date (YYYY-MM-DD). If provided, only receipts with matching TxnDate are considered "existing".
    
    Returns:
        Tuple of (existing_docnumbers, date_mismatches):
        - existing_docnumbers: Set of DocNumbers that exist in QBO with matching TxnDate (or any TxnDate if target_date not provided)
        - date_mismatches: Dict {DocNumber: TxnDate} for receipts that exist but have different TxnDate
    """
    existing = set()
    date_mismatches = {}
    
    # Query in batches to avoid URL length limits
    for i in range(0, len(docnumbers), batch_size):
        batch = docnumbers[i:i + batch_size]
        # Build query: select Id, DocNumber, TxnDate from SalesReceipt where DocNumber in ('SR-...', 'SR-...', ...)
        docnumber_list = "', '".join(d.replace("'", "''") for d in batch)
        query = f"select Id, DocNumber, TxnDate from SalesReceipt where DocNumber in ('{docnumber_list}')"
        url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
        
        resp = _make_qbo_request("GET", url, token_mgr)
        if resp.status_code == 200:
            data = resp.json()
            receipts = data.get("QueryResponse", {}).get("SalesReceipt", [])
            if not isinstance(receipts, list):
                receipts = [receipts] if receipts else []
            
            for receipt in receipts:
                doc_num = receipt.get("DocNumber")
                if not doc_num:
                    continue
                
                txn_date = receipt.get("TxnDate")
                
                # If target_date is provided, only consider it "existing" if TxnDate matches
                if target_date and txn_date:
                    if txn_date == target_date:
                        existing.add(doc_num)
                    else:
                        # Receipt exists but with different TxnDate - this is a date mismatch
                        date_mismatches[doc_num] = txn_date
                else:
                    # No target_date provided - consider any match as existing
                    existing.add(doc_num)
    
    return existing, date_mismatches


def find_latest_single_csv(repo_root: str, config) -> str:
    """
    Find the most recently modified CSV file matching company's prefix pattern.
    """
    pattern = os.path.join(repo_root, f"{config.csv_prefix}_*.csv")
    files = glob.glob(pattern)
    if not files:
        raise FileNotFoundError(
            f"No {config.csv_prefix}_*.csv files found in {repo_root}"
        )
    return max(files, key=os.path.getmtime)


def get_payment_method_id_by_name(name: str, token_mgr: TokenManager, realm_id: str, cache: Dict[str, Optional[str]]) -> Optional[str]:
    """
    Resolve a PaymentMethod name to a PaymentMethod Id with simple caching.
    
    - If the name exists in cache, reuse its Id.
    - Otherwise, try a QBO query by Name.
    - Returns None if payment method not found or name is empty.
    """
    if not name or not name.strip():
        return None
    
    name_clean = name.strip()
    if name_clean in cache:
        return cache[name_clean]
    
    safe_name = name_clean.replace("'", "''")
    query = f"select Id, Name from PaymentMethod where Name = '{safe_name}'"
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    
    resp = _make_qbo_request("GET", url, token_mgr)
    payment_method_id: Optional[str] = None
    if resp.status_code == 200:
        data = resp.json()
        payment_methods = data.get("QueryResponse", {}).get("PaymentMethod", [])
        if payment_methods:
            payment_method_id = payment_methods[0].get("Id")
    
    if payment_method_id:
        cache[name_clean] = payment_method_id
    else:
        # Cache None to avoid repeated failed queries
        cache[name_clean] = None
    
    return payment_method_id


def infer_payment_method_id(memo: str, token_mgr: TokenManager = None, realm_id: str = None, cache: Dict[str, Optional[str]] = None) -> Optional[str]:
    """
    Try to map the memo text (tender type) to a QBO PaymentMethod Id.
    
    If token_mgr and realm_id are provided, queries QBO by name.
    Otherwise, falls back to legacy hardcoded mapping (Company A).
    
    Includes mapping for common variations (e.g., "Card" -> "Card payment").
    """
    if not memo:
        return None
    memo_clean = memo.strip()
    
    # Payment method name mapping (CSV value -> QBO name)
    # This handles cases where CSV uses different names than QBO
    PAYMENT_METHOD_MAPPING = {
        "Card": "Card payment",  # CSV "Card" -> QBO "Card payment"
    }
    
    # Map CSV value to QBO name if needed
    qbo_name = PAYMENT_METHOD_MAPPING.get(memo_clean, memo_clean)
    
    # If we have QBO access, query by name (preferred)
    if token_mgr and realm_id and cache is not None:
        # Try mapped name first
        payment_method_id = get_payment_method_id_by_name(qbo_name, token_mgr, realm_id, cache)
        if payment_method_id:
            return payment_method_id
        # If mapped name not found, try original name as fallback
        if qbo_name != memo_clean:
            payment_method_id = get_payment_method_id_by_name(memo_clean, token_mgr, realm_id, cache)
            if payment_method_id:
                return payment_method_id
        return None
    
    # Fallback to legacy mapping (backward compatibility)
    return LEGACY_PAYMENT_METHOD_BY_NAME.get(memo_clean)


def _qbo_headers(access_token: str) -> dict:
    return {
        "Authorization": f"Bearer {access_token}",
        "Accept": "application/json",
        "Content-Type": "application/json",
    }


class TokenManager:
    """
    Manages QBO access token state during a run.
    Automatically refreshes token on 401 errors.
    Uses token_manager for company-specific token isolation.
    """
    def __init__(self, company_key: str, realm_id: str):
        self.company_key = company_key
        self.realm_id = realm_id
        self.access_token = get_access_token(company_key, realm_id)
    
    def get(self) -> str:
        """Get the current access token."""
        return self.access_token
    
    def refresh(self) -> str:
        """Refresh the access token and update internal state."""
        tokens = refresh_access_token(self.company_key, self.realm_id)
        self.access_token = tokens["access_token"]
        return self.access_token


def _make_qbo_request(
    method: str,
    url: str,
    token_mgr: TokenManager,
    **kwargs
) -> requests.Response:
    """
    Make a QBO API request with automatic token refresh on 401 errors.
    
    Args:
        method: HTTP method ('GET', 'POST', etc.)
        url: Full URL for the request
        token_mgr: TokenManager instance to get/refresh tokens
        **kwargs: Additional arguments to pass to requests (headers, json, data, etc.)
    
    Returns:
        requests.Response object
    """
    # Ensure headers include the access token
    headers = kwargs.pop("headers", {})
    if "Authorization" not in headers:
        headers.update(_qbo_headers(token_mgr.get()))
    kwargs["headers"] = headers
    
    # Make the request
    resp = requests.request(method, url, **kwargs)
    
    # If we get a 401, refresh token and retry once
    if resp.status_code == 401:
        print("[INFO] Got 401, refreshing access token and retrying...")
        token_mgr.refresh()
        # Update headers with new token
        headers["Authorization"] = f"Bearer {token_mgr.get()}"
        kwargs["headers"] = headers
        resp = requests.request(method, url, **kwargs)
    
    return resp


def get_tax_code_id_by_name(name: str, token_mgr: TokenManager, realm_id: str, cache: Dict[str, Optional[str]]) -> Optional[str]:
    """
    Resolve a TaxCode name to a TaxCode Id with simple caching.
    
    - If the name exists in cache, reuse its Id.
    - Otherwise, try a QBO query by Name.
    - Returns None if tax code not found or name is empty.
    """
    if not name or not name.strip():
        return None
    
    name_clean = name.strip()
    if name_clean in cache:
        return cache[name_clean]
    
    safe_name = name_clean.replace("'", "''")
    query = f"select Id, Name from TaxCode where Name = '{safe_name}'"
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    
    resp = _make_qbo_request("GET", url, token_mgr)
    tax_code_id: Optional[str] = None
    if resp.status_code == 200:
        data = resp.json()
        tax_codes = data.get("QueryResponse", {}).get("TaxCode", [])
        if tax_codes:
            tax_code_id = tax_codes[0].get("Id")
    
    if tax_code_id:
        cache[name_clean] = tax_code_id
    else:
        # Cache None to avoid repeated failed queries
        cache[name_clean] = None
    
    return tax_code_id


def load_category_account_mapping(config) -> Dict[str, Dict[str, str]]:
    """
    Load category → account mapping from Product.Mapping.csv.
    Tolerates header variants (Categories/Category, Cost of Sale Account/COGS, etc.)
    and strips whitespace. Ignores unnamed columns.
    
    Args:
        config: CompanyConfig instance
    
    Returns:
        Dict mapping normalized category to account names:
        {category_normalized: {asset: "...", income: "...", expense: "..."}}
    
    Raises:
        FileNotFoundError: If mapping file doesn't exist
        ValueError: If CSV is malformed or required columns missing
    """
    mapping_file = config.product_mapping_file
    
    if not mapping_file.exists():
        raise FileNotFoundError(
            f"Product mapping file not found: {mapping_file}. "
            f"Please ensure mappings/Product.Mapping.csv exists or set product_mapping_file in config."
        )
    
    try:
        df = pd.read_csv(mapping_file)
    except Exception as e:
        raise ValueError(f"Failed to read Product.Mapping.csv: {e}")
    
    # Normalize column names: strip, lowercase for matching
    def norm(col: str) -> str:
        return str(col).strip().lower()
    
    # Synonyms -> canonical names
    synonym_to_canonical = {
        "category": "category",
        "categories": "category",
        "inventory account": "inventory account",
        "revenue account": "revenue account",
        "cost of sale account": "cost of sale account",
        "cost of sale": "cost of sale account",
        "cogs": "cost of sale account",
    }
    required_canonicals = {"category", "inventory account", "revenue account", "cost of sale account"}
    
    # Build canonical -> actual column name (first match wins)
    canonical_to_actual: Dict[str, str] = {}
    detected_headers = list(df.columns)
    
    for actual_col in df.columns:
        n = norm(actual_col)
        if not n or re.match(r"^unnamed", n):
            continue
        canonical = synonym_to_canonical.get(n)
        if canonical and canonical not in canonical_to_actual:
            canonical_to_actual[canonical] = actual_col
    
    missing = required_canonicals - set(canonical_to_actual.keys())
    if missing:
        raise ValueError(
            f"Product.Mapping.csv missing required columns (after normalization): {', '.join(sorted(missing))}. "
            f"Detected headers: {detected_headers}. "
            f"Canonical mapping used: {canonical_to_actual}"
        )
    
    category_col = canonical_to_actual["category"]
    inventory_col = canonical_to_actual["inventory account"]
    revenue_col = canonical_to_actual["revenue account"]
    cost_col = canonical_to_actual["cost of sale account"]
    
    mapping = {}
    for _, row in df.iterrows():
        category = str(row[category_col]).strip()
        category = re.sub(r"\s+", " ", category)
        if not category or category.lower() in ("nan", "none", ""):
            continue
        mapping[category] = {
            "asset": str(row[inventory_col]).strip(),
            "income": str(row[revenue_col]).strip(),
            "expense": str(row[cost_col]).strip(),
        }
    
    if not mapping:
        raise ValueError("Product.Mapping.csv contains no valid category mappings")
    
    print(f"[INFO] Mapping loader: file={mapping_file}, detected_headers={detected_headers}, "
          f"canonical_to_actual={canonical_to_actual}, categories_loaded={len(mapping)}")
    
    return mapping


def resolve_account_id_by_name(account_string: str, token_mgr: TokenManager, realm_id: str, cache: Dict[str, Optional[str]]) -> Optional[str]:
    """
    Resolve an account string to a QBO Account ID using Name-based (leaf) matching only.

    Leaf = substring after last ':' then strip. E.g. "120000 - Inventory:120300 - Non - Food Items" -> "120300 - Non - Food Items".

    Resolution strategy:
    1. Primary: Query by exact Name = leaf
    2. Fallback: Query by Name like '%leaf%'; if multiple, pick exact match if present else first Active

    Args:
        account_string: Account string from mapping CSV
        token_mgr: TokenManager instance
        realm_id: QBO Realm ID
        cache: Account cache dict {account_string: account_id}

    Returns:
        Account ID or None if not found

    Raises:
        ValueError: When resolution fails, with original mapping string, leaf used, and exact QBO query(ies) tried.
    """
    if not account_string or not account_string.strip():
        return None

    account_string = account_string.strip()

    # Check cache first
    if account_string in cache:
        return cache[account_string]

    leaf = account_string.split(":")[-1].strip()
    if not leaf:
        cache[account_string] = None
        raise ValueError(
            f"Account resolution failed: mapping_string={account_string!r} leaf={leaf!r} (empty after last ':')"
        )

    safe_leaf = leaf.replace("'", "''")
    account_id = None
    queries_tried: List[str] = []

    # Primary: exact Name = leaf
    query_exact = f"select Id, Name from Account where Name = '{safe_leaf}' maxresults 10"
    queries_tried.append(query_exact)
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query_exact)}&minorversion=70"
    resp = _make_qbo_request("GET", url, token_mgr)
    if resp.status_code == 200:
        data = resp.json()
        accounts = data.get("QueryResponse", {}).get("Account", [])
        if isinstance(accounts, dict):
            accounts = [accounts]
        if accounts:
            account_id = accounts[0].get("Id")
    
    # Fallback: Name like '%leaf%'
    if not account_id:
        safe_like = safe_leaf.replace("%", "").replace("_", "")[:80]
        query_like = f"select Id, Name from Account where Name like '%{safe_like}%' maxresults 10"
        queries_tried.append(query_like)
        url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query_like)}&minorversion=70"
        resp = _make_qbo_request("GET", url, token_mgr)
        if resp.status_code == 200:
            data = resp.json()
            accounts = data.get("QueryResponse", {}).get("Account", [])
            if not isinstance(accounts, list):
                accounts = [accounts] if accounts else []
            if accounts:
                exact = next((a for a in accounts if (a.get("Name") or "") == leaf), None)
                if exact:
                    account_id = exact.get("Id")
                else:
                    active_first = next((a for a in accounts if a.get("Active", True)), accounts[0])
                    account_id = active_first.get("Id")

    if not account_id:
        cache[account_string] = None
        raise ValueError(
            f"Account resolution failed: mapping_string={account_string!r} leaf={leaf!r} queries_tried={queries_tried}"
        )

    cache[account_string] = account_id
    return account_id


def build_account_refs_for_category(
    category: str,
    mapping_cache: Dict[str, Dict[str, str]],
    account_cache: Dict[str, Optional[str]],
    token_mgr: TokenManager,
    realm_id: str,
    config
) -> Dict[str, Dict[str, str]]:
    """
    Build account references for a category.
    
    Args:
        category: Product category from EPOS CSV
        mapping_cache: Category → account names mapping
        account_cache: Account string → account ID cache
        token_mgr: TokenManager instance
        realm_id: QBO Realm ID
        config: CompanyConfig instance
    
    Returns:
        Dict with AssetAccountRef, IncomeAccountRef, ExpenseAccountRef
    
    Raises:
        ValueError: If category missing in mapping or account not found
    """
    # Normalize category: strip whitespace and collapse repeated whitespace
    category_normalized = category.strip()
    category_normalized = re.sub(r'\s+', ' ', category_normalized)
    
    # Lookup category in mapping
    if category_normalized not in mapping_cache:
        raise ValueError(
            f"Missing category '{category}' (normalized: '{category_normalized}') "
            f"in Product.Mapping.csv for company {config.company_key}"
        )
    
    account_names = mapping_cache[category_normalized]
    
    # Resolve each account name → ID
    asset_account_id = resolve_account_id_by_name(
        account_names["asset"], token_mgr, realm_id, account_cache
    )
    income_account_id = resolve_account_id_by_name(
        account_names["income"], token_mgr, realm_id, account_cache
    )
    expense_account_id = resolve_account_id_by_name(
        account_names["expense"], token_mgr, realm_id, account_cache
    )
    
    # Fail fast if any account not found
    if not asset_account_id:
        raise ValueError(
            f"Account '{account_names['asset']}' not found in QBO for category '{category}' "
            f"(company {config.company_key})"
        )
    if not income_account_id:
        raise ValueError(
            f"Account '{account_names['income']}' not found in QBO for category '{category}' "
            f"(company {config.company_key})"
        )
    if not expense_account_id:
        raise ValueError(
            f"Account '{account_names['expense']}' not found in QBO for category '{category}' "
            f"(company {config.company_key})"
        )
    
    return {
        "AssetAccountRef": {"value": asset_account_id},
        "IncomeAccountRef": {"value": income_account_id},
        "ExpenseAccountRef": {"value": expense_account_id},
    }


def get_department_id(name: str, token_mgr: TokenManager, realm_id: str, cache: Dict[str, Optional[str]], config=None) -> Optional[str]:
    """
    Resolve a Department (shown as "Location" in the QBO UI) name to a Department Id with simple caching.

    - First checks if there's a department_mapping in config (maps CSV location -> QBO Department ID)
    - If the name exists in cache, reuse its Id.
    - Otherwise, try a QBO query by Name.
    - Returns None if department not found or name is empty.
    """
    if not name or not name.strip():
        return None
    
    name_clean = name.strip()
    
    # Check config mapping first (if available)
    if config:
        department_mapping = config.get_qbo_config().get("department_mapping", {})
        if name_clean in department_mapping:
            department_id = department_mapping[name_clean]
            cache[name_clean] = department_id  # Cache it for future use
            return department_id
    
    # Check cache
    if name_clean in cache:
        return cache[name_clean]
    
    # Query QBO by name
    safe_name = name_clean.replace("'", "''")
    query = f"select Id from Department where Name = '{safe_name}'"
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    
    resp = _make_qbo_request("GET", url, token_mgr)
    department_id: Optional[str] = None
    if resp.status_code == 200:
        data = resp.json()
        departments = data.get("QueryResponse", {}).get("Department", [])
        if departments:
            department_id = departments[0].get("Id")
    
    if department_id:
        cache[name_clean] = department_id
    else:
        # Cache None to avoid repeated failed queries
        cache[name_clean] = None
    
    return department_id


def find_inventory_items_with_future_start_date(
    token_mgr: TokenManager,
    realm_id: str,
    target_date: str,
) -> List[Dict[str, Any]]:
    """
    Find QBO Inventory items whose InvStartDate is after the run target_date.
    Such items can cause QBO error 6270 (Transaction date prior to start date).

    Read-only: does not mutate QBO data.
    """
    # TrackQtyOnHand = true: items that enforce InvStartDate; include all Active states
    query = "select Id, Name, InvStartDate, Active from Item where Type = 'Inventory' and TrackQtyOnHand = true maxresults 1000"
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    resp = _make_qbo_request("GET", url, token_mgr)
    issues: List[Dict[str, Any]] = []
    if resp.status_code != 200:
        return issues
    data = resp.json()
    items = data.get("QueryResponse", {}).get("Item", [])
    if not isinstance(items, list):
        items = [items] if items else []
    for item in items:
        inv_start = item.get("InvStartDate")
        if not inv_start:
            continue
        # Parse as YYYY-MM-DD (QBO may return full ISO or date-only)
        inv_date_str = inv_start[:10] if len(inv_start) >= 10 else inv_start
        try:
            if inv_date_str > target_date:
                issues.append({
                    "Id": item.get("Id", ""),
                    "Name": item.get("Name", ""),
                    "InvStartDate": inv_date_str,
                    "Active": item.get("Active", ""),
                })
        except (TypeError, ValueError):
            continue
    issues.sort(key=lambda x: x.get("InvStartDate", ""))
    return issues


def write_inventory_start_date_issues_report(
    issues: List[Dict[str, Any]],
    company_key: str,
    target_date: str,
    out_dir: str = "reports",
) -> Optional[str]:
    """
    Write a CSV report of inventory items with InvStartDate after target_date.
    Returns the file path if written, else None.
    """
    if not issues:
        return None
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    safe_date = target_date.replace("-", "")
    safe_key = re.sub(r"[^\w-]", "_", company_key)
    filename = f"inventory_start_date_issues_{safe_key}_{target_date}.csv"
    filepath = os.path.join(out_dir, filename)
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=["Id", "Name", "InvStartDate", "Active"])
        w.writeheader()
        w.writerows(issues)
    return filepath


def get_or_create_item_category_id(
    token_mgr: TokenManager,
    realm_id: str,
    category_name: str,
    cache: Optional[Dict[str, str]] = None,
) -> str:
    """
    Resolve or create a QBO Item of Type="Category" for the given category name.
    Used to assign ParentRef/SubItem when creating Inventory items so they appear under the category in QBO UI.

    Args:
        token_mgr: TokenManager instance
        realm_id: QBO Realm ID
        category_name: EPOS category string (e.g. "COSMETICS AND TOILETRIES")
        cache: Optional dict to cache category_name -> item_id (avoids repeated queries)

    Returns:
        QBO Item Id of the Category item

    Raises:
        RuntimeError: If GET or POST fails
    """
    category_normalized = (category_name or "").strip()
    category_normalized = re.sub(r"\s+", " ", category_normalized) if category_normalized else ""
    if not category_normalized:
        raise ValueError("Category name is empty after normalization")

    if cache is not None and category_normalized in cache:
        return cache[category_normalized]

    safe_name = category_normalized.replace("'", "''")
    query = (
        f"select Id, Name, Type, Active from Item where Type = 'Category' and Name = '{safe_name}' maxresults 10"
    )
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
    resp = _make_qbo_request("GET", url, token_mgr)

    if resp.status_code == 200:
        data = resp.json()
        items = data.get("QueryResponse", {}).get("Item", [])
        if not isinstance(items, list):
            items = [items] if items else []
        if items:
            # Prefer Active=True; if multiple, prefer exact Name match
            active_first = [i for i in items if i.get("Active", True)]
            candidates = active_first if active_first else items
            exact = next(
                (i for i in candidates if (i.get("Name") or "").strip() == category_normalized),
                None,
            )
            chosen = exact or candidates[0]
            cat_id = chosen.get("Id")
            if cat_id:
                if cache is not None:
                    cache[category_normalized] = cat_id
                print(f"[INFO] Reused existing Category item: Name={category_normalized!r} Id={cat_id}")
                return cat_id

    # Create Category item
    create_url = f"{BASE_URL}/v3/company/{realm_id}/item?minorversion=70"
    payload = {
        "Name": category_normalized,
        "Type": "Category",
        "Active": True,
    }
    create_resp = _make_qbo_request("POST", create_url, token_mgr, json=payload)
    if create_resp.status_code not in (200, 201):
        error_msg = f"Failed to create Category item '{category_normalized}': HTTP {create_resp.status_code}"
        try:
            body = create_resp.json()
            fault = body.get("fault")
            if fault:
                errors = fault.get("error", [])
                if errors:
                    error_msg += "\n" + "; ".join(
                        err.get("message", err.get("detail", "")) for err in errors
                    )
        except Exception:
            error_msg += f"\nResponse: {create_resp.text[:500]}"
        raise RuntimeError(error_msg)

    created = create_resp.json().get("Item")
    if not created:
        raise RuntimeError(f"No Item in response when creating Category '{category_normalized}'")
    cat_id = created.get("Id")
    if not cat_id:
        raise RuntimeError(f"Created Category item has no Id: {created}")
    if cache is not None:
        cache[category_normalized] = cat_id
    print(f"[INFO] Created Category item: Name={category_normalized!r} Id={cat_id}")
    return cat_id


def create_inventory_item(
    name: str,
    category: str,
    unit_sales_price: float,
    unit_purchase_cost: float,
    config,
    token_mgr: TokenManager,
    realm_id: str,
    mapping_cache: Dict[str, Dict[str, str]],
    account_cache: Dict[str, Optional[str]],
    target_date: Optional[str] = None,
    category_item_id: Optional[str] = None,
) -> str:
    """
    Create a QBO Inventory item.
    
    Args:
        name: Product name
        category: Product category (normalized)
        unit_sales_price: Per-unit sales price (NET Sales / qty; tax-inclusive in UI)
        unit_purchase_cost: Per-unit purchase cost (Cost Price / qty)
        config: CompanyConfig instance
        token_mgr: TokenManager instance
        realm_id: QBO Realm ID
        mapping_cache: Category → account names mapping
        account_cache: Account string → account ID cache
        target_date: Optional run target date (YYYY-MM-DD). When set, InvStartDate
            is set to target_date so receipts for that date are allowed; otherwise
            config.inventory_start_date is used.
        category_item_id: Optional QBO Item Id of Type="Category" to set as parent (SubItem=True, ParentRef).
            When set, the new Inventory item appears under that category in QBO UI.
    
    Returns:
        Created item ID

    Raises:
        ValueError: If category missing or accounts not found
        RuntimeError: If QBO API call fails
    """
    # Build account references
    account_refs = build_account_refs_for_category(
        category, mapping_cache, account_cache, token_mgr, realm_id, config
    )
    
    # Use run target_date for InvStartDate when set (e.g. "yesterday" run), so receipts
    # for that date are allowed. Otherwise fall back to config.inventory_start_date.
    inv_start_date = target_date if target_date else config.inventory_start_date
    
    tax_code_id = config.tax_code_id or "2"
    
    # Build Item payload: pricing + tax-inclusive + tax code refs
    payload = {
        "Name": name,
        "Type": "Inventory",
        "TrackQtyOnHand": True,
        "QtyOnHand": config.default_qty_on_hand,
        "InvStartDate": inv_start_date,
        "Description": f"Sale(s) of {name}",
        "UnitPrice": unit_sales_price,
        "PurchaseCost": unit_purchase_cost,
        "SalesTaxIncluded": True,
        "PurchaseTaxIncluded": True,
        "Taxable": True,
        "IncomeAccountRef": account_refs["IncomeAccountRef"],
        "AssetAccountRef": account_refs["AssetAccountRef"],
        "ExpenseAccountRef": account_refs["ExpenseAccountRef"],
        "PurchaseDesc": f"Purchase of {name}",
    }
    if tax_code_id:
        payload["SalesTaxCodeRef"] = {"value": tax_code_id}
        payload["PurchaseTaxCodeRef"] = {"value": tax_code_id}

    if category_item_id:
        payload["SubItem"] = True
        payload["ParentRef"] = {"value": category_item_id}
        print(f"[INFO] Attached ParentRef/SubItem to Inventory item '{name}' (category item Id: {category_item_id})")
    else:
        print(f"[WARN] No category item id; creating Inventory item '{name}' without ParentRef/SubItem")
    
    # Create item via QBO API (tax codes applied at SalesReceipt line level; Item-level refs may be rejected in some regions)
    create_url = f"{BASE_URL}/v3/company/{realm_id}/item?minorversion=70"
    create_resp = _make_qbo_request("POST", create_url, token_mgr, json=payload)
    
    if create_resp.status_code in (200, 201):
        created = create_resp.json().get("Item")
        if created:
            item_id = created.get("Id")
            if item_id:
                print(f"[INFO] Created Inventory item '{name}' (ID: {item_id})")
                return item_id
    
    # On 400, retry without tax code refs (QBO may reject them for Inventory in some regions)
    if create_resp.status_code == 400 and ("SalesTaxCodeRef" in payload or "PurchaseTaxCodeRef" in payload):
        payload_retry = {k: v for k, v in payload.items() if k not in ("SalesTaxCodeRef", "PurchaseTaxCodeRef")}
        create_resp = _make_qbo_request("POST", create_url, token_mgr, json=payload_retry)
        if create_resp.status_code in (200, 201):
            created = create_resp.json().get("Item")
            if created and created.get("Id"):
                print(f"[WARN] QBO rejected SalesTaxCodeRef/PurchaseTaxCodeRef on create; item created without them.")
                print(f"[INFO] Created Inventory item '{name}' (ID: {created.get('Id')})")
                return created.get("Id")
    
    # Failed to create
    error_msg = f"Failed to create Inventory item '{name}': HTTP {create_resp.status_code}"
    try:
        error_body = create_resp.json()
        fault = error_body.get("fault")
        if fault:
            errors = fault.get("error", [])
            if errors:
                error_details = [err.get("message", err.get("detail", "")) for err in errors]
                error_msg += f"\nError details: {'; '.join(error_details)}"
    except Exception:
        error_msg += f"\nResponse: {create_resp.text[:500]}"
    
    raise RuntimeError(error_msg)


def rename_and_inactivate_item(
    token_mgr: TokenManager,
    realm_id: str,
    item_id: str,
    new_name: str,
    *,
    make_inactive: bool = True,
) -> dict:
    """
    Rename and optionally inactivate a QBO Item to free its name for inventory creation.
    
    Args:
        token_mgr: TokenManager instance
        realm_id: QBO Realm ID
        item_id: Item ID to update
        new_name: New name for the item
        make_inactive: Whether to set Active=False (default: True)
    
    Returns:
        Updated item JSON from QBO response
    
    Raises:
        RuntimeError: If GET or POST fails
    """
    # Fetch current item to get SyncToken and preserve fields
    get_url = f"{BASE_URL}/v3/company/{realm_id}/item/{item_id}?minorversion=70"
    get_resp = _make_qbo_request("GET", get_url, token_mgr)
    
    if get_resp.status_code != 200:
        error_msg = f"Failed to fetch item {item_id} for rename: HTTP {get_resp.status_code}"
        try:
            error_body = get_resp.json()
            fault = error_body.get("fault")
            if fault:
                errors = fault.get("error", [])
                if errors:
                    error_details = [err.get("message", err.get("detail", "")) for err in errors]
                    error_msg += f"\nError details: {'; '.join(error_details)}"
        except Exception:
            error_msg += f"\nResponse: {get_resp.text[:500]}"
        raise RuntimeError(error_msg)
    
    current_item = get_resp.json().get("Item")
    if not current_item:
        raise RuntimeError(f"No Item in response when fetching {item_id}")
    
    old_name = current_item.get("Name", "")
    sync_token = current_item.get("SyncToken")
    if not sync_token:
        raise RuntimeError(f"Item {item_id} missing SyncToken (required for updates)")
    
    # Idempotency: if name already contains "(LEGACY", do not re-rename; only inactivate
    current_name = (old_name or "").strip()
    if "(LEGACY" in current_name.upper():
        effective_name = old_name  # keep current name, avoid "LEGACY LEGACY ..."
        idempotent_rename = True
    else:
        effective_name = new_name
        idempotent_rename = False

    # Sparse update: only Id, SyncToken, Name, Active (QBO-safe; no Type/account refs required)
    update_payload = {
        "sparse": True,
        "Id": item_id,
        "SyncToken": sync_token,
        "Name": effective_name,
        "Active": False if make_inactive else current_item.get("Active", True),
    }

    # Update item via QBO API
    update_url = f"{BASE_URL}/v3/company/{realm_id}/item?minorversion=70"
    update_resp = _make_qbo_request("POST", update_url, token_mgr, json=update_payload)
    
    if update_resp.status_code not in (200, 201):
        error_msg = f"Failed to rename/inactivate item {item_id}: HTTP {update_resp.status_code}"
        try:
            error_body = update_resp.json()
            fault = error_body.get("fault")
            if fault:
                errors = fault.get("error", [])
                if errors:
                    error_details = [err.get("message", err.get("detail", "")) for err in errors]
                    error_msg += f"\nError details: {'; '.join(error_details)}"
        except Exception:
            error_msg += f"\nResponse: {update_resp.text[:500]}"
        raise RuntimeError(error_msg)
    
    updated_item = update_resp.json().get("Item")
    if not updated_item:
        raise RuntimeError(f"No Item in response when updating {item_id}")
    
    active_status = "Active=False" if make_inactive else f"Active={updated_item.get('Active', True)}"
    if idempotent_rename:
        print(f"[INFO] Inactivated item (already LEGACY-named): Id={item_id} name={effective_name!r} {active_status}")
    else:
        print(f"[INFO] Renamed and inactivated item: Id={item_id} old_name={old_name!r} new_name={effective_name!r} {active_status}")
    
    return updated_item


def get_or_create_item_id(
    name: str,
    token_mgr: TokenManager,
    realm_id: str,
    config,
    cache: Dict[str, str],
    category: Optional[str] = None,
    unit_sales_price: Optional[float] = None,
    unit_purchase_cost: Optional[float] = None,
    mapping_cache: Optional[Dict[str, Dict[str, str]]] = None,
    account_cache: Optional[Dict[str, Optional[str]]] = None,
    target_date: Optional[str] = None,
    items_wrong_type: Optional[List[Dict[str, Any]]] = None,
    items_autofixed: Optional[List[Dict[str, Any]]] = None,
    category_item_cache: Optional[Dict[str, str]] = None,
    items_patched_pricing_tax: Optional[List[Dict[str, Any]]] = None,
) -> Tuple[str, bool, str, Optional[str]]:
    """
    Resolve an Item name to an Item Id with simple caching.
    Returns (item_id, was_created, created_type, fallback_reason).
    created_type: "Inventory" | "Service" | "Default" | "existing" | "existing_inventory" | "existing_non_inventory" | "created_inventory_after_fix"
    fallback_reason: optional, e.g. "blank_name", "inventory_failed", "service_creation_failed"
    When inventory mode is enabled and an existing item has Type != Inventory, appends to items_wrong_type if provided.
    When auto_fix_wrong_type_items is enabled, renames/inactivates wrong-type items and creates inventory items.
    """
    default_item_id = config.get_qbo_config().get("default_item_id", "1")
    default_income_account_id = config.get_qbo_config().get("default_income_account_id", "1")
    auto_create_items = True  # Can be made configurable later
    created_type = "existing"
    fallback_reason: Optional[str] = None

    # Normalize name: strip and collapse internal whitespace (use for cache and QBO)
    name = (name or "").strip()
    name = re.sub(r"\s+", " ", name) if name else ""

    if not name:
        print(f"[WARN] Blank item name → using DEFAULT_ITEM_ID")
        return (default_item_id, False, "Default", "blank_name")

    if name in cache:
        return (cache[name], False, "existing", None)

    safe_name = name.replace("'", "''")
    query = f"select Id, Name, Type, TrackQtyOnHand, InvStartDate, Active from Item where Name = '{safe_name}' maxresults 10"
    url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"

    resp = _make_qbo_request("GET", url, token_mgr)
    item_id: Optional[str] = None
    was_created = False
    # Track auto-fix state: if we rename/inactivate a wrong-type item, remember its details
    autofixed_old_item_id: Optional[str] = None
    autofixed_old_type: Optional[str] = None
    autofixed_effective_new_name: Optional[str] = None  # actual name after update (for report)
    if resp.status_code == 200:
        data = resp.json()
        items = data.get("QueryResponse", {}).get("Item", [])
        if not isinstance(items, list):
            items = [items] if items else []
        if items:
            first = items[0]
            item_id = first.get("Id")
            item_type = first.get("Type") or ""
            if item_type == "Inventory":
                created_type = "existing_inventory"
                # PATCH existing Inventory: category (ParentRef/SubItem) and/or pricing/tax (UnitPrice, PurchaseCost, tax flags)
                try:
                    get_url = f"{BASE_URL}/v3/company/{realm_id}/item/{item_id}?minorversion=70"
                    get_resp = _make_qbo_request("GET", get_url, token_mgr)
                    if get_resp.status_code == 200:
                        current = get_resp.json().get("Item")
                        if current and current.get("SyncToken") is not None:
                            patch_payload = {
                                "Id": item_id,
                                "SyncToken": current["SyncToken"],
                                "sparse": True,
                                "Type": "Inventory",
                            }
                            # Category: add ParentRef/SubItem if missing
                            parent_ref = current.get("ParentRef")
                            if (category or "").strip() and category_item_cache is not None and (not parent_ref or not parent_ref.get("value")):
                                cat_id = get_or_create_item_category_id(
                                    token_mgr, realm_id, category, cache=category_item_cache
                                )
                                patch_payload["SubItem"] = True
                                patch_payload["ParentRef"] = {"value": cat_id}
                            # Pricing/tax: only set if current is 0/missing/false; do NOT overwrite non-zero prices
                            tax_code_id = config.tax_code_id or "2"
                            cur_unit_price = current.get("UnitPrice")
                            cur_purchase_cost = current.get("PurchaseCost")
                            cur_sales_tax_inc = current.get("SalesTaxIncluded", True)
                            cur_purchase_tax_inc = current.get("PurchaseTaxIncluded", True)
                            cur_taxable = current.get("Taxable", True)
                            cur_sales_tax_ref = (current.get("SalesTaxCodeRef") or {}).get("value") if isinstance(current.get("SalesTaxCodeRef"), dict) else None
                            cur_purchase_tax_ref = (current.get("PurchaseTaxCodeRef") or {}).get("value") if isinstance(current.get("PurchaseTaxCodeRef"), dict) else None
                            pricing_tax_changes = []
                            if (cur_unit_price is None or float(cur_unit_price or 0) == 0) and unit_sales_price is not None and unit_sales_price > 0:
                                patch_payload["UnitPrice"] = unit_sales_price
                                pricing_tax_changes.append(f"UnitPrice:0->{unit_sales_price}")
                            if (cur_purchase_cost is None or float(cur_purchase_cost or 0) == 0) and unit_purchase_cost is not None and unit_purchase_cost > 0:
                                patch_payload["PurchaseCost"] = unit_purchase_cost
                                pricing_tax_changes.append(f"PurchaseCost:0->{unit_purchase_cost}")
                            if not cur_sales_tax_inc:
                                patch_payload["SalesTaxIncluded"] = True
                                pricing_tax_changes.append("SalesTaxIncluded:False->True")
                            if not cur_purchase_tax_inc:
                                patch_payload["PurchaseTaxIncluded"] = True
                                pricing_tax_changes.append("PurchaseTaxIncluded:False->True")
                            if not cur_taxable:
                                patch_payload["Taxable"] = True
                                pricing_tax_changes.append("Taxable:False->True")
                            if tax_code_id and not cur_sales_tax_ref:
                                patch_payload["SalesTaxCodeRef"] = {"value": tax_code_id}
                                pricing_tax_changes.append("SalesTaxCodeRef:->" + tax_code_id)
                            if tax_code_id and not cur_purchase_tax_ref:
                                patch_payload["PurchaseTaxCodeRef"] = {"value": tax_code_id}
                                pricing_tax_changes.append("PurchaseTaxCodeRef:->" + tax_code_id)
                            if len(patch_payload) > 4:
                                patch_url = f"{BASE_URL}/v3/company/{realm_id}/item?minorversion=70"
                                patch_resp = _make_qbo_request("POST", patch_url, token_mgr, json=patch_payload)
                                if patch_resp.status_code in (200, 201):
                                    if "ParentRef" in patch_payload:
                                        print(f"[INFO] Categorized existing Inventory item: Id={item_id} ParentRef={patch_payload['ParentRef']['value']} category={category!r}")
                                    if pricing_tax_changes:
                                        print(f"[INFO] Patched Inventory item fields: Id={item_id} " + " ".join(pricing_tax_changes))
                                        if items_patched_pricing_tax is not None:
                                            items_patched_pricing_tax.append({
                                                "ItemId": item_id,
                                                "Name": name,
                                                "Category": category or "",
                                                "UnitPrice_old": cur_unit_price,
                                                "UnitPrice_new": patch_payload.get("UnitPrice", cur_unit_price),
                                                "PurchaseCost_old": cur_purchase_cost,
                                                "PurchaseCost_new": patch_payload.get("PurchaseCost", cur_purchase_cost),
                                                "SalesTaxIncluded_old/new": f"{cur_sales_tax_inc}->{patch_payload.get('SalesTaxIncluded', cur_sales_tax_inc)}",
                                                "PurchaseTaxIncluded_old/new": f"{cur_purchase_tax_inc}->{patch_payload.get('PurchaseTaxIncluded', cur_purchase_tax_inc)}",
                                                "Taxable_old/new": f"{cur_taxable}->{patch_payload.get('Taxable', cur_taxable)}",
                                                "TxnDate": target_date or "",
                                                "DocNumber": "",
                                            })
                                else:
                                    if "SalesTaxCodeRef" in patch_payload or "PurchaseTaxCodeRef" in patch_payload:
                                        try:
                                            err_body = patch_resp.json()
                                            err_msg = str(err_body.get("fault", {}).get("error", [{}])[0].get("message", patch_resp.text[:200]))
                                            if "400" in str(patch_resp.status_code):
                                                print(f"[WARN] QBO rejected tax refs on item {item_id}: {err_msg}. Taxable/included flags and pricing still applied if sent.")
                                        except Exception:
                                            pass
                                    print(f"[WARN] Failed to PATCH item {item_id}: HTTP {patch_resp.status_code}")
                except Exception as e:
                    print(f"[WARN] Failed to patch existing Inventory item {item_id}: {e}")
                cache[name] = item_id
                return (item_id, False, created_type, None)
            if config.inventory_enabled:
                # Check if auto-fix is enabled
                if config.auto_fix_wrong_type_items:
                    # Try to rename and inactivate the wrong-type item
                    old_item_id = item_id
                    try:
                        new_name = f"{name} (LEGACY {item_type} {old_item_id})"
                        updated_item = rename_and_inactivate_item(token_mgr, realm_id, old_item_id, new_name, make_inactive=True)
                        autofixed_old_item_id = old_item_id
                        autofixed_old_type = item_type
                        autofixed_effective_new_name = (updated_item.get("Name") or "").strip() or new_name
                        item_id = None
                        # Continue to create path below
                    except Exception as e:
                        # Rename failed: fall back to current behavior
                        print(f"[WARN] Failed to auto-fix wrong-type item '{name}' (Id={old_item_id}): {e}. "
                              f"Will use existing item for receipt lines.")
                        if items_wrong_type is not None:
                            items_wrong_type.append({
                                "Name": name,
                                "Id": str(old_item_id) if old_item_id else "",
                                "Type": item_type,
                                "ExpectedType": "Inventory",
                            })
                        created_type = "existing_non_inventory"
                        cache[name] = old_item_id
                        return (old_item_id, False, created_type, None)
                else:
                    # Auto-fix disabled: use current behavior
                    print(f"[WARN] Inventory mode enabled but item exists as Type={item_type!r}. Cannot auto-convert. "
                          f"Will use existing item for receipt lines. name={name!r} Id={item_id}")
                    if items_wrong_type is not None:
                        items_wrong_type.append({
                            "Name": name,
                            "Id": str(item_id) if item_id else "",
                            "Type": item_type,
                            "ExpectedType": "Inventory",
                        })
                    created_type = "existing_non_inventory"
                    cache[name] = item_id
                    return (item_id, False, created_type, None)
            created_type = "existing"

    if not item_id and auto_create_items:
        if config.inventory_enabled:
            # Create Inventory item
            if not category or unit_sales_price is None or unit_purchase_cost is None:
                raise ValueError(
                    f"Inventory mode enabled but missing required parameters for item '{name}'. "
                    f"Need: category, unit_sales_price, unit_purchase_cost"
                )
            if mapping_cache is None or account_cache is None:
                raise ValueError(
                    f"Inventory mode enabled but missing mapping_cache or account_cache for item '{name}'"
                )

            category_item_id_val: Optional[str] = None
            if (category or "").strip():
                try:
                    category_item_id_val = get_or_create_item_category_id(
                        token_mgr, realm_id, category, cache=category_item_cache
                    )
                except Exception as e:
                    print(f"[WARN] Failed to resolve Category item for {category!r}: {e}. Creating Inventory item without ParentRef/SubItem.")
            else:
                print(f"[WARN] Category missing/empty; creating Inventory item '{name}' without ParentRef/SubItem")

            try:
                item_id = create_inventory_item(
                    name, category, unit_sales_price, unit_purchase_cost,
                    config, token_mgr, realm_id, mapping_cache, account_cache,
                    target_date=target_date,
                    category_item_id=category_item_id_val,
                )
                was_created = True
                # Check if this was created after auto-fix
                if autofixed_old_item_id:
                    created_type = "created_inventory_after_fix"
                    # Append to autofix report (DocNumber/TxnDate will be filled by caller)
                    if items_autofixed is not None:
                        items_autofixed.append({
                            "OriginalName": name,
                            "OldItemId": str(autofixed_old_item_id),
                            "OldType": autofixed_old_type or "",
                            "OldActive": "True",
                            "NewName": autofixed_effective_new_name or f"{name} (LEGACY {autofixed_old_type} {autofixed_old_item_id})",
                            "NewInventoryItemId": str(item_id),
                            "TxnDate": target_date or "",
                            "DocNumber": "",  # Will be filled by caller
                        })
                else:
                    created_type = "Inventory"
            except Exception as e:
                # Do NOT fall back to default "Services". Create Service item with product name.
                print(f"[WARN] Failed to create Inventory item '{name}' (category={category!r}): {e}. "
                      f"Falling back to Service item with product name (NOT default Services)")
                create_url = f"{BASE_URL}/v3/company/{realm_id}/item?minorversion=70"
                payload = {
                    "Name": name,
                    "Type": "Service",
                    "IncomeAccountRef": {"value": default_income_account_id},
                }
                create_resp = _make_qbo_request("POST", create_url, token_mgr, json=payload)
                if create_resp.status_code in (200, 201):
                    created = create_resp.json().get("Item")
                    if created:
                        item_id = created.get("Id")
                        was_created = True
                        created_type = "Service"
                        fallback_reason = "inventory_failed"
                if not item_id:
                    raise RuntimeError(
                        f"Could not create Inventory item and Service item fallback also failed for '{name}'. "
                        f"Inventory error: {e}. Do not use default Services for non-blank product names."
                    )
        else:
            # Inventory disabled: Create Service item (existing behavior)
            create_url = f"{BASE_URL}/v3/company/{realm_id}/item?minorversion=70"
            payload = {
                "Name": name,
                "Type": "Service",
                "IncomeAccountRef": {"value": default_income_account_id},
            }
            create_resp = _make_qbo_request("POST", create_url, token_mgr, json=payload)
            if create_resp.status_code in (200, 201):
                created = create_resp.json().get("Item")
                if created:
                    item_id = created.get("Id")
                    was_created = True
                    created_type = "Service"
            else:
                print(f"[WARN] Failed to create Item '{name}': {create_resp.status_code}")
                try:
                    print(create_resp.text)
                except Exception:
                    pass
                print(f"[WARN] Using DEFAULT_ITEM_ID for '{name}' (inventory disabled, service creation failed)")
                created_type = "Default"
                fallback_reason = "service_creation_failed"

    if not item_id:
        item_id = default_item_id
        created_type = "Default"
        if fallback_reason is None:
            fallback_reason = "service_creation_failed"

    cache[name] = item_id
    return (item_id, was_created, created_type, fallback_reason)


def build_sales_receipt_payload(
    group: pd.DataFrame,
    token_mgr: TokenManager,
    realm_id: str,
    config,
    item_cache: Dict[str, str],
    department_cache: Dict[str, Optional[str]],
    payment_method_cache: Dict[str, Optional[str]] = None,
    target_date: Optional[str] = None,
    mapping_cache: Optional[Dict[str, Dict[str, str]]] = None,
    account_cache: Optional[Dict[str, Optional[str]]] = None,
    items_wrong_type: Optional[List[Dict[str, Any]]] = None,
    items_autofixed: Optional[List[Dict[str, Any]]] = None,
    category_item_cache: Optional[Dict[str, str]] = None,
    items_patched_pricing_tax: Optional[List[Dict[str, Any]]] = None,
) -> dict:
    """
    Build a SalesReceipt payload from a group of CSV rows (one SalesReceiptNo).

    Behaviour:
    - One SalesReceipt per group.
    - One line per row in the group.
    - We treat ItemRate / *ItemAmount as GROSS (inclusive of VAT).
    - QBO is told that amounts are tax-inclusive, so it backs out the VAT.
    - The Rate column in QBO will match ItemRate from the CSV whenever valid.
    """
    first_row = group.iloc[0]

    # Determine TxnDate: use target_date if trading_day_enabled and target_date is provided, otherwise parse from CSV
    if config.trading_day_enabled and target_date:
        # Trading day mode: use the target_date (trading date) directly
        txn_date = target_date
        print(f"[INFO] Trading day mode: using target_date={target_date} for TxnDate (overriding CSV date)")
    else:
        # Calendar day mode: parse date from CSV using company's date_format
        txn_date_str = str(first_row[DATE_COL])
        try:
            # Parse using the company's date format
            date_obj = datetime.strptime(txn_date_str, config.date_format)
            # Convert to ISO format (YYYY-MM-DD) for QBO
            txn_date = date_obj.strftime("%Y-%m-%d")
        except (ValueError, TypeError):
            # Fallback: try to parse as ISO format or use pandas
            try:
                date_obj = pd.to_datetime(txn_date_str).to_pydatetime()
                txn_date = date_obj.strftime("%Y-%m-%d")
            except Exception:
                # Last resort: use as-is (may cause QBO errors)
                print(f"[WARN] Could not parse date '{txn_date_str}', using as-is. QBO may reject it.")
                txn_date = txn_date_str
    
    memo = str(first_row[MEMO_COL])
    doc_number = str(first_row[DOCNUM_COL])
    location_name = str(first_row.get(LOCATION_COL, "")).strip()

    lines = []
    gross_total = 0.0
    net_total = 0.0
    inventory_created_count = 0
    service_created_count = 0
    default_fallback_count = 0

    for _, row in group.iterrows():
        # Product/Service: normalize (strip + collapse whitespace)
        item_name = str(row.get(ITEM_NAME_COL, "")).strip()
        item_name = re.sub(r"\s+", " ", item_name) if item_name else ""

        # Extract category from ItemDescription (contains Category from EPOS CSV)
        category = str(row.get(ITEM_DESC_COL, "")).strip()
        if category:
            category = category.strip()
            category = re.sub(r"\s+", " ", category)
        else:
            category = None

        # Extract NET Sales and Cost Price columns
        def safe_numeric(value, default=0.0):
            """Safely convert to float, handling commas, NaN, etc."""
            if pd.isna(value) or value == "" or value is None:
                return default
            try:
                # Strip commas and convert to float
                if isinstance(value, str):
                    value = value.replace(",", "").strip()
                return float(value)
            except (TypeError, ValueError):
                return default
        
        net_sales_total = safe_numeric(row.get("NET Sales", 0))
        cost_price_total = safe_numeric(row.get("Cost Price", 0))
        amount_gross = safe_numeric(row.get(AMOUNT_COL, 0))
        
        # Quantity (default to 1 if missing/NaN or <=0)
        try:
            qty_val = safe_numeric(row.get(QTY_COL, 1))
            if qty_val <= 0:
                qty_val = 1.0
        except (TypeError, ValueError):
            qty_val = 1.0
        
        # Per-unit prices for item create/patch and receipt line
        unit_sales_price = (net_sales_total / qty_val) if qty_val else 0.0
        unit_purchase_cost = (cost_price_total / qty_val) if qty_val else 0.0
        if unit_sales_price == 0 and qty_val and amount_gross > 0:
            unit_sales_price = amount_gross / qty_val
        
        # Get or create item ID (with inventory support if enabled)
        item_ref_id, item_was_created, created_type, fallback_reason = get_or_create_item_id(
            item_name, token_mgr, realm_id, config, item_cache,
            category=category if config.inventory_enabled else None,
            unit_sales_price=unit_sales_price if config.inventory_enabled else None,
            unit_purchase_cost=unit_purchase_cost if config.inventory_enabled else None,
            mapping_cache=mapping_cache if config.inventory_enabled else None,
            account_cache=account_cache if config.inventory_enabled else None,
            target_date=target_date,
            items_wrong_type=items_wrong_type,
            items_autofixed=items_autofixed,
            category_item_cache=category_item_cache if config.inventory_enabled else None,
            items_patched_pricing_tax=items_patched_pricing_tax if config.inventory_enabled else None,
        )

        # Fill in DocNumber and TxnDate for autofixed items
        if created_type == "created_inventory_after_fix" and items_autofixed:
            # Find the most recent entry (last appended) and fill DocNumber/TxnDate
            for entry in reversed(items_autofixed):
                if entry.get("DocNumber") == "" and entry.get("OriginalName") == item_name:
                    entry["DocNumber"] = doc_number
                    entry["TxnDate"] = txn_date
                    break
        # Fill in DocNumber/TxnDate for patched pricing/tax items
        if created_type == "existing_inventory" and items_patched_pricing_tax:
            for entry in reversed(items_patched_pricing_tax):
                if entry.get("DocNumber") == "" and entry.get("Name") == item_name:
                    entry["DocNumber"] = doc_number
                    entry["TxnDate"] = txn_date
                    break

        if created_type == "Inventory" or created_type == "created_inventory_after_fix":
            inventory_created_count += 1
        elif created_type == "Service":
            service_created_count += 1
        elif created_type == "Default":
            default_fallback_count += 1

        log_line = (f"[INFO] Line item: DocNumber={doc_number} TxnDate={txn_date} item_name={item_name!r} category={category!r} "
                    f"qty={qty_val} unit_sales_price={unit_sales_price} unit_purchase_cost={unit_purchase_cost} item_id={item_ref_id} created={item_was_created} type={created_type}")
        if fallback_reason:
            log_line += f" fallback_reason={fallback_reason}"
        print(log_line)

        # Authoritative gross amount from CSV (*ItemAmount) – VAT-inclusive
        try:
            amount_csv = float(row[AMOUNT_COL])
        except (TypeError, ValueError, KeyError):
            amount_csv = 0.0

        # Per-line tax amount from CSV. If missing, derive from the configured rate.
        try:
            tax_amount = float(row.get(TAX_AMOUNT_COL, 0.0) or 0.0)
        except (TypeError, ValueError):
            tax_amount = 0.0

        # For tax-inclusive logic we treat *ItemAmount as the authoritative GROSS
        # line amount. We'll derive a net amount (for Amount) and a net UnitPrice
        # so that QBO's validation rule Amount == UnitPrice * Qty holds.
        amount_gross = amount_csv

        # Service date: fall back to TxnDate if missing or invalid
        # Parse service date using company's date_format and convert to ISO format
        service_date_str = str(row.get(SERVICE_DATE_COL, "")).strip()
        if not service_date_str or service_date_str == "nan" or service_date_str.lower() == "none":
            # Use TxnDate if Service Date is missing or empty
            service_date = txn_date
        else:
            try:
                # Parse using the company's date format
                service_date_obj = datetime.strptime(service_date_str, config.date_format)
                service_date = service_date_obj.strftime("%Y-%m-%d")
            except (ValueError, TypeError):
                # If already in ISO format or can't parse, try pandas or use TxnDate
                try:
                    service_date_obj = pd.to_datetime(service_date_str).to_pydatetime()
                    service_date = service_date_obj.strftime("%Y-%m-%d")
                except Exception:
                    # Fallback to TxnDate (already in ISO format)
                    service_date = txn_date

        # Description: prefer ItemDescription, fall back to memo
        description = str(row.get(ITEM_DESC_COL, memo))

        # Tax code handling: both companies now use vat_inclusive_7_5 mode
        # Use tax_code_id from config (Company A: "2", Company B: "22")
        tax_code_id = config.tax_code_id
        if not tax_code_id:
            # Fallback: try to query by name if tax_code_name is provided
            if hasattr(config, 'tax_code_name') and config.tax_code_name:
                if not hasattr(build_sales_receipt_payload, '_tax_code_cache'):
                    build_sales_receipt_payload._tax_code_cache = {}
                tax_code_id = get_tax_code_id_by_name(
                    config.tax_code_name, 
                    token_mgr, 
                    realm_id, 
                    build_sales_receipt_payload._tax_code_cache
                )
                if not tax_code_id:
                    print(f"[WARN] Tax code '{config.tax_code_name}' not found in QBO. Line will be created without tax code.")
            else:
                # Default fallback
                tax_code_id = "2" if config.company_key == "company_a" else "22"
        
        # QBO API: ItemRef with both value (Id) and name so the receipt shows the product name
        sales_item_detail = {
            "ItemRef": {"value": item_ref_id, "name": item_name},
            "Qty": qty_val,
            # UnitPrice will be set after we compute the net amount below
            "UnitPrice": None,
            "ServiceDate": service_date,
        }
        
        # Add tax code reference if we have one (for both Company A and Company B)
        if tax_code_id:
            sales_item_detail["TaxCodeRef"] = {"value": tax_code_id}

        # Calculate NET amount (exclusive of tax) from GROSS (tax-inclusive amount)
        # For Company A (vat_inclusive_7_5): use ItemTaxAmount from CSV if available
        # For Company B (tax_inclusive_composite): calculate from config tax_rate
        if config.tax_mode == "tax_inclusive_composite":
            # Company B: Calculate net using the full composite tax rate (12.5% = 7.5% + 5%)
            tax_rate = config.tax_rate or 0.125
            raw_amount_net = round(amount_gross / (1 + tax_rate), 2)
        else:
            # Company A: Use ItemTaxAmount from CSV
            raw_amount_net = amount_gross - tax_amount
            if raw_amount_net < 0:
                raw_amount_net = 0.0

        # Net unit price so that Amount == UnitPrice * Qty holds for QBO validation.
        unit_price_net = round(raw_amount_net / qty_val, 2) if qty_val else raw_amount_net
        amount_net = round(unit_price_net * qty_val, 2)
        sales_item_detail["UnitPrice"] = unit_price_net

        # TaxInclusiveAmt tells QBO what the original gross amount is
        # This is needed for both Company A and Company B to show correct totals
        sales_item_detail["TaxInclusiveAmt"] = amount_gross

        lines.append(
            {
                "DetailType": "SalesItemLineDetail",
                "Amount": amount_net,  # net per line; matches QBO's stored Amount
                "Description": description,
                "SalesItemLineDetail": sales_item_detail,
            }
        )
        gross_total += amount_gross
        net_total += amount_net

    payload: dict = {
        "TxnDate": txn_date,
        "PrivateNote": memo,
        "DocNumber": doc_number,
        "Line": lines,
    }
    
    # Tax handling based on company config
    if config.tax_mode == "vat_inclusive_7_5":
        # Tax-inclusive mode for Company A (single-rate VAT)
        payload["GlobalTaxCalculation"] = "TaxInclusive"
        
        # Explicit tax summary for tax-inclusive calculation
        try:
            # Get tax rate from config (Company A: 0.075 = 7.5%)
            tax_rate = config.tax_rate
            tax_percent = tax_rate * 100  # Convert to percentage for QBO
            
            net_base = round(net_total or (gross_total / (1 + tax_rate)), 2)
            total_tax = round(gross_total - net_base, 2)
            
            # Get TaxRate ID from config (required by QBO when TxnTaxDetail is provided)
            tax_rate_id = config.get_qbo_config().get("tax_rate_id")
            if not tax_rate_id:
                # Fallback: try using tax_code_id if tax_rate_id not set (Company A: "2" works for both)
                tax_rate_id = config.tax_code_id
                if not tax_rate_id:
                    raise ValueError("tax_rate_id or tax_code_id must be set in config for tax-inclusive mode")
            
            payload["TxnTaxDetail"] = {
                "TotalTax": total_tax,
                "TaxLine": [
                    {
                        "Amount": total_tax,
                        "DetailType": "TaxLineDetail",
                        "TaxLineDetail": {
                            "TaxRateRef": {"value": tax_rate_id},
                            "PercentBased": True,
                            "TaxPercent": tax_percent,
                            "NetAmountTaxable": net_base,
                        },
                    }
                ],
            }
        except Exception:
            # If anything goes wrong, fall back to letting QBO compute.
            pass
    elif config.tax_mode == "tax_inclusive_composite":
        # Tax-inclusive mode for Company B with composite tax (12.5% = 7.5% VAT + 5% Lagos)
        # 
        # Strategy: Same as Company A but with TWO TaxLines in TxnTaxDetail
        # - Line items have TaxInclusiveAmt = gross amount
        # - GlobalTaxCalculation = "TaxInclusive"
        # - TxnTaxDetail has explicit breakdown for each tax component
        # - Subtotal = gross, Total = gross (tax is INCLUDED, shown as breakdown)
        payload["GlobalTaxCalculation"] = "TaxInclusive"
        
        try:
            # Get tax components from config
            tax_components = config.get_qbo_config().get("tax_components", [])
            if not tax_components:
                raise ValueError("tax_components must be set in config for tax_inclusive_composite mode")
            
            # KEY FIX: Use net_total (sum of per-line amounts) for TxnTaxDetail
            # This matches how Company A does it in the reference script
            # The reference script uses: net_base = net_total or (gross_total / (1 + tax_rate))
            # This ensures TxnTaxDetail matches the actual sum of line amounts
            total_tax_rate = sum(c.get("rate", 0) for c in tax_components)  # 0.125 for 12.5%
            net_base = round(net_total or (gross_total / (1 + total_tax_rate)), 2)
            total_tax = round(gross_total - net_base, 2)
            
            # Build TaxLines for each component
            # Distribute tax proportionally, with last component getting the remainder
            tax_lines = []
            allocated_tax = 0.0
            
            for i, component in enumerate(tax_components):
                rate = component.get("rate", 0)  # e.g., 0.075 for 7.5%
                tax_rate_id = component.get("tax_rate_id")
                
                if not tax_rate_id:
                    raise ValueError(f"tax_rate_id missing for component: {component.get('name')}")
                
                if i == len(tax_components) - 1:
                    # Last component gets the remainder to avoid rounding errors
                    component_tax = round(total_tax - allocated_tax, 2)
                else:
                    # Calculate proportional share: (rate / total_rate) * total_tax
                    component_tax = round((rate / total_tax_rate) * total_tax, 2)
                    allocated_tax += component_tax
                
                tax_lines.append({
                    "Amount": component_tax,
                    "DetailType": "TaxLineDetail",
                    "TaxLineDetail": {
                        "TaxRateRef": {"value": tax_rate_id},
                        "PercentBased": True,
                        "TaxPercent": rate * 100,
                        "NetAmountTaxable": net_base,
                    },
                })
            
            payload["TxnTaxDetail"] = {
                "TotalTax": total_tax,
                "TaxLine": tax_lines,
            }
        except Exception as e:
            # If anything goes wrong, log and let QBO try to compute
            print(f"[WARN] Error building composite tax detail: {e}. QBO may not display tax correctly.")
            pass
    # Note: Company A uses vat_inclusive_7_5 (single-rate with explicit TxnTaxDetail)
    #       Company B uses tax_inclusive_composite (multi-rate with explicit TxnTaxDetail for each component)

    # Payment method (tender type) from memo - query QBO by name
    if payment_method_cache is None:
        payment_method_cache = {}
    payment_method_id = infer_payment_method_id(memo, token_mgr, realm_id, payment_method_cache)
    if payment_method_id:
        payload["PaymentMethodRef"] = {"value": payment_method_id}
    elif memo:
        # Only warn if memo exists but payment method not found
        print(f"[WARN] Payment method '{memo}' not found in QBO, skipping PaymentMethodRef")

    # Location from CSV -> QBO Department (Location tracking)
    if location_name:
        department_id = get_department_id(location_name, token_mgr, realm_id, department_cache, config)
        if department_id:
            payload["DepartmentRef"] = {"value": department_id}
        else:
            print(f"[WARN] Department/Location '{location_name}' not found in QBO, skipping DepartmentRef")
    
    # Deposit account from config (for Company B, may be in CSV; for Company A, use config default)
    deposit_account_name = str(first_row.get("*DepositAccount", "")).strip()
    if deposit_account_name:
        # Try to resolve deposit account by name (Company B pattern)
        # For now, we'll let QBO use its default if not specified
        pass

    # No CustomerRef -> customer left blank (as desired)
    return payload, inventory_created_count, service_created_count, default_fallback_count


def _log_sales_receipt_line_items_for_6270(payload: dict) -> None:
    """
    Log a concise dump of SalesReceipt line items (ItemRef value, name, Qty) for QBO 6270 debugging.
    Does not log secrets or tokens.
    """
    lines = payload.get("Line") or []
    if not lines:
        return
    parts = []
    for i, line in enumerate(lines):
        if line.get("DetailType") != "SalesItemLineDetail":
            continue
        detail = line.get("SalesItemLineDetail") or {}
        item_ref = detail.get("ItemRef") or {}
        item_id = item_ref.get("value", "")
        item_name = item_ref.get("name") or line.get("Description") or ""
        qty = detail.get("Qty", "")
        parts.append(f"  {i + 1}) ItemRef={item_id} name={item_name!r} Qty={qty}")
    if parts:
        print("[INFO] QBO 6270: SalesReceipt line items (InvStartDate may be after TxnDate):")
        print("\n".join(parts))


# Chunk size for QBO "Id in (...)" queries to avoid URL/query limits
_ITEM_IDS_QUERY_CHUNK_SIZE = 20


def _query_items_by_ids(
    token_mgr: TokenManager,
    realm_id: str,
    id_list: List[str],
) -> List[Dict[str, Any]]:
    """
    Query QBO for Item by Id list. Chunked to avoid query limits. Read-only.
    Returns list of dicts with Id, Name, Type, TrackQtyOnHand, InvStartDate, Active.
    """
    if not id_list:
        return []
    result: List[Dict[str, Any]] = []
    for i in range(0, len(id_list), _ITEM_IDS_QUERY_CHUNK_SIZE):
        chunk = id_list[i : i + _ITEM_IDS_QUERY_CHUNK_SIZE]
        safe_ids = [str(uid).replace("'", "''") for uid in chunk]
        id_list_str = "','".join(safe_ids)
        query = f"select Id, Name, Type, TrackQtyOnHand, InvStartDate, Active from Item where Id in ('{id_list_str}')"
        url = f"{BASE_URL}/v3/company/{realm_id}/query?query={quote(query)}&minorversion=70"
        resp = _make_qbo_request("GET", url, token_mgr)
        if resp.status_code != 200:
            continue
        data = resp.json()
        items = data.get("QueryResponse", {}).get("Item", [])
        if not isinstance(items, list):
            items = [items] if items else []
        result.extend(items)
    return result


def _parse_yyyy_mm_dd(s: Optional[str]) -> Optional[datetime]:
    """Parse YYYY-MM-DD from string (uses first 10 chars if longer). Returns None if invalid."""
    if not s:
        return None
    s = str(s).strip()[:10]
    if len(s) != 10:
        return None
    try:
        return datetime.strptime(s, "%Y-%m-%d")
    except ValueError:
        return None


def _diagnose_6270_and_report(
    payload: dict,
    token_mgr: TokenManager,
    realm_id: str,
    config: Any,
) -> None:
    """
    On QBO 6270: parse payload Line[] (ItemRef.value + Qty), query QBO for those
    items by Id (chunked), log receipt-level summary (DocNumber, TxnDate, then
    each item: Name, Id, InvStartDate, Qty), and append rows to
    reports/inventory_start_date_blockers_{company_key}_{TxnDate}.csv.
    Read-only (no QBO writes). Does not fail the run on diagnostic errors.
    """
    doc_number = payload.get("DocNumber", "")
    txn_date = payload.get("TxnDate", "")
    lines = payload.get("Line") or []
    line_items: List[Tuple[str, Any]] = []  # (item_id, qty)
    for line in lines:
        if line.get("DetailType") != "SalesItemLineDetail":
            continue
        detail = line.get("SalesItemLineDetail") or {}
        item_ref = detail.get("ItemRef") or {}
        item_id = item_ref.get("value") or ""
        if not item_id:
            continue
        qty = detail.get("Qty", "")
        line_items.append((str(item_id), qty))

    if not line_items:
        return

    # Collect (item_name, category/description) from payload for observability
    payload_line_names: List[Tuple[str, str]] = []
    for line in lines:
        if line.get("DetailType") != "SalesItemLineDetail":
            continue
        detail = line.get("SalesItemLineDetail") or {}
        item_ref = detail.get("ItemRef") or {}
        item_name = item_ref.get("name", "") or ""
        description = line.get("Description", "") or ""
        payload_line_names.append((item_name, description))
    if payload_line_names:
        print(f"[INFO] QBO 6270: {doc_number} (TxnDate={txn_date}) payload line items (item_name, category): {payload_line_names}")

    # Query QBO for those item IDs (chunked)
    id_list = [item_id for item_id, _ in line_items]
    qbo_items = _query_items_by_ids(token_mgr, realm_id, id_list)
    id_to_item: Dict[str, Dict[str, Any]] = {str(it.get("Id", "")): it for it in qbo_items if it.get("Id")}

    # Parse TxnDate once for comparison (handles YYYY-MM-DD or longer strings like ISO with TZ).
    txn_date_obj = _parse_yyyy_mm_dd(txn_date)

    # Filter to only items where InvStartDate > TxnDate (actual blockers). Compare as dates.
    # Also collect items with missing InvStartDate (shortlist when no blockers found).
    blocking_items: List[Tuple[str, Any]] = []
    items_missing_inv_start: List[Tuple[str, Any]] = []
    for item_id, qty in line_items:
        it = id_to_item.get(item_id, {})
        inv_start = it.get("InvStartDate", "")
        inv_date_str = inv_start[:10] if inv_start and len(str(inv_start)) >= 10 else (str(inv_start) if inv_start else "")
        inv_date_obj = _parse_yyyy_mm_dd(inv_date_str or inv_start)
        if inv_date_obj is None or not inv_date_str.strip():
            items_missing_inv_start.append((item_id, qty))
            continue
        if txn_date_obj is not None and inv_date_obj > txn_date_obj:
            blocking_items.append((item_id, qty))

    # Log only blocking items; or missing shortlist; or single line when neither
    if blocking_items:
        print(f"[INFO] QBO 6270: {doc_number} (TxnDate={txn_date}) blocked by inventory items (InvStartDate > TxnDate):")
        for item_id, qty in blocking_items:
            it = id_to_item.get(item_id, {})
            name = it.get("Name", "")
            inv_start = it.get("InvStartDate", "")
            inv_date_str = inv_start[:10] if inv_start and len(str(inv_start)) >= 10 else str(inv_start or "")
            print(f"  - {name} (Id={item_id}) InvStartDate={inv_date_str} Qty={qty}")
    elif items_missing_inv_start:
        print(f"[INFO] QBO 6270: {doc_number} (TxnDate={txn_date}) — no items with InvStartDate > TxnDate; shortlist of items with missing InvStartDate (possible blockers):")
        for item_id, qty in items_missing_inv_start:
            it = id_to_item.get(item_id, {})
            name = it.get("Name", "") or "(unknown)"
            print(f"  - {name} (Id={item_id}) InvStartDate=(missing) Qty={qty}")
    else:
        print(f"[INFO] QBO 6270: {doc_number} (TxnDate={txn_date}) — no items with InvStartDate > TxnDate (list may be incomplete if QBO did not return all item details)")

    # Append to blockers CSV: blocking items, or (when none) shortlist of items with missing InvStartDate
    if not config:
        return
    company_key = getattr(config, "company_key", "")
    if not company_key or not txn_date:
        return
    rows_to_write = blocking_items if blocking_items else items_missing_inv_start
    if not rows_to_write:
        return
    out_dir = os.path.join(get_repo_root(), "reports")
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    safe_key = re.sub(r"[^\w-]", "_", company_key)
    filename = f"inventory_start_date_blockers_{safe_key}_{txn_date}.csv"
    filepath = os.path.join(out_dir, filename)
    fieldnames = ["DocNumber", "TxnDate", "ItemId", "ItemName", "InvStartDate", "TrackQtyOnHand", "Active", "QuantityOnReceipt"]
    file_exists = os.path.exists(filepath)
    use_missing_shortlist = not blocking_items and items_missing_inv_start
    with open(filepath, "a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            w.writeheader()
        for item_id, qty in rows_to_write:
            it = id_to_item.get(item_id, {})
            inv_start = it.get("InvStartDate", "")
            inv_date_str = inv_start[:10] if inv_start and len(str(inv_start)) >= 10 else str(inv_start or "")
            if use_missing_shortlist:
                inv_date_str = "(missing)"
            w.writerow({
                "DocNumber": doc_number,
                "TxnDate": txn_date,
                "ItemId": item_id,
                "ItemName": it.get("Name", ""),
                "InvStartDate": inv_date_str,
                "TrackQtyOnHand": it.get("TrackQtyOnHand", ""),
                "Active": it.get("Active", ""),
                "QuantityOnReceipt": qty,
            })
    if blocking_items:
        print(f"[INFO] QBO 6270: Appended blockers to {filepath}")
    else:
        print(f"[INFO] QBO 6270: Appended missing-InvStartDate shortlist to {filepath}")


def send_sales_receipt(payload: dict, token_mgr: TokenManager, realm_id: str, config=None):
    """
    Send a Sales Receipt to QuickBooks API.
    
    Args:
        payload: SalesReceipt payload
        token_mgr: TokenManager instance
        realm_id: QBO Realm ID
        config: CompanyConfig instance (optional, required for inventory error handling)
    
    Returns:
        Tuple of (success: bool, inventory_warning: bool, inventory_rejection: bool)
        - success: True if SalesReceipt was created successfully
        - inventory_warning: True if inventory warning detected (but receipt accepted)
        - inventory_rejection: True if inventory rejection detected
    
    Raises RuntimeError if the API returns a non-2xx status code (unless inventory rejection handled).
    """
    url = f"{BASE_URL}/v3/company/{realm_id}/salesreceipt?minorversion=70"

    response = _make_qbo_request(
        "POST",
        url,
        token_mgr,
        json=payload,
    )

    print("Status:", response.status_code)
    
    # Parse response body for logging/error messages
    try:
        body = response.json()
        print(json.dumps(body, indent=2))
    except Exception:
        body = None
        print(response.text)
    
    # Check for inventory-related errors/warnings
    inventory_warning = False
    inventory_rejection = False
    
    # Validate response status first
    is_success = (200 <= response.status_code < 300)
    
    if body:
        # Check for inventory-related messages
        response_text = json.dumps(body).lower()
        inventory_keywords = ["insufficient quantity", "quantity on hand", "inventory", "not enough"]
        
        if is_success:
            # Success response - check for warnings
            if any(phrase in response_text for phrase in inventory_keywords):
                inventory_warning = True
        else:
            # Error response - check if it's inventory-related
            fault = body.get("fault")
            if fault:
                errors = fault.get("error", [])
                if errors:
                    error_text = " ".join([
                        str(err.get("message", "")) + " " + str(err.get("detail", ""))
                        for err in errors
                    ]).lower()
                    if any(phrase in error_text for phrase in inventory_keywords):
                        inventory_rejection = True
    
    # Handle inventory rejection errors
    if not is_success and inventory_rejection:
        if config and config.allow_negative_inventory:
            # QBO rejected due to inventory, but we allow negative inventory
            error_msg = (
                "QBO rejected SalesReceipt due to negative inventory. "
                "Enable negative inventory in QBO settings (Settings → Company Settings → Sales → Allow negative inventory) "
                "or disable inventory items."
            )
            raise RuntimeError(error_msg)
        # If allow_negative_inventory is False, treat as fatal (existing behavior)
    
    # Validate response status - raise error if not successful
    if not is_success:
        error_msg = f"Failed to create Sales Receipt: HTTP {response.status_code}"
        
        # Extract error details from response if available (QBO may use "Fault"/"Error" or "fault"/"error")
        if body:
            fault = body.get("Fault") or body.get("fault")
            if fault:
                errors = fault.get("Error") or fault.get("error") or []
                if errors:
                    error_details = []
                    is_6270 = False
                    for err in errors:
                        if str(err.get("code", "")) == "6270":
                            is_6270 = True
                        detail = err.get("message", err.get("detail", ""))
                        if detail:
                            error_details.append(detail)
                    if error_details:
                        error_msg += f"\nError details: {'; '.join(error_details)}"
                    # On QBO 6270 (InvStartDate > TxnDate): full diagnostic + blockers CSV (non-blocking)
                    if is_6270 and payload:
                        try:
                            _diagnose_6270_and_report(payload, token_mgr, realm_id, config)
                        except Exception as diag_err:
                            print(f"[WARN] 6270 diagnostic failed (non-blocking): {diag_err}")
        
        # Include response text if JSON parsing failed
        if not body:
            error_msg += f"\nResponse: {response.text[:500]}"  # Limit length
        
        raise RuntimeError(error_msg)
    
    # Success - log inventory warning if present
    if inventory_warning and config and config.allow_negative_inventory:
        print(f"[WARN] Inventory warning detected but SalesReceipt accepted (negative inventory allowed)")
    
    return (True, inventory_warning, inventory_rejection)
    
    # Success - verify we got a SalesReceipt back
    if body:
        sales_receipt = body.get("SalesReceipt")
        if sales_receipt:
            receipt_id = sales_receipt.get("Id")
            doc_number = sales_receipt.get("DocNumber")
            if receipt_id:
                print(f"[OK] Sales Receipt created: ID={receipt_id}, DocNumber={doc_number}")
            else:
                print("[WARN] Sales Receipt response missing Id")
        else:
            print("[WARN] Sales Receipt response missing SalesReceipt object")


def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Upload Sales Receipts to QuickBooks for a specific company."
    )
    parser.add_argument(
        "--company",
        required=True,
        choices=get_available_companies(),
        help="Company identifier (REQUIRED). Available: %(choices)s",
    )
    parser.add_argument(
        "--target-date",
        help="Target date in YYYY-MM-DD format (used when trading_day_enabled is True)",
    )
    args = parser.parse_args()
    
    # Load company configuration
    try:
        config = load_company_config(args.company)
    except Exception as e:
        print(f"Error: Failed to load company config for '{args.company}': {e}")
        sys.exit(1)
    
    # Safety check: verify realm_id matches tokens
    try:
        verify_realm_match(config.company_key, config.realm_id)
    except RuntimeError as e:
        print(f"Error: Realm ID safety check failed: {e}")
        sys.exit(1)
    
    # Log company info for safety
    print("=" * 60)
    print(f"COMPANY: {config.display_name} ({config.company_key})")
    print(f"REALM ID: {config.realm_id}")
    print(f"DEPOSIT ACCOUNT: {config.deposit_account}")
    print(f"TAX MODE: {config.tax_mode}")
    print("=" * 60)
    
    # Resolved target_date: CLI arg, or (when trading-day mode) default to yesterday for pre-flight/single-day behavior
    resolved_target_date: Optional[str] = args.target_date
    if not resolved_target_date and getattr(config, "trading_day_enabled", False):
        resolved_target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

    # Initialize token manager (will refresh automatically on 401)
    token_mgr = TokenManager(config.company_key, config.realm_id)

    repo_root = get_repo_root()

    csv_path = find_latest_single_csv(repo_root, config)
    print(f"Using CSV: {csv_path}")

    df = pd.read_csv(csv_path)
    print(f"Loaded {len(df)} rows")

    grouped = df.groupby(GROUP_COL)
    print(f"Found {len(grouped)} distinct SalesReceiptNo groups")

    # Layer A: Load local ledger of uploaded DocNumbers (for reference/stats only)
    ledger_docnumbers = load_uploaded_docnumbers(repo_root, config)
    print(f"Loaded {len(ledger_docnumbers)} DocNumbers from local ledger")

    # Collect all DocNumbers to check
    all_docnumbers = list(grouped.groups.keys())
    
    # Layer B: Check QBO for existing DocNumbers (QBO is the source of truth)
    # Also check TxnDate if target_date is provided (for trading-day mode)
    print("Checking QBO for existing DocNumbers...")
    qbo_existing, date_mismatches = check_qbo_existing_docnumbers(
        all_docnumbers, token_mgr, config.realm_id, target_date=args.target_date
    )
    print(f"Found {len(qbo_existing)} existing DocNumbers in QBO with matching TxnDate")
    
    # Warn about date mismatches (receipts exist but with wrong TxnDate)
    if date_mismatches:
        print(f"\n[WARN] Found {len(date_mismatches)} receipt(s) in QBO with different TxnDate:")
        for docnum, wrong_date in sorted(date_mismatches.items()):
            print(f"  {docnum}: exists in QBO with TxnDate={wrong_date} (expected {args.target_date})")
        print(f"  These will be attempted for upload (will fail with duplicate DocNumber error)")
    
    # Detect stale ledger entries (in ledger but NOT in QBO with matching TxnDate)
    stale_ledger_entries = ledger_docnumbers - qbo_existing - set(date_mismatches.keys())
    stale_in_current_batch = stale_ledger_entries & set(all_docnumbers)
    
    if stale_in_current_batch:
        print(f"\n[WARN] Stale ledger entries detected: {len(stale_in_current_batch)} DocNumber(s) in ledger but not in QBO")
        for docnum in sorted(stale_in_current_batch):
            print(f"  Stale ledger entry detected: {docnum} is in {config.uploaded_docnumbers_file} but not in QBO; will attempt upload.")
    
    # Skip ONLY if DocNumber exists in QBO with matching TxnDate
    # Receipts with date mismatches will be attempted (and will fail, but that's expected)
    skip_docnumbers = qbo_existing
    if skip_docnumbers:
        print(f"Skipping {len(skip_docnumbers)} DocNumbers (confirmed existing in QBO with matching TxnDate)")

    item_cache: Dict[str, str] = {}
    department_cache: Dict[str, Optional[str]] = {}
    payment_method_cache: Dict[str, Optional[str]] = {}
    category_item_cache: Dict[str, str] = {}
    
    # Load category mapping and account cache if inventory enabled
    mapping_cache: Optional[Dict[str, Dict[str, str]]] = None
    account_cache: Dict[str, Optional[str]] = {}
    inventory_start_date_issues_count = 0
    inventory_start_date_report_path: Optional[str] = None
    if config.inventory_enabled:
        print(f"\n[INFO] Inventory mode ENABLED. QtyOnHand starts at {config.default_qty_on_hand}. QBO must allow negative inventory.")
        try:
            mapping_cache = load_category_account_mapping(config)
            print(f"[INFO] Loaded {len(mapping_cache)} category mappings from {config.product_mapping_file}")
        except Exception as e:
            print(f"[ERROR] Failed to load category mapping: {e}")
            raise
        # Pre-flight: find inventory items with InvStartDate > target_date (may cause QBO 6270)
        # Use resolved target_date so pre-flight runs for scheduled runs (e.g. default yesterday) even when CLI --target-date not set
        if resolved_target_date:
            print(f"\n[INFO] Running pre-flight inventory start-date check (target_date={resolved_target_date})")
            try:
                issues = find_inventory_items_with_future_start_date(token_mgr, config.realm_id, resolved_target_date)
                inventory_start_date_issues_count = len(issues)
                if issues:
                    print(f"[WARN] Found {inventory_start_date_issues_count} inventory items with InvStartDate AFTER target_date={resolved_target_date}. "
                          "These may fail with QBO error 6270.")
                    report_path = write_inventory_start_date_issues_report(
                        issues, config.company_key, resolved_target_date, out_dir=os.path.join(repo_root, "reports")
                    )
                    if report_path:
                        inventory_start_date_report_path = report_path
                        print(f"[INFO] Report written: {report_path}")
                else:
                    print(f"[INFO] Pre-flight: 0 issues found.")
            except Exception as e:
                print(f"[WARN] Pre-flight inventory start date check failed (non-blocking): {e}")
    
    # Pre-fetch tax code for Company B (tax_inclusive_composite mode) to validate it exists
    if config.tax_mode == "tax_inclusive_composite" and config.tax_code_name:
        tax_code_cache: Dict[str, Optional[str]] = {}
        tax_code_id = get_tax_code_id_by_name(
            config.tax_code_name,
            token_mgr,
            config.realm_id,
            tax_code_cache
        )
        if tax_code_id:
            print(f"[INFO] Found Tax Code '{config.tax_code_name}' with ID: {tax_code_id}")
            # Store in function-level cache for use in build_sales_receipt_payload
            if not hasattr(build_sales_receipt_payload, '_tax_code_cache'):
                build_sales_receipt_payload._tax_code_cache = {}
            build_sales_receipt_payload._tax_code_cache[config.tax_code_name] = tax_code_id
        else:
            print(f"[WARN] Tax Code '{config.tax_code_name}' not found in QBO.")
            print(f"       Receipts will be created without tax codes.")
            print(f"       You can add 'tax_code_id' to {config.company_key}.json to specify it directly.")
    
    stats = {
        "attempted": 0,
        "skipped": 0,
        "uploaded": 0,
        "failed": 0,
        "stale_ledger_entries_detected": len(stale_in_current_batch),
        "date_mismatches_detected": len(date_mismatches),
        "items_created_count": 0,
        "inventory_items_created_count": 0,
        "service_items_created_count": 0,
        "default_item_fallback_count": 0,
        "inventory_warnings_count": 0,
        "inventory_rejections_count": 0,
        "inventory_start_date_issues_count": inventory_start_date_issues_count,
        "inventory_start_date_report_path": inventory_start_date_report_path,
    }

    items_wrong_type: List[Dict[str, Any]] = []
    items_autofixed: List[Dict[str, Any]] = []
    items_patched_pricing_tax: List[Dict[str, Any]] = []

    for group_key, group_df in grouped:
        stats["attempted"] += 1
        
        # Skip ONLY if exists in QBO with matching TxnDate (QBO is source of truth)
        if group_key in skip_docnumbers:
            print(f"\nSkipping SalesReceiptNo: {group_key} (exists in QBO with matching TxnDate)")
            stats["skipped"] += 1
            # Add to ledger if confirmed in QBO (healing: sync ledger with QBO truth)
            if group_key not in ledger_docnumbers:
                save_uploaded_docnumber(repo_root, group_key, config)
                print(f"  Added {group_key} to ledger (confirmed in QBO)")
            continue
        
        # Check if this receipt has a date mismatch (exists but with wrong TxnDate)
        # We'll attempt upload anyway - it will fail with duplicate DocNumber error, but that's expected
        if group_key in date_mismatches:
            wrong_date = date_mismatches[group_key]
            print(f"\n[WARN] SalesReceiptNo: {group_key} exists in QBO with TxnDate={wrong_date} (expected {args.target_date})")
            print(f"       Attempting upload anyway (will fail with duplicate DocNumber error)")
        
        try:
            payload, inv_created, svc_created, default_fallback = build_sales_receipt_payload(
                group_df, token_mgr, config.realm_id, config, item_cache, department_cache, payment_method_cache,
                target_date=args.target_date,
                mapping_cache=mapping_cache,
                account_cache=account_cache,
                items_wrong_type=items_wrong_type,
                items_autofixed=items_autofixed,
                category_item_cache=category_item_cache,
                items_patched_pricing_tax=items_patched_pricing_tax,
            )
            stats["items_created_count"] += inv_created + svc_created
            stats["inventory_items_created_count"] += inv_created
            stats["service_items_created_count"] += svc_created
            stats["default_item_fallback_count"] += default_fallback
            print(f"\nSending SalesReceiptNo: {group_key}")
            success, inventory_warning, inventory_rejection = send_sales_receipt(payload, token_mgr, config.realm_id, config)
            # Track inventory stats (will be added to stats dict in main)
            if inventory_warning:
                stats.setdefault("inventory_warnings_count", 0)
                stats["inventory_warnings_count"] += 1
            if inventory_rejection:
                stats.setdefault("inventory_rejections_count", 0)
                stats["inventory_rejections_count"] += 1
            
            # Success - add to local ledger
            save_uploaded_docnumber(repo_root, group_key, config)
            stats["uploaded"] += 1
        except Exception as e:
            print(f"\n[ERROR] Failed to upload SalesReceiptNo {group_key}: {e}")
            stats["failed"] += 1
            # Don't add to ledger on failure

    if items_wrong_type:
        reports_dir = Path(repo_root) / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        target_date_str = args.target_date or "unknown"
        report_path = reports_dir / f"items_wrong_type_{config.company_key}_{target_date_str}.csv"
        seen_ids = set()
        rows = []
        for r in items_wrong_type:
            rid = r.get("Id", "")
            if rid and rid not in seen_ids:
                seen_ids.add(rid)
                rows.append(r)
        if rows:
            with open(report_path, "w", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=["Name", "Id", "Type", "ExpectedType"])
                w.writeheader()
                w.writerows(rows)
            print(f"[INFO] Wrote {len(rows)} item(s) with wrong type to {report_path}")

    if items_autofixed:
        reports_dir = Path(repo_root) / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        target_date_str = args.target_date or "unknown"
        report_path = reports_dir / f"items_autofixed_{config.company_key}_{target_date_str}.csv"
        with open(report_path, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["OriginalName", "OldItemId", "OldType", "OldActive", "NewName", "NewInventoryItemId", "TxnDate", "DocNumber"])
            w.writeheader()
            w.writerows(items_autofixed)
        print(f"[INFO] Wrote {len(items_autofixed)} item(s) auto-fixed (renamed/inactivated) to {report_path}")

    if items_patched_pricing_tax:
        reports_dir = Path(repo_root) / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        target_date_str = args.target_date or "unknown"
        report_path = reports_dir / f"items_patched_pricing_tax_{config.company_key}_{target_date_str}.csv"
        fieldnames = ["ItemId", "Name", "Category", "UnitPrice_old", "UnitPrice_new", "PurchaseCost_old", "PurchaseCost_new",
                      "SalesTaxIncluded_old/new", "PurchaseTaxIncluded_old/new", "Taxable_old/new", "TxnDate", "DocNumber"]
        with open(report_path, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
            w.writeheader()
            w.writerows(items_patched_pricing_tax)
        print(f"[INFO] Wrote {len(items_patched_pricing_tax)} item(s) patched (pricing/tax) to {report_path}")

    # Print summary
    print(f"\n=== Upload Summary ===")
    print(f"Attempted: {stats['attempted']}")
    print(f"Skipped (exists in QBO): {stats['skipped']}")
    print(f"Uploaded: {stats['uploaded']}")
    print(f"Failed: {stats['failed']}")
    if stats['stale_ledger_entries_detected'] > 0:
        print(f"Stale ledger entries detected: {stats['stale_ledger_entries_detected']} (healed by uploading)")
    if stats['date_mismatches_detected'] > 0:
        print(f"Date mismatches detected: {stats['date_mismatches_detected']} receipt(s) exist in QBO with wrong TxnDate")
        print(f"  These receipts need manual correction in QBO (change TxnDate to {args.target_date}) or delete and re-upload")
    print(f"\nLedger vs QBO sync:")
    print(f"  DocNumbers in ledger: {len(ledger_docnumbers)}")
    print(f"  DocNumbers confirmed in QBO (matching TxnDate): {len(qbo_existing)}")
    if date_mismatches:
        print(f"  Date mismatches (wrong TxnDate in QBO): {len(date_mismatches)}")
    if stale_in_current_batch:
        print(f"  Stale ledger entries (in ledger, not in QBO): {len(stale_in_current_batch)}")
    
    # Write stats to metadata for Slack notification
    metadata_path = os.path.join(repo_root, config.metadata_file)
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r") as f:
                metadata = json.load(f)
            metadata["upload_stats"] = stats
            with open(metadata_path, "w") as f:
                json.dump(metadata, f, indent=2)
        except Exception as e:
            print(f"[WARN] Failed to update metadata with upload stats: {e}")
    
    # Exit with error code if any uploads failed
    if stats['failed'] > 0:
        print(f"\n[ERROR] {stats['failed']} upload(s) failed. Exiting with error code.")
        sys.exit(1)
    
    # Exit with error code if no uploads succeeded (and there were attempts)
    if stats['attempted'] > 0 and stats['uploaded'] == 0 and stats['skipped'] == 0:
        print(f"\n[ERROR] All {stats['attempted']} upload attempt(s) failed. Exiting with error code.")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path=".gitignore">
# QuickBooks tokens and credentials
# Current: SQLite database (per-company tokens)
qbo_tokens.sqlite
qbo_tokens.sqlite-wal
qbo_tokens.sqlite-shm
*.sqlite-wal
*.sqlite-shm

# Legacy: JSON token files (no longer used)
qbo_tokens.json
qbo_tokens_cache.json
qbo_tokens_gp.json
qbo_tokens_cache_gp.json

# Uploaded docnumbers ledger (per-company)
uploaded_docnumbers*.json


# Environment-specific configuration
# Note: .env.example is NOT ignored (it's a template that should be committed)
.env
.env.local
.env.gp

# Python cache
__pycache__/
*.pyc
*.pyo
*.pyd
.Python

# Temporary processing files (these get archived, but exclude from git)
*.csv
last_epos_transform.json
transform_metadata.json

# RAW spill staging (future-date rows awaiting processing)
# These files are managed by run_pipeline.py and archived after use
uploads/spill_raw/

# Split staging (temporary per-run, cleaned up after success)
# Used for range mode split files and single-day staging
uploads/range_raw/

# Logs
logs/
*.log

# Archived files (authoritative after successful runs, but exclude from git)
Uploaded/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Virtual environments
venv/
env/
ENV/
.venv
myenv/

# Local token broker (personal SSH access tool - keep local only)
token_broker.py
start_broker.ps1
README-infra.md

# Safety: raw EPOS downloads (should be auto-archived after success)
BookKeeping_*.csv
CombinedRaw_*.csv

# Pre-commit hooks: downloaded binaries (auto-downloaded by gitleaks-wrapper)
.pre-commit-hooks/.bin/
</file>

<file path="README.md">
# EPOS → QuickBooks Automation

This repo contains an automation pipeline that:

1. Logs into **EPOS Now HQ** and downloads the daily **BookKeeping** CSV.
2. Splits the raw CSV by date (WAT timezone) and handles **RAW spill** for future dates.
3. Transforms each day's raw data into QuickBooks-ready CSV format.
4. Uploads the data into **QuickBooks Online** as Sales Receipts using the QBO API.
5. Archives all processed files to `Uploaded/<date>/` after successful upload.
6. Reconciles EPOS totals vs QBO totals to verify data integrity.

The pipeline is designed to be run as a single command and take care of all phases in sequence.

---

## TL;DR – Quick Start

1. **Set up credentials:**

   ```bash
   cp .env.example .env
   # Edit .env and fill in your QBO_CLIENT_ID, QBO_CLIENT_SECRET, EPOS_USERNAME, EPOS_PASSWORD
   ```

2. **Create initial OAuth tokens:**

   - Perform OAuth flow to get access/refresh tokens
   - Store tokens in `qbo_tokens.sqlite` using `store_tokens_from_oauth()` (see [Initial Setup](#2-get-initial-oauth-tokens) for details)

3. **Install dependencies:**

   ```bash
   # Create virtual environment (recommended)
   python -m venv .venv

   # Activate virtual environment
   # On Windows (PowerShell):
   .\.venv\Scripts\Activate.ps1
   # On macOS/Linux:
   source .venv/bin/activate

   # Install dependencies
   pip install -r requirements.txt

   # Install Playwright browser (required after installing playwright package)
   playwright install chromium
   ```

4. **Run the pipeline:**

   **Standard (yesterday's data):**

   ```bash
   python run_pipeline.py --company company_a
   ```

   **Specific date:**

   ```bash
   python run_pipeline.py --company company_a --target-date 2025-12-24
   ```

   **Custom date range:**

   ```bash
   python run_pipeline.py --company company_b --from-date 2025-12-08 --to-date 2025-12-14
   ```

   **Skip download (use existing split files):**

   ```bash
   python run_pipeline.py --company company_b --from-date 2025-01-29 --to-date 2025-01-31 --skip-download
   ```

   > **Note:** `--skip-download` only works in range mode and uses existing split files from `uploads/range_raw/`. Useful when you already have CSV files and want to reprocess without re-downloading from EPOS.

That's it! The pipeline will download, split, transform, upload, archive, and reconcile automatically. If `SLACK_WEBHOOK_URL` is configured, you'll receive notifications for pipeline start, success, failure events, and reconciliation results.

> 💡 **Tip:** See [Initial Setup](#initial-setup) below for detailed instructions on each step.
>
> **Note:** All examples use `python` for cross-platform compatibility. On macOS/Linux, use `python3` if `python` points to Python 2 or is missing.

---

## Running the Pipeline for All Companies (Daily Run)

The `run_all_companies.py` script orchestrates running the pipeline for all configured companies in sequence. It's designed for daily automation via cron or Task Scheduler.

**What it does:**

- Runs `run_pipeline.py` once per configured company
- Uses the pipeline's default behavior (processes "yesterday" if no date is supplied)
- Automatically discovers companies via `get_available_companies()`
- Explicitly ignores template/example configs (e.g., `company_example`)

**Usage:**

```bash
# Process all companies (yesterday's data)
python run_all_companies.py

# Process all companies for a specific date
python run_all_companies.py --target-date 2025-12-24

# Process all companies for a date range
python run_all_companies.py --from-date 2025-12-08 --to-date 2025-12-14

# Process specific companies only
python run_all_companies.py --companies company_a company_b

# Skip download (use existing split files in range mode)
python run_all_companies.py --from-date 2025-01-29 --to-date 2025-01-31 --skip-download
```

**Failure behavior:**

- If one company fails, execution stops immediately
- This is intentional to avoid silent partial failures
- Each company still emits its own Slack notifications (if configured)

**Design note:**

This script is intentionally thin — all business logic remains in `run_pipeline.py`. This makes it suitable for cron / Task Scheduler / daily automation where you want a single entry point that processes all companies sequentially.

---

## Architecture Overview

### RAW-First Processing

The pipeline enforces **date correctness BEFORE transformation**:

1. EPOS CSV is treated as a multi-day ledger (may contain rows from multiple dates due to timezone differences)
2. The downloaded CSV is split by WAT date immediately after download
3. Future-date rows become **RAW spill files** (stored for later processing)
4. Transform receives only rows for the target date — it never creates or merges spills

> **Why RAW-first is safer:** Date filtering happens at the raw data level, before any transformation. This prevents double-processing, ensures no rows are lost, and keeps transform.py simple and stateless.

### RAW Spill System (Pipeline-Managed)

When processing date D, if the EPOS download contains rows for future dates (D+1, D+2, etc.):

1. **Creation:** Future rows are written as RAW spill files:

   ```
   uploads/spill_raw/<CompanyDir>/BookKeeping_raw_spill_YYYY-MM-DD.csv
   ```

2. **Merge:** When processing date D+1, the pipeline checks for a RAW spill file and merges it with the split file before transform

3. **Archive:** Used RAW spill files are moved to:

   ```
   Uploaded/YYYY-MM-DD/RAW_SPILL_BookKeeping_raw_spill_YYYY-MM-DD.csv
   ```

4. **Lifecycle:** RAW spill files remain in `uploads/spill_raw/` until their date is processed, then they're archived

> **Note:** There is no `uploads/spill/` directory. The old "transformed spill" system has been removed. All spill handling now happens at the RAW level in `run_pipeline.py`.

### Split Staging (Temporary)

The `uploads/range_raw/` directory is used ONLY as a staging area during processing:

- Single-day: `uploads/range_raw/<CompanyDir>/<date>_to_<date>/`
- Range mode: `uploads/range_raw/<CompanyDir>/<from>_to_<to>/`

These directories are **always cleaned up** after successful runs. No files in `uploads/` are authoritative after success.

### Archive Structure (Authoritative)

After a successful run, all relevant files are archived to:

```
Uploaded/YYYY-MM-DD/
├── ORIGINAL_<EPOS CSV>                          # Original downloaded EPOS CSV
├── RAW_SPLIT_BookKeeping_YYYY-MM-DD.csv         # Split raw file for this date
├── RAW_COMBINED_CombinedRaw_YYYY-MM-DD.csv      # (Only if RAW spill was merged)
├── RAW_SPILL_BookKeeping_raw_spill_*.csv        # (Only if RAW spill was used)
├── gp_sales_receipts_*.csv                      # Transformed/processed CSV
└── transform_metadata.json                       # Processing metadata
```

### Guarantees

- **No duplicate QBO uploads** — Deduplication via local ledger + QBO API checks
- **No silent row loss** — Future rows become RAW spill, past rows are logged
- **Spill rows processed exactly once** — RAW spills are archived after use
- **Repo root clean after success** — Original EPOS CSV is archived, staging dirs removed

---

## Files / Scripts

### Core Pipeline Scripts

- `run_pipeline.py`  
  **Main entry point** — Orchestrates all phases for single-day or range mode:

  1. Download EPOS CSV (`epos_playwright.py`)
  2. Split by WAT date and create RAW spill files for future dates
  3. Merge RAW spill (if exists for target date)
  4. Transform to QuickBooks CSV (`transform.py`)
  5. Upload to QuickBooks (`qbo_upload.py`)
  6. Archive all files to `Uploaded/<date>/`
  7. Reconcile EPOS vs QBO totals

  **Usage:**

  ```bash
  # Single-day (yesterday)
  python run_pipeline.py --company company_a

  # Single-day (specific date)
  python run_pipeline.py --company company_a --target-date 2025-12-24

  # Date range
  python run_pipeline.py --company company_b --from-date 2025-12-08 --to-date 2025-12-14

  # Skip download (use existing split files in uploads/range_raw/)
  python run_pipeline.py --company company_b --from-date 2025-01-29 --to-date 2025-01-31 --skip-download
  ```

  **Skip Download Mode:**

  The `--skip-download` flag allows you to process existing split CSV files without downloading from EPOS. This is useful when:
  - You already have split files in `uploads/range_raw/` from a previous run
  - You want to reprocess data without re-downloading
  - You're working with manually prepared CSV files

  **Requirements:**
  - Only works in range mode (`--from-date` and `--to-date` required)
  - Split files must exist in `uploads/range_raw/<CompanyDir>/<range_folder>/`
  - Files should be named `BookKeeping_YYYY-MM-DD.csv` or `CombinedRaw_YYYY-MM-DD.csv`

- `epos_playwright.py`  
  Uses **Playwright** to log into EPOS Now, navigate to the BookKeeping report, and download the CSV.
  Supports both single-date (`--target-date`) and range (`--from-date` / `--to-date`) downloads.

- `transform.py`  
  Transforms raw EPOS CSV into QuickBooks-ready format using company-specific configuration.

  **Important:** Transform.py receives a pre-filtered raw file via `--raw-file` and transforms only that data. All date filtering and spill handling happens at the RAW level in `run_pipeline.py`.

- `qbo_upload.py`  
  Uploads transformed CSV to QuickBooks Online via REST API.

  **Features:**

  - **Deduplication (Layer A)**: Local ledger tracks uploaded DocNumbers
  - **Deduplication (Layer B)**: Bulk QBO API checks before uploading
    - In **trading-day mode** with `--target-date`: Checks both DocNumber AND TxnDate to ensure receipts exist with the correct trading date
    - In calendar-day mode: Checks DocNumber only
  - Automatic token refresh on 401 errors
  - Location/Department mapping
  - VAT-inclusive amount handling

### Configuration

- `company_config.py` — Loads company-specific settings from JSON files
- `companies/company_a.json` — Company A configuration
- `companies/company_b.json` — Company B configuration

### Supporting Files

- `token_manager.py` — QuickBooks OAuth2 token management (SQLite storage, per-company tokens)
- `query_qbo_for_company.py` — QuickBooks query/reconciliation tool
- `slack_notify.py` — Slack notification helpers
- `load_env.py` — Environment variable loader

### Data Folders

- `Uploaded/<date>/` — **Authoritative archive** after successful runs
- `uploads/spill_raw/` — RAW spill files awaiting processing (temporary)
- `uploads/range_raw/` — Split staging during processing (temporary, cleaned up)
- `logs/` — Pipeline execution logs

---

## Folder Structure

```text
code-scripts/
├── run_pipeline.py              # Main orchestrator
├── epos_playwright.py           # EPOS download
├── transform.py                 # CSV transformation
├── qbo_upload.py                # QuickBooks upload
├── company_config.py            # Company config loader
├── companies/
│   ├── company_a.json
│   └── company_b.json
│
├── uploads/                     # TEMPORARY staging (ignored by git)
│   ├── spill_raw/              # RAW spill files for future dates
│   │   └── <CompanyDir>/
│   │       └── BookKeeping_raw_spill_YYYY-MM-DD.csv
│   └── range_raw/              # Split staging (cleaned after success)
│       └── <CompanyDir>/
│           └── <from>_to_<to>/
│               ├── BookKeeping_YYYY-MM-DD.csv
│               └── CombinedRaw_YYYY-MM-DD.csv
│
├── Uploaded/                    # AUTHORITATIVE archive (ignored by git)
│   └── YYYY-MM-DD/
│       ├── ORIGINAL_*.csv
│       ├── RAW_SPLIT_*.csv
│       ├── RAW_COMBINED_*.csv   # (if spill merged)
│       ├── RAW_SPILL_*.csv      # (if spill used)
│       ├── gp_sales_receipts_*.csv
│       └── transform_metadata.json
│
└── logs/                        # Execution logs (ignored by git)
    └── pipeline_YYYYMMDD-HHMMSS.log
```

---

## Workflow Details

### Single-Day Mode

```bash
python run_pipeline.py --company company_a --target-date 2025-12-28
```

**Flow:**

1. **Download:** EPOS CSV for 2025-12-28 → repo root
2. **Split:** By WAT date
   - Rows for 2025-12-28 → `uploads/range_raw/.../BookKeeping_2025-12-28.csv`
   - Rows for 2025-12-29 → `uploads/spill_raw/.../BookKeeping_raw_spill_2025-12-29.csv`
3. **Merge:** Check if RAW spill exists for 2025-12-28, merge if so
4. **Transform:** Process merged/split file via `transform.py --raw-file ...`
5. **Upload:** Send to QuickBooks
6. **Archive:** Move all artifacts to `Uploaded/2025-12-28/`
7. **Cleanup:** Remove staging dirs, archive original CSV from repo root

### Range Mode

```bash
python run_pipeline.py --company company_b --from-date 2025-12-08 --to-date 2025-12-14
```

**Flow:**

1. **Download:** EPOS CSV for full range → repo root (skipped if `--skip-download` is used)
2. **Split:** By WAT date (all days) — or use existing split files if `--skip-download`
   - Rows for 2025-12-26 → `uploads/range_raw/.../BookKeeping_2025-12-26.csv`
   - Rows for 2025-12-27 → `uploads/range_raw/.../BookKeeping_2025-12-27.csv`
   - Rows for 2025-12-28 → `uploads/range_raw/.../BookKeeping_2025-12-28.csv`
   - Rows for 2025-12-29 → `uploads/spill_raw/.../BookKeeping_raw_spill_2025-12-29.csv`
3. **Loop per day:** For each day in range:
   - Check/merge RAW spill
   - Transform
   - Upload
   - Archive
4. **Final archive:** Archive range staging folder and original CSV (if downloaded)

**Skip Download Mode:**

When using `--skip-download`, the pipeline:
- Skips the EPOS download step
- Searches for existing split files in `uploads/range_raw/`
- Processes each day's split file (or `CombinedRaw_` file if spill was merged)
- Archives split files after successful completion
- Note: Trading-day cutoff info is included, but per-date reassignment counts are unavailable (requires original raw CSV)

### Timeline Example: RAW Spill Flow

**Day 1: Process 2025-12-27**

```
Download EPOS → Contains rows: 12-27 (500 rows), 12-28 (23 rows)
Split:
  → BookKeeping_2025-12-27.csv (500 rows) → transform → upload → archive
  → BookKeeping_raw_spill_2025-12-28.csv (23 rows) → stays in spill_raw/
```

**Day 2: Process 2025-12-28**

```
Download EPOS → Contains rows: 12-28 (480 rows), 12-29 (15 rows)
Split:
  → BookKeeping_2025-12-28.csv (480 rows)
  → BookKeeping_raw_spill_2025-12-29.csv (15 rows) → stays in spill_raw/
Merge: Found spill for 12-28! Merge 480 + 23 = 503 rows
  → CombinedRaw_2025-12-28.csv (503 rows) → transform → upload → archive
Archive: RAW_SPILL_BookKeeping_raw_spill_2025-12-28.csv moved to Uploaded/2025-12-28/
```

---

## Slack Notifications

If `SLACK_WEBHOOK_URL` is configured, the pipeline sends:

- **Start:** Pipeline beginning (includes date/range and company)
- **Watchdog Update:** When RAW spills are created or merged (high-signal only)
- **Success:** All phases completed with summary
- **Failure:** Critical error with concise reason

**Watchdog messages include:**

- Future RAW spill creation: `"Future raw spill: 2025-12-29 (23 rows)"`
- RAW spill merge: `"2025-12-28: merged target split (480 rows) + raw spill (23 rows) -> final (503 rows)"`

**Range Mode Final Summary:**

When running in range mode (`--from-date` / `--to-date`), the final success message includes **Range Totals** that sum reconciliation results across all days:

```
• Range Totals (sum of per-day reconciliation):
  – EPOS: ₦X (N receipts)
  – QBO: ₦Y (M receipts)
  – Difference: ₦(X-Y)
```

If some days had reconciliation NOT RUN, the header shows: `Range Totals (partial — K/T days included):`

---

## Requirements

- **Python 3.9+**
- **EPOS Now HQ** account credentials
- **QuickBooks Online** account with Developer app access

### Install

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # macOS/Linux
# .\.venv\Scripts\Activate.ps1  # Windows

# Install dependencies
pip install -r requirements.txt

# Install Playwright browser
playwright install chromium
```

---

## Initial Setup

### 1. Configure Credentials

Copy and edit the environment file:

```bash
cp .env.example .env
```

Required variables:

```
# QuickBooks OAuth credentials (shared across all companies)
QBO_CLIENT_ID=your_client_id
QBO_CLIENT_SECRET=your_client_secret

# EPOS credentials (company-specific)
EPOS_USERNAME_A=your_epos_username_for_company_a
EPOS_PASSWORD_A=your_epos_password_for_company_a
EPOS_USERNAME_B=your_epos_username_for_company_b
EPOS_PASSWORD_B=your_epos_password_for_company_b

# Slack webhooks (optional, company-specific)
SLACK_WEBHOOK_URL_A=your_slack_webhook_for_company_a  # Optional
SLACK_WEBHOOK_URL_B=your_slack_webhook_for_company_b  # Optional
```

**Note:** `QBO_REALM_ID` is **not** required as an environment variable. Realm IDs are configured per-company in `companies/company_a.json` and `companies/company_b.json`.

### 2. Get Initial OAuth Tokens

The pipeline uses `qbo_tokens.sqlite` to store OAuth tokens, isolated by company and realm_id.

**For each company:**

1. Perform OAuth flow via Intuit's OAuth playground or your OAuth implementation
2. Store tokens using the helper script `store_tokens.py`:

**Example store command (company_a):**

```bash
python store_tokens.py --company company_a --access-token "..." --refresh-token "..." --expires-in 3600 --env production
```

**Example store command (company_b):**

```bash
python store_tokens.py --company company_b --access-token "..." --refresh-token "..." --expires-in 3600 --env production
```

**Example list command (view stored tokens):**

```bash
python store_tokens.py --list
```

**Notes:**

- `qbo_tokens.sqlite` is local state, gitignored, and must be created per machine (or copied manually)
- Do not commit tokens or the database file
- The script automatically loads the `realm_id` from your company configuration file
- Optional: You can use a GUI tool like [DB Browser for SQLite](https://sqlitebrowser.org/) to view the database contents (useful for debugging or verifying stored tokens)

**Adding a second company:** Simply run the OAuth flow again for the new company and store tokens using the same script with the new company's `--company` argument. The SQLite database stores tokens separately per company.

### 3. Verify .gitignore

Ensure these are ignored:

- `qbo_tokens.sqlite` — OAuth tokens database (SQLite)
- `*.sqlite-wal`, `*.sqlite-shm` — SQLite sidecar files
- `.env` — Credentials
- `uploads/` — Temporary staging
- `Uploaded/` — Archive
- `logs/` — Execution logs
- `*.csv` — Processing files

### 4. (Optional) Enable Pre-commit Secret Scanning

To catch hardcoded secrets before committing, you can enable pre-commit hooks:

```bash
# Install pre-commit (or use requirements-dev.txt)
pip install -r requirements-dev.txt
# OR: pip install pre-commit

# Install the git hooks
pre-commit install

# Run on all files (optional, to check existing code)
pre-commit run --all-files
```

**Note:** The pre-commit hook will automatically download gitleaks (v8.18.0) on first run. You do not need to install gitleaks manually — it's fully self-contained and works on macOS, Windows, and Linux.

This will automatically scan for secrets before each commit. The same scanning also runs in CI on pull requests and will block PRs if secrets are detected.

**Note:** Secret scanning is enforced in CI regardless of whether you use pre-commit locally.

---

## Adding a New Company

The pipeline supports multiple companies, each with its own configuration file. Company configs use a **flexible schema** — different companies may have different fields depending on their requirements (tax modes, location mapping, etc.).

### Step-by-Step: Adding `company_c`

1. **Copy the template:**

   ```bash
   cp companies/company.example.json companies/company_c.json
   ```

2. **Edit `companies/company_c.json` and update required fields:**

   **Required (minimum viable schema):**

   - `company_key`: `"company_c"` (must match filename)
   - `qbo.realm_id`: Your QBO Realm ID (replace `"REPLACE_WITH_YOUR_REALM_ID"`)
   - `qbo.deposit_account`: Your deposit account name (e.g., `"100900 - Undeposited Funds"`)
   - `epos.username_env_key`: Environment variable name (e.g., `"EPOS_USERNAME_C"`)
   - `epos.password_env_key`: Environment variable name (e.g., `"EPOS_PASSWORD_C"`)
   - `transform.group_by`: Choose grouping strategy:
     - `["date", "tender"]` — Simple grouping (like Company A)
     - `["date", "location", "tender"]` — Location-aware grouping (like Company B)
   - `transform.date_format`: Date format string (e.g., `"%Y-%m-%d"` or `"%d/%m/%Y"`)
   - `transform.receipt_prefix`: Receipt prefix (e.g., `"SR"`)
   - `transform.receipt_number_format`: Choose format:
     - `"date_tender_sequence"` — For simple grouping (SR-YYYYMMDD-SEQ)
     - `"date_location_sequence"` — For location-aware grouping (SR-YYYYMMDD-LOC-SEQ)
   - `output.csv_prefix`: Unique prefix for CSV files (e.g., `"sales_receipts"`)
   - `output.metadata_file`: Unique metadata filename (e.g., `"last_transform.json"`)
   - `output.uploaded_docnumbers_file`: Unique ledger filename (e.g., `"uploaded_docnumbers.json"`)

   > **Note:** `metadata_file` and `uploaded_docnumbers_file` are per-company state files. They may differ between companies depending on transform logic and should remain unique. For example, Company A uses `last_epos_transform.json` while Company B uses `last_gp_transform.json` — this prevents state file conflicts when running the pipeline for different companies.

   **Optional fields (configure as needed):**

   - `display_name`: Human-readable company name (defaults to `company_key` if omitted)
   - `qbo.tax_mode`:
     - `"vat_inclusive_7_5"` (default) — Single-rate VAT
     - `"tax_inclusive_composite"` — Multi-component tax (requires `tax_components`)
   - `qbo.tax_rate`: Tax rate as decimal (defaults to `0.075` if omitted)
   - `qbo.tax_code_id`: QBO Tax Code ID (optional, used if provided)
   - `qbo.tax_code_name`: Tax code name to query from QBO (optional)
   - `qbo.tax_rate_id`: QBO Tax Rate ID (required for `vat_inclusive_7_5` mode if `tax_code_id` not set)
   - `qbo.default_item_id`: Default item ID (defaults to `"1"`)
   - `qbo.default_income_account_id`: Default income account ID (defaults to `"1"`)
   - `qbo.department_mapping`: Maps location names to QBO Department IDs (empty object `{}` if not needed)
   - `transform.location_mapping`: Maps EPOS location names to location codes (empty object `{}` if not needed)
   - `slack.webhook_url_env_key`: Environment variable name or direct URL for Slack notifications (entire `slack` section optional)

   **Conditional fields (required only for specific tax modes):**

   - `qbo.tax_components`: **Required only if `tax_mode == "tax_inclusive_composite"`**. Array of tax components:
     ```json
     "tax_components": [
       {"name": "VAT", "rate": 0.075, "tax_rate_id": "17"},
       {"name": "Lagos State", "rate": 0.05, "tax_rate_id": "30"}
     ]
     ```

3. **Add environment variables to `.env`:**

   ```bash
   EPOS_USERNAME_C=your_epos_username
   EPOS_PASSWORD_C=your_epos_password
   SLACK_WEBHOOK_URL_C=your_slack_webhook_url  # Optional
   ```

4. **Authorize QBO tokens:**

   Follow the OAuth flow (see [Initial Setup](#2-get-initial-oauth-tokens)) and store tokens for `company_c`:

   ```python
   from token_manager import store_tokens_from_oauth
   from company_config import load_company_config

   config = load_company_config("company_c")
   store_tokens_from_oauth(
       company_key="company_c",
       realm_id=config.realm_id,
       access_token="your_access_token",
       refresh_token="your_refresh_token",
       expires_in=3600
   )
   ```

5. **Test the configuration:**

   ```bash
   python run_pipeline.py --company company_c --target-date 2025-01-01
   ```

### Configuration Schema Notes

- **Flexible schema:** Company configs may vary — some companies need `department_mapping`, others don't. The code handles missing optional fields gracefully.
- **Tax mode differences:**
  - `vat_inclusive_7_5`: Single tax rate, requires `tax_code_id` or `tax_rate_id`
  - `tax_inclusive_composite`: Multiple tax components, requires `tax_components` array
- **Location handling:**
  - If `group_by` includes `"location"`, you'll likely need `location_mapping` to map EPOS locations to codes
  - If `receipt_number_format == "date_location_sequence"`, location codes are used in receipt numbers
- **All company config files are committed to git** (they contain no secrets, only configuration and environment variable key names)

---

## Inventory Items Configuration

The pipeline supports creating QBO Inventory items (instead of Service items) when products don't exist in QuickBooks. This feature is configurable per company and uses category-based account mapping.

### Configuration

Add an optional `inventory` section to your company JSON config:

```json
{
  "inventory": {
    "enable_inventory_items": false,
    "allow_negative_inventory": false,
    "inventory_start_date": "today",
    "default_qty_on_hand": 0,
    "product_mapping_file": "mappings/Product.Mapping.csv"
  }
}
```

**Fields:**
- `enable_inventory_items`: Enable inventory item creation (default: `false`)
- `allow_negative_inventory`: Allow negative inventory when posting SalesReceipts (default: `false`)
- `inventory_start_date`: Start date for inventory tracking - use `"today"` or ISO date like `"2026-01-26"` (default: `"today"`)
- `default_qty_on_hand`: Starting quantity for new inventory items (default: `0`)
- `product_mapping_file`: Path to category mapping CSV (default: `"mappings/Product.Mapping.csv"`)

### Environment Variable Overrides

Precedence: **ENV → company JSON → defaults**

You can override inventory settings via environment variables:

```bash
COMPANY_A_ENABLE_INVENTORY_ITEMS=true
COMPANY_A_ALLOW_NEGATIVE_INVENTORY=true
COMPANY_A_INVENTORY_START_DATE=2026-01-26  # or "today"
COMPANY_A_DEFAULT_QTY_ON_HAND=0
```

### Product Category Mapping

The pipeline uses `mappings/Product.Mapping.csv` to map EPOS product categories to QBO accounts. The CSV must have these exact headers:

- `Category` — EPOS product category (matches EPOS CSV "Category" column)
- `Inventory Account` — Asset account (e.g., `"120000 - Inventory:120100 - Grocery"`)
- `Revenue Account` — Income account (e.g., `"400000 - Revenue:400100 - Revenue - Grocery"`)
- `Cost of Sale account` — COGS account (e.g., `"200000 - Cost of sales:200100 - Purchases - Groceries"`)

**Account Resolution:**
- Accounts are resolved by `FullyQualifiedName` first
- Falls back to `AccountNumber` if FullyQualifiedName not found
- Account strings format: `"<AccountNumber> - <FullyQualifiedName>"`

**Important:** If any EPOS category is missing in the mapping CSV, the pipeline will fail with a clear error message.

### QuickBooks Settings

When `allow_negative_inventory` is enabled, you must also enable negative inventory in QuickBooks:

1. Go to **Settings** → **Company Settings** → **Sales**
2. Enable **"Allow negative inventory"**
3. Save changes

If negative inventory is not enabled in QBO, SalesReceipts will be rejected with an error message.

### Example: Company A Configuration

```json
{
  "company_key": "company_a",
  "inventory": {
    "enable_inventory_items": true,
    "allow_negative_inventory": true,
    "inventory_start_date": "today",
    "default_qty_on_hand": 0
  }
}
```

### Behavior

**When `enable_inventory_items` is `true`:**
- Missing products are created as **Inventory items** (not Service items)
- Items start with `QtyOnHand = default_qty_on_hand` (typically 0)
- Accounts are mapped from category using `Product.Mapping.csv`
- Unit prices are set from EPOS CSV `NET Sales` column (per-unit)
- Purchase costs are set from EPOS CSV `Cost Price` column (per-unit)

**When `enable_inventory_items` is `false` (default):**
- Missing products are created as **Service items** (existing behavior)
- No account mapping required
- No inventory tracking

**Negative Inventory Handling:**
- If `allow_negative_inventory` is `true` and QBO accepts the SalesReceipt (with warnings), the pipeline continues and logs a warning
- If QBO rejects due to inventory, the pipeline fails with instructions to enable negative inventory in QBO settings
- If `allow_negative_inventory` is `false`, inventory errors are treated as fatal (existing behavior)

### Pre-flight: Inventory Start Date Check (QBO 6270)

When inventory is enabled and a run has a target date, the upload step runs a **non-blocking** pre-flight check before sending SalesReceipts:

- It queries QBO for Inventory items whose **InvStartDate** is **after** the run’s target date.
- Such items can cause QBO error **6270** (“Transaction date is prior to start date for inventory item”) when posting backdated receipts.
- If any are found:
  - A **warning** is logged with the count.
  - A CSV report is written to `reports/inventory_start_date_issues_{company_key}_{target_date}.csv` (columns: Id, Name, InvStartDate, Active).
- The run **does not fail**; this is reporting-only. Fix items in QBO (edit InvStartDate or delete/re-upload) and re-run as needed.

### Verification Checklist

After enabling inventory items, verify:
- [ ] No "Uncategorised items or services" in Profit & Loss
- [ ] Products appear as Inventory items (not Service) in QBO
- [ ] Inventory items show correct accounts (Asset, Income, COGS)
- [ ] Companies without inventory enabled still create Service items (unchanged behavior)
- [ ] Slack summary includes inventory stats (items created, warnings, rejections)

---

## Troubleshooting

### RAW Spill Not Being Merged

- Verify spill file exists: `uploads/spill_raw/<CompanyDir>/BookKeeping_raw_spill_YYYY-MM-DD.csv`
- File name must match target date exactly
- Check logs for "Found raw spill file for..." message

### Duplicate Sales Receipts

The pipeline includes automatic deduplication:

- **Layer A:** Local ledger (`uploaded_docnumbers.json`) — tracks DocNumbers that have been uploaded
- **Layer B:** Bulk QBO API check before upload
  - **Trading-day mode** (when `trading_day.enabled: true` and `--target-date` is provided): Checks both DocNumber AND TxnDate to ensure receipts exist with the correct trading date. This prevents skipping receipts that exist with the wrong date.
  - **Calendar-day mode:** Checks DocNumber only

**QBO is the source of truth:** If a DocNumber exists in QBO (with matching TxnDate in trading-day mode), the upload is skipped. Stale ledger entries (in ledger but not in QBO) are detected, logged, and healed by attempting upload.

If you need to re-upload, delete existing receipts first using `query_qbo_for_company.py`.

### Token Refresh Fails

- Refresh tokens expire after ~100 days
- Re-authorize via OAuth flow to get new tokens and store using `store_tokens_from_oauth()`
- Verify `QBO_CLIENT_ID` and `QBO_CLIENT_SECRET` are correct in `.env`
- Check that tokens exist in `qbo_tokens.sqlite` for the company/realm_id combination

### Missing Environment Variables

```bash
# Check if set
echo $QBO_CLIENT_ID

# Use .env file (recommended) or export directly
export QBO_CLIENT_ID="your_id"
```

---

## Security Best Practices

- **Credentials:** Use `.env` file or environment variables, never hardcode
- **Tokens:** `qbo_tokens.sqlite` is auto-created with restricted permissions (0o600)
- **Git:** `.gitignore` excludes all sensitive files (including `qbo_tokens.sqlite` and SQLite sidecar files)
- **Production:** Use a secrets manager (AWS Secrets Manager, HashiCorp Vault)

---

## Design Notes

### RAW-First Design

The pipeline uses a RAW-first approach: all date filtering and spill handling happens at the raw data level in `run_pipeline.py`, before transformation. This ensures:

1. **Single source of truth:** Date filtering happens once, at download time
2. **No double-processing:** Rows are assigned to exactly one date
3. **Stateless transform:** `transform.py` receives pre-filtered data and has no knowledge of spills
4. **Clear lifecycle:** RAW spill files are created, awaited, merged, and archived — never modified

### Why RAW-First Is Safer

- **Single source of truth:** Date filtering happens once, at download time
- **Immutable spill files:** RAW spill files are never modified, only archived
- **Clear lifecycle:** Create → Await → Merge → Archive
- **Stateless transform:** `transform.py` has no knowledge of spills

---

## Notes

- Start with a **QuickBooks sandbox** before using production credentials
- Files are automatically archived after success — check `Uploaded/<date>/` if looking for processed data
- The pipeline cleans up staging directories after success — `uploads/range_raw/` should be empty
- RAW spill files stay in `uploads/spill_raw/` until their date is processed
</file>

<file path="run_pipeline.py">
import subprocess
import sys
import json
import shutil
import argparse
import os
import re
from pathlib import Path
from typing import Optional
import logging
from datetime import datetime, timedelta

from load_env import load_env_file
from slack_notify import (
    notify_pipeline_success,
    notify_pipeline_failure,
    notify_pipeline_start,
    notify_pipeline_update,
)
# qbo_query imported lazily (only when reconciliation is needed) to avoid QBO_REALM_ID requirement
from company_config import load_company_config, get_available_companies
from token_manager import verify_realm_match
import pandas as pd
from typing import List

# Load .env file to make environment variables available (shared secrets only)
load_env_file()


def company_dir_name(display_name: str) -> str:
    """
    Convert company display name to Title_Case_With_Underscores.
    Safe for filesystem paths across OSes.
    
    Examples:
        - Akponora Ventures Ltd → Akponora_Ventures_Ltd
        - Precious & Sons Nigeria → Precious_Sons_Nigeria
        - MAIN STORE (HQ) → Main_Store_Hq
    """
    # Remove special characters (anything not alphanumeric or space)
    name = re.sub(r"[^A-Za-z0-9 ]+", " ", str(display_name or "").strip())
    # Collapse multiple spaces
    name = re.sub(r"\s+", " ", name).strip()
    if not name:
        return "Company"
    # Title case each word and join with underscores
    return "_".join(word.capitalize() for word in name.split())


def run_step(label: str, script_name: str, args: list = None) -> None:
    """
    Run a Python script in this repo using the current interpreter.
    Raises SystemExit if the script exits with a non-zero status.
    
    Args:
        label: Human-readable label for logging
        script_name: Name of the script file to run
        args: Optional list of command-line arguments to pass to the script
    """
    repo_root = Path(__file__).resolve().parent
    script_path = repo_root / script_name

    if not script_path.exists():
        error_msg = f"[ERROR] {label}: script not found at {script_path}"
        logging.error(error_msg)
        raise SystemExit(error_msg)

    cmd = [sys.executable, str(script_path)]
    if args:
        cmd.extend(args)

    logging.info(f"\n=== {label} ===")
    logging.info(f"Running: {' '.join(cmd)}")

    result = subprocess.run(
        cmd,
        cwd=str(repo_root),
        capture_output=True,
        text=True,
    )

    # Log stdout and stderr
    if result.stdout:
        logging.info("Script output:")
        for line in result.stdout.splitlines():
            logging.info(f"  {line}")
    if result.stderr:
        logging.warning("Script errors:")
        for line in result.stderr.splitlines():
            logging.warning(f"  {line}")

    if result.returncode != 0:
        error_msg = f"[ERROR] {label} failed with exit code {result.returncode}"
        if result.stdout:
            error_msg += f"\nOutput: {result.stdout}"
        if result.stderr:
            error_msg += f"\nErrors: {result.stderr}"
        logging.error(error_msg)
        raise SystemExit(error_msg)

    logging.info(f"[OK] {label} completed successfully.")


repo_root = Path(__file__).resolve().parent
logs_dir = repo_root / "logs"
logs_dir.mkdir(exist_ok=True)
log_file = logs_dir / f"pipeline_{datetime.now().strftime('%Y%m%d-%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler(sys.stdout)
    ],
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def archive_range_raw_files(
    repo_root: Path,
    company_dir: str,
    company_key: str,
    from_date: str,
    to_date: str,
    original_csv_path: Path
) -> None:
    """
    Archive the original range CSV and all split raw files after successful range completion.
    
    Moves:
        uploads/range_raw/<company_dir>/<from>_to_<to>/
    To:
        Uploaded/ranges/<company_dir>/<from>_to_<to>/
    
    Uses human-readable company_dir (Title_Case_With_Underscores) for folder names.
    Falls back to legacy company_key folders if company_dir folders don't exist.
    
    Renames the original CSV to ORIGINAL_<filename>.csv.
    
    This function is only called after ALL per-day loops complete successfully.
    If any per-day step fails, this function is NOT called (range remains atomic).
    
    Args:
        repo_root: Repository root path
        company_dir: Human-readable company folder name (Title_Case_With_Underscores)
        company_key: Company identifier (for legacy fallback)
        from_date: Start date in YYYY-MM-DD format
        to_date: End date in YYYY-MM-DD format
        original_csv_path: Path to the original downloaded CSV file (may be None in skip-download mode, or may not exist if already moved)
    """
    date_range = f"{from_date}_to_{to_date}"
    
    # Primary path using human-readable company_dir
    range_raw_dir = repo_root / "uploads" / "range_raw" / company_dir / date_range
    archive_dir = repo_root / "Uploaded" / "ranges" / company_dir / date_range
    
    # Legacy fallback: check if old company_key folder exists instead
    used_legacy = False
    if not range_raw_dir.exists():
        legacy_range_raw_dir = repo_root / "uploads" / "range_raw" / company_key / date_range
        if legacy_range_raw_dir.exists():
            logging.warning(
                f"Range raw directory not found at '{company_dir}', using legacy path '{company_key}'"
            )
            range_raw_dir = legacy_range_raw_dir
            # Keep archive destination with new naming (company_dir), not legacy
            used_legacy = True
        else:
            logging.warning(f"Range raw directory not found: {range_raw_dir}")
            logging.warning(f"  (Also checked legacy path: {legacy_range_raw_dir})")
            return
    
    # Create archive directory
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    # Move original CSV to archive with ORIGINAL_ prefix (if it still exists in repo root)
    # Skip if original_csv_path is None (skip-download mode)
    if original_csv_path is not None:
        if original_csv_path.exists() and original_csv_path.is_file():
            original_archive_name = f"ORIGINAL_{original_csv_path.name}"
            original_archive_path = archive_dir / original_archive_name
            # Check if already exists (shouldn't happen, but safety check)
            if not original_archive_path.exists():
                shutil.move(str(original_csv_path), str(original_archive_path))
                logging.info(f"Moved original CSV: {original_csv_path.name} -> {original_archive_name}")
            else:
                logging.warning(f"Original CSV already archived: {original_archive_name}")
        else:
            logging.info(f"Original CSV not found in repo root (may have been moved already): {original_csv_path.name}")
    else:
        logging.info("Skip-download mode: no original CSV to archive")
    
    # Move all files from range_raw_dir to archive_dir
    items_moved = 0
    for item in list(range_raw_dir.iterdir()):  # Convert to list to avoid iteration issues
        dest = archive_dir / item.name
        if item.is_file():
            if not dest.exists():  # Safety check: don't overwrite
                shutil.move(str(item), str(dest))
                logging.info(f"Moved split file: {item.name}")
                items_moved += 1
            else:
                logging.warning(f"Split file already exists in archive: {item.name}")
        elif item.is_dir():
            if not dest.exists():
                shutil.move(str(item), str(dest))
                logging.info(f"Moved directory: {item.name}")
                items_moved += 1
            else:
                logging.warning(f"Directory already exists in archive: {item.name}")
    
    # Remove empty range_raw_dir
    try:
        range_raw_dir.rmdir()
        logging.info(f"Removed empty range raw directory: {range_raw_dir}")
    except OSError:
        # Directory not empty or already removed - this is OK
        pass
    
    # Remove empty parent directories if possible (cleanup)
    # Use the actual parent (could be company_dir or company_key if legacy)
    parent_folder = company_key if used_legacy else company_dir
    try:
        parent_dir = repo_root / "uploads" / "range_raw" / parent_folder
        if parent_dir.exists() and not any(parent_dir.iterdir()):
            parent_dir.rmdir()
            logging.info(f"Removed empty parent directory: {parent_dir}")
    except OSError:
        pass
    
    try:
        grandparent_dir = repo_root / "uploads" / "range_raw"
        if grandparent_dir.exists() and not any(grandparent_dir.iterdir()):
            grandparent_dir.rmdir()
            logging.info(f"Removed empty grandparent directory: {grandparent_dir}")
    except OSError:
        pass
    
    logging.info(f"[OK] Archived range raw files to Uploaded/ranges/{company_dir}/{date_range}/")
    logging.info(f"  Moved {items_moved} item(s) from range_raw directory")


def archive_files(repo_root: Path, config) -> None:
    """
    Phase 4: Archive processed files after successful upload.
    Reads metadata file and moves files to Uploaded/<date>/ folder.
    """
    metadata_path = repo_root / config.metadata_file
    
    if not metadata_path.exists():
        logging.warning("Metadata file not found. Skipping archive step.")
        return
    
    try:
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
    except Exception as e:
        logging.error(f"Failed to read metadata file: {e}")
        return
    
    normalized_date = metadata.get("normalized_date")
    if not normalized_date:
        logging.warning("No normalized_date in metadata. Skipping archive step.")
        return
    
    # Create Uploaded/<date>/ folder
    archive_dir = repo_root / "Uploaded" / normalized_date
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    # Move raw file
    raw_file_path_str = metadata.get("raw_file_path", "")
    raw_file_path: Optional[Path] = None
    
    if raw_file_path_str:
        raw_file_path = Path(raw_file_path_str)
        if not raw_file_path.is_absolute():
            raw_file_path = repo_root / raw_file_path
    else:
        # Fallback to basename if full path not available
        raw_file_basename = metadata.get("raw_file", "")
        if raw_file_basename:
            raw_file_path = repo_root / raw_file_basename
    
    if raw_file_path:
        # Safety check: ensure we're not trying to move the repo root itself
        if raw_file_path == repo_root or raw_file_path.parent == repo_root and not raw_file_path.name:
            logging.warning(f"Invalid raw_file_path in metadata (points to repo root), skipping raw file archive")
        elif raw_file_path.exists() and raw_file_path.is_file():
            dest_raw = archive_dir / raw_file_path.name
            shutil.move(str(raw_file_path), str(dest_raw))
            logging.info(f"Moved raw file: {raw_file_path.name} -> Uploaded/{normalized_date}/")
        else:
            logging.warning(f"Raw file not found or is not a file: {raw_file_path}")
    else:
        logging.warning("No raw_file_path or raw_file in metadata, skipping raw file archive")
    
    # Move processed file(s)
    processed_files = metadata.get("processed_files", [])
    for processed_file in processed_files:
        if not processed_file or not processed_file.strip():
            logging.warning("Empty processed_file entry in metadata, skipping")
            continue
        
        processed_path = repo_root / processed_file
        
        # Safety check: ensure we're not trying to move the repo root or a directory
        if processed_path == repo_root:
            logging.warning(f"Invalid processed_file path (points to repo root): {processed_file}, skipping")
            continue
        
        if processed_path.exists() and processed_path.is_file():
            dest_processed = archive_dir / processed_file
            shutil.move(str(processed_path), str(dest_processed))
            logging.info(f"Moved processed file: {processed_file} -> Uploaded/{normalized_date}/")
        else:
            logging.warning(f"Processed file not found or is not a file: {processed_file}")
    
    # NOTE: Transformed spill archiving has been removed (Step 3).
    # RAW spill files (uploads/spill_raw/) are now archived separately in run_pipeline.py
    # after each day's processing completes successfully.
    
    # Move metadata file to archive as well
    dest_metadata = archive_dir / config.metadata_file
    shutil.move(str(metadata_path), str(dest_metadata))
    logging.info(f"Moved metadata: {config.metadata_file} -> Uploaded/{normalized_date}/")
    
    logging.info(f"[OK] Phase 4: Archive completed. Files archived to Uploaded/{normalized_date}/")


def merge_raw_csvs(base_csv: Path, extra_csvs: List[Path], out_csv: Path) -> dict:
    """
    Merge multiple raw CSV files into one combined file.
    
    Args:
        base_csv: Primary raw CSV file
        extra_csvs: Additional CSV files to merge (e.g., raw spill files)
        out_csv: Output path for combined CSV
    
    Returns:
        Dict with merge statistics: {base_rows, extra_rows, total_rows}
    """
    df_base = pd.read_csv(base_csv)
    base_rows = len(df_base)
    frames = [df_base]
    extra_rows = 0
    for p in extra_csvs:
        df_extra = pd.read_csv(p)
        frames.append(df_extra)
        extra_rows += len(df_extra)
    df_all = pd.concat(frames, ignore_index=True)
    df_all.to_csv(out_csv, index=False)
    return {
        "base_rows": base_rows,
        "extra_rows": extra_rows,
        "total_rows": len(df_all)
    }


def compute_trading_date(dt_wat: datetime, start_hour: int, start_minute: int) -> datetime.date:
    """
    Compute the trading date for a given datetime in WAT.
    
    Trading day logic:
    - If dt_wat.time() < cutoff (start_hour:start_minute), trading_date = (dt_wat.date() - 1 day)
    - Otherwise, trading_date = dt_wat.date()
    
    Example:
        dt = 2025-01-31 04:59:00, cutoff = 05:00 => trading_date = 2025-01-30
        dt = 2025-01-31 05:00:00, cutoff = 05:00 => trading_date = 2025-01-31
    
    Args:
        dt_wat: Datetime in WAT timezone
        start_hour: Trading day start hour (default: 5)
        start_minute: Trading day start minute (default: 0)
    
    Returns:
        Trading date as a date object
    """
    from datetime import time
    
    cutoff = time(start_hour, start_minute)
    dt_time = dt_wat.time()
    
    if dt_time < cutoff:
        # Before cutoff: belongs to previous trading day
        trading_date = (dt_wat.date() - timedelta(days=1))
    else:
        # At or after cutoff: belongs to current trading day
        trading_date = dt_wat.date()
    
    return trading_date


def split_csv_by_date(csv_path: Path, from_date: str, to_date: str, company_dir: str, repo_root: Path, config=None) -> tuple:
    """
    Split a downloaded EPOS CSV into per-day raw files.
    Also writes future out-of-range rows as raw spill files.
    
    Works for both range mode (from_date != to_date) and single-day mode (from_date == to_date).
    
    If trading_day_enabled is True in config:
        - Computes trading_date for each row based on datetime in WAT and trading day cutoff
        - Writes rows to uploads/range_raw/<company_dir>/<from>_to_<to>/BookKeeping_<trading_date>.csv
        - Does NOT create spill files in this mode
    If trading_day_enabled is False (default):
        - Uses calendar date (existing behavior)
        - Creates spill files for future dates
    
    Args:
        csv_path: Path to the downloaded CSV file
        from_date: Start date in YYYY-MM-DD format
        to_date: End date in YYYY-MM-DD format
        company_dir: Human-readable company folder name (Title_Case_With_Underscores)
        repo_root: Repository root path
        config: Optional CompanyConfig object (required if trading_day_enabled is True)
    
    Returns:
        Tuple of three items:
        - date_to_file: Dict mapping in-range date strings (YYYY-MM-DD) to file paths
        - future_spill_to_file: Dict mapping out-of-range future dates to raw spill file paths (empty if trading_day_enabled)
        - split_stats: Dict with row counts for logging/notifications:
            - total_rows: Total rows in original CSV
            - in_range_rows: Rows written to in-range split files
            - future_rows: Rows written to future spill files (0 if trading_day_enabled)
            - past_rows: Rows for dates before from_date (ignored but logged)
            - null_rows: Rows with unparseable dates (ignored)
            - future_spill_details: Dict {date_str: row_count} for Slack notifications (empty if trading_day_enabled)
    """
    from datetime import timezone, timedelta
    
    # WAT timezone (UTC+1)
    WAT_TZ = timezone(timedelta(hours=1))
    
    # Check if trading day mode is enabled
    trading_day_enabled = False
    trading_day_start_hour = 5
    trading_day_start_minute = 0
    if config:
        trading_day_enabled = config.trading_day_enabled
        trading_day_start_hour = config.trading_day_start_hour
        trading_day_start_minute = config.trading_day_start_minute
    
    if trading_day_enabled:
        logging.info(f"Trading day mode enabled: cutoff={trading_day_start_hour:02d}:{trading_day_start_minute:02d} WAT")
    
    # Parse date range
    from_dt = datetime.strptime(from_date, "%Y-%m-%d")
    to_dt = datetime.strptime(to_date, "%Y-%m-%d")
    
    # Generate all dates in range
    date_list = []
    current = from_dt
    while current <= to_dt:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    
    # Load CSV
    df = pd.read_csv(csv_path)
    total_rows = len(df)
    
    # Parse dates from Date/Time column (fallback to Date)
    def parse_date(value):
        """Parse common date/time strings into a naive datetime."""
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return None
        s = str(value).strip()
        if s == "":
            return None
        for fmt in ("%d/%m/%Y %H:%M:%S", "%Y-%m-%d %H:%M:%S", "%d/%m/%Y", "%Y-%m-%d"):
            try:
                return datetime.strptime(s, fmt)
            except Exception:
                pass
        dt = pd.to_datetime(s, errors="coerce")
        if pd.isna(dt):
            return None
        return dt.to_pydatetime()
    
    # Get date column (prefer Date/Time, fallback to Date)
    date_col = "Date/Time" if "Date/Time" in df.columns else "Date"
    if date_col not in df.columns:
        raise ValueError(f"CSV must contain either 'Date/Time' or 'Date' column")
    
    dates_series = df[date_col].apply(parse_date)
    
    # Convert to WAT timezone and compute trading date or calendar date
    # Also track calendar dates for trading day boundary stats
    def get_date_in_wat(dt):
        try:
            if dt is None or pd.isna(dt):
                return None, None
        except (TypeError, ValueError):
            return None, None
        try:
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=WAT_TZ)
            elif dt.tzinfo != WAT_TZ:
                dt = dt.astimezone(WAT_TZ)
            
            calendar_date = dt.date()
            
            if trading_day_enabled:
                # Compute trading date based on cutoff
                trading_date = compute_trading_date(dt, trading_day_start_hour, trading_day_start_minute)
                return trading_date.strftime("%Y-%m-%d"), calendar_date.strftime("%Y-%m-%d")
            else:
                # Use calendar date (existing behavior)
                return calendar_date.strftime("%Y-%m-%d"), None
        except (AttributeError, ValueError, TypeError):
            return None, None
    
    # Apply function and extract both trading dates and calendar dates
    date_results = dates_series.apply(get_date_in_wat)
    date_strings = date_results.apply(lambda x: x[0] if isinstance(x, tuple) else x)
    
    # Extract calendar dates for trading day stats (only when trading_day_enabled)
    calendar_date_strings = None
    if trading_day_enabled:
        calendar_date_strings = date_results.apply(lambda x: x[1] if isinstance(x, tuple) and x[1] else None)
    
    # Log trading day assignments for debugging (sample a few rows)
    if trading_day_enabled:
        sample_indices = min(5, len(df))
        for idx in range(sample_indices):
            dt_val = dates_series.iloc[idx]
            if dt_val is not None and not pd.isna(dt_val):
                try:
                    if dt_val.tzinfo is None:
                        dt_wat = dt_val.replace(tzinfo=WAT_TZ)
                    else:
                        dt_wat = dt_val.astimezone(WAT_TZ)
                    trading_date = compute_trading_date(dt_wat, trading_day_start_hour, trading_day_start_minute)
                    logging.info(f"Trading day sample: row_datetime={dt_wat.strftime('%Y-%m-%d %H:%M:%S')} WAT => trading_date={trading_date}")
                except Exception:
                    pass
    
    # Count null/unparseable dates
    null_count = int(date_strings.isna().sum())
    if null_count > 0:
        logging.warning(f"Found {null_count} row(s) with null/unparseable dates (will be ignored)")
    
    # Count past dates (< from_date) - log but don't save
    all_dates = date_strings.dropna().unique()
    past_dates = [d for d in all_dates if d < from_date]
    past_count = 0
    if past_dates:
        past_mask = date_strings.isin(past_dates)
        past_count = int(past_mask.sum())
        logging.info(f"Found {past_count} row(s) for past dates (< {from_date}): {', '.join(sorted(past_dates))} (ignored)")
    
    # Create directory for split files using human-readable company folder name
    split_dir = repo_root / "uploads" / "range_raw" / company_dir / f"{from_date}_to_{to_date}"
    split_dir.mkdir(parents=True, exist_ok=True)
    
    # Split by date (in-range) - track row counts without re-reading
    date_to_file = {}
    in_range_count = 0
    for target_date_str in date_list:
        target_mask = date_strings == target_date_str
        target_df = df[target_mask].copy().reset_index(drop=True)
        
        if len(target_df) > 0:
            # Create filename: BookKeeping_YYYY-MM-DD.csv
            split_filename = f"BookKeeping_{target_date_str}.csv"
            split_path = split_dir / split_filename
            target_df.to_csv(split_path, index=False)
            date_to_file[target_date_str] = str(split_path)
            in_range_count += len(target_df)
            logging.info(f"Created split file for {target_date_str}: {split_filename} ({len(target_df)} rows)")
        else:
            logging.warning(f"No rows found for {target_date_str}, skipping split file")
    
    # Identify and write future out-of-range rows as raw spill files
    # Future = date > to_date
    # NOTE: In trading day mode, we do NOT create spill files (all rows are assigned to trading dates)
    future_spill_to_file = {}
    future_spill_details = {}  # {date: row_count} for notifications
    future_count = 0
    
    if not trading_day_enabled:
        # Only create spill files in calendar day mode
        # Get all unique dates that are future (> to_date)
        future_dates = [d for d in all_dates if d > to_date]
        
        if future_dates:
            # Create spill_raw directory
            spill_raw_dir = repo_root / "uploads" / "spill_raw" / company_dir
            spill_raw_dir.mkdir(parents=True, exist_ok=True)
            
            logging.info(f"\nFound {len(future_dates)} future date(s) outside range (> {to_date})")
            
            for future_date_str in sorted(future_dates):
                future_mask = date_strings == future_date_str
                future_df = df[future_mask].copy().reset_index(drop=True)
                
                if len(future_df) > 0:
                    spill_filename = f"BookKeeping_raw_spill_{future_date_str}.csv"
                    spill_path = spill_raw_dir / spill_filename
                    future_df.to_csv(spill_path, index=False)
                    future_spill_to_file[future_date_str] = str(spill_path)
                    future_spill_details[future_date_str] = len(future_df)
                    future_count += len(future_df)
                    logging.info(f"Created raw spill file for {future_date_str}: {spill_filename} ({len(future_df)} rows)")
    else:
        logging.info("Trading day mode: skipping spill file creation (all rows assigned to trading dates)")
    
    # Log summary (no re-reading CSVs needed)
    logging.info(f"\nSplit summary: {in_range_count} rows in-range, {future_count} rows future spill, {past_count} rows past (ignored), {null_count} rows null (ignored)")
    
    # Build stats dict for caller
    split_stats = {
        "total_rows": total_rows,
        "in_range_rows": in_range_count,
        "future_rows": future_count,
        "past_rows": past_count,
        "null_rows": null_count,
        "future_spill_details": future_spill_details,
    }
    
    # Compute trading day boundary stats if trading day mode is enabled
    if trading_day_enabled and calendar_date_strings is not None:
        cutoff_str = f"{trading_day_start_hour:02d}:{trading_day_start_minute:02d}"
        trading_day_stats = {
            "cutoff": cutoff_str,
            "by_date": {}
        }
        
        # For each trading date in the range, compute boundary stats
        for target_date_str in date_list:
            if target_date_str not in date_to_file:
                continue  # Skip dates with no rows
            
            # Get rows for this trading date
            target_mask = date_strings == target_date_str
            target_calendar_dates = calendar_date_strings[target_mask]
            
            # Count rows where calendar_date == trading_date + 1 day (pre-cutoff reassigned)
            target_trading_date = datetime.strptime(target_date_str, "%Y-%m-%d").date()
            next_calendar_date = (target_trading_date + timedelta(days=1)).strftime("%Y-%m-%d")
            
            pre_cutoff_reassigned = 0
            same_calendar_day = 0
            total_for_trading_day = int(target_mask.sum())
            
            # Count reassigned and same-day rows
            for cal_date_str in target_calendar_dates:
                if cal_date_str is None or pd.isna(cal_date_str):
                    continue
                if cal_date_str == next_calendar_date:
                    # Calendar date is D+1, but assigned to trading date D (pre-cutoff)
                    pre_cutoff_reassigned += 1
                elif cal_date_str == target_date_str:
                    # Calendar date equals trading date
                    same_calendar_day += 1
            
            trading_day_stats["by_date"][target_date_str] = {
                "total": total_for_trading_day,
                "pre_cutoff_reassigned": pre_cutoff_reassigned,
                "same_calendar_day": same_calendar_day
            }
            
            # Log trading day adjustment for this date
            if pre_cutoff_reassigned > 0:
                logging.info(f"Trading-day adjustment for {target_date_str}: pre-cutoff reassigned={pre_cutoff_reassigned} (cutoff={cutoff_str} WAT)")
        
        split_stats["trading_day_stats"] = trading_day_stats
    else:
        # No trading day stats for calendar day mode
        split_stats["trading_day_stats"] = None
    
    return date_to_file, future_spill_to_file, split_stats


def reconcile_company(company_key: str, target_date: str, config, repo_root: Path) -> dict:
    """
    Reconcile EPOS totals vs QBO totals for a specific company and date.
    Returns a reconcile dict for inclusion in summary.
    
    Args:
        company_key: Company identifier
        target_date: Target date in YYYY-MM-DD format
        config: CompanyConfig object
        repo_root: Repository root path
    
    Returns:
        Dict with reconcile status, totals, and counts
    """
    reconcile_result = {
        "status": "NOT RUN",
        "reason": "unknown"
    }
    
    try:
        # Determine the processed CSV prefix for this company.
        # CompanyConfig implementations differ across iterations, so we support multiple attribute shapes.
        csv_prefix = None
        try:
            output_cfg = getattr(config, "output_config", None)
            if isinstance(output_cfg, dict):
                csv_prefix = output_cfg.get("csv_prefix")
        except Exception:
            csv_prefix = None

        if not csv_prefix:
            csv_prefix = getattr(config, "csv_prefix", None)

        if not csv_prefix:
            # Fallbacks (historical defaults)
            csv_prefix = "single_sales_receipts"

        # Get EPOS total from processed CSV
        # Check repo root first (before archiving), then Uploaded folder (after archiving)
        csv_pattern = f"{csv_prefix}_*.csv"
        csv_files = []

        # Check repo root first (where CSV is before archiving)
        csv_files = list(repo_root.glob(csv_pattern))

        # Fallback to Uploaded folder if not found in repo root
        if not csv_files:
            uploaded_dir = repo_root / "Uploaded" / target_date
            if uploaded_dir.exists():
                csv_files = list(uploaded_dir.glob(csv_pattern))

        if not csv_files:
            reconcile_result["reason"] = "processed CSV file not found"
            return reconcile_result

        # Use most recent CSV
        latest_csv = max(csv_files, key=lambda p: p.stat().st_mtime)
        df = pd.read_csv(latest_csv)

                # Filter by target_date if *SalesReceiptDate column exists
        if "*SalesReceiptDate" in df.columns:
            # Parse *SalesReceiptDate using the company-configured format when possible.
            # Company A often uses YYYY-MM-DD; Company B may use DD/MM/YYYY.
            date_format = getattr(config, "date_format", None)

            # Attempt strict parse first if a format is provided
            if isinstance(date_format, str) and date_format.strip():
                try:
                    df["_sr_date"] = pd.to_datetime(
                        df["*SalesReceiptDate"],
                        format=date_format,
                        errors="coerce",
                    )
                except Exception:
                    df["_sr_date"] = pd.to_datetime(df["*SalesReceiptDate"], errors="coerce")
            else:
                # Heuristic: if values contain '/', prefer dayfirst=True
                sample = df["*SalesReceiptDate"].dropna().astype(str).head(20)
                dayfirst = any("/" in s for s in sample)
                df["_sr_date"] = pd.to_datetime(
                    df["*SalesReceiptDate"],
                    errors="coerce",
                    dayfirst=dayfirst,
                )

            target_dt = pd.to_datetime(target_date, errors="coerce")
            if pd.isna(target_dt):
                reconcile_result["reason"] = f"invalid target_date: {target_date}"
                return reconcile_result

            df = df[df["_sr_date"].dt.date == target_dt.date()].copy()
            df = df.drop(columns=["_sr_date"], errors="ignore")

        # Calculate EPOS totals
        if "*ItemAmount" in df.columns:
            epos_total = float(df["*ItemAmount"].sum())
        else:
            epos_total = 0.0

        epos_count = df["*SalesReceiptNo"].nunique() if "*SalesReceiptNo" in df.columns else 0

        # Get QBO total using query_qbo_for_company.py
        query_script = repo_root / "query_qbo_for_company.py"
        if not query_script.exists():
            reconcile_result["reason"] = "query_qbo_for_company.py not found"
            return reconcile_result

        # Query QBO for receipts on target_date (get Id and TotalAmt)
        cmd = [
            sys.executable,
            str(query_script),
            "--company", company_key,
            "query",
            f"SELECT Id, TotalAmt FROM SalesReceipt WHERE TxnDate = '{target_date}'"
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(repo_root))

        if result.returncode != 0:
            reconcile_result["reason"] = f"QBO query failed: {result.stderr[:100]}"
            return reconcile_result

        try:
            qbo_data = json.loads(result.stdout)
            receipts = qbo_data.get("QueryResponse", {}).get("SalesReceipt", [])
            if not isinstance(receipts, list):
                receipts = [receipts] if receipts else []

            qbo_total = sum(float(r.get("TotalAmt", 0) or 0) for r in receipts)
            qbo_count = len(receipts)

            # Compare
            difference = abs(qbo_total - epos_total)
            tolerance = 1.0  # Allow ₦1.00 difference for rounding

            if difference <= tolerance:
                status = "MATCH"
            else:
                status = "MISMATCH"

            reconcile_result = {
                "status": status,
                "epos_total": epos_total,
                "epos_count": epos_count,
                "qbo_total": qbo_total,
                "qbo_count": qbo_count,
                "difference": difference
            }

        except (json.JSONDecodeError, KeyError, ValueError) as e:
            reconcile_result["reason"] = f"Failed to parse QBO response: {str(e)[:100]}"
            return reconcile_result

    except Exception as e:
        logging.warning(f"Reconciliation failed: {e}")
        reconcile_result["reason"] = str(e)[:100]
        return reconcile_result

    return reconcile_result


def main(company_key: str, target_date: Optional[str] = None, from_date: Optional[str] = None, to_date: Optional[str] = None, skip_download: bool = False) -> int:
    """
    Full pipeline for a specific company:

    1) epos_playwright.py
       - Logs into EPOS and downloads the latest bookkeeping CSV
         into the repo root directory.

    2) transform.py
       - Reads the latest raw EPOS file from repo root
         and produces a single consolidated QuickBooks-ready CSV
         in repo root (company-specific prefix).
       - Writes metadata to company-specific metadata file

    3) qbo_upload.py
       - Reads the latest CSV from repo root
         and creates Sales Receipts in the QBO company via API.

    4) Archive (run_pipeline.py)
       - After successful upload, reads metadata file
       - Creates Uploaded/<date>/ folder
       - Moves raw CSV, processed CSV(s), and metadata to archive folder
    
    Args:
        company_key: Company identifier ('company_a' or 'company_b') - REQUIRED
        target_date: Target business date in YYYY-MM-DD format. If None, uses yesterday.
        from_date: Start date for range mode in YYYY-MM-DD format (must be used with to_date)
        to_date: End date for range mode in YYYY-MM-DD format (must be used with from_date)
        skip_download: If True, skip EPOS download and use existing split files in uploads/range_raw/ (range mode only)
    """
    # Load company configuration
    try:
        config = load_company_config(company_key)
    except Exception as e:
        logging.error(f"Failed to load company config for '{company_key}': {e}")
        available = get_available_companies()
        if available:
            logging.error(f"Available companies: {', '.join(available)}")
        else:
            logging.error("No company configs found. Please create config files in companies/ directory.")
        raise SystemExit(1)
    
    # Safety check: verify realm_id matches tokens
    try:
        verify_realm_match(company_key, config.realm_id)
    except RuntimeError as e:
        logging.error(f"Realm ID safety check failed: {e}")
        raise SystemExit(1)
    
    # Log company info for safety
    logging.info("=" * 60)
    logging.info(f"COMPANY: {config.display_name} ({company_key})")
    logging.info(f"REALM ID: {config.realm_id}")
    logging.info(f"DEPOSIT ACCOUNT: {config.deposit_account}")
    logging.info(f"TAX MODE: {config.tax_mode}")
    logging.info("=" * 60)
    
    # Determine mode: range mode if both from_date and to_date are provided
    is_range_mode = from_date is not None and to_date is not None
    
    if is_range_mode:
        logging.info(f"Range mode: {from_date} to {to_date}")
        date_range_str = f"{from_date} to {to_date}"
    else:
        # Single-day mode: determine target_date
        if not target_date:
            target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
            logging.info(f"No target_date provided, using yesterday: {target_date}")
        else:
            logging.info(f"Using provided target_date: {target_date}")
        date_range_str = target_date
    
    pipeline_name = f"{config.display_name} -> QuickBooks Pipeline"

    logging.info(f"Starting {pipeline_name}...\n")
    
    # Send ONE start notification (for range or single day)
    start_metadata = {
        "target_date": target_date if not is_range_mode else None,
        "from_date": from_date if is_range_mode else None,
        "to_date": to_date if is_range_mode else None,
        "company_key": company_key,
        "company_name": config.display_name
    }
    notify_pipeline_start(pipeline_name, log_file, date_range_str, config.slack_webhook_url, start_metadata)

    # Track warnings for watchdog notification (sent ONCE if any warnings occur)
    warnings = []
    watchdog_sent = False

    try:
        if is_range_mode:
            # RANGE MODE: Atomic processing - one download, split, then process per day
            # 
            # Range mode semantics:
            # - One EPOS download for the entire range
            # - One Slack start notification for the range
            # - Per-day processing: transform, upload, reconcile, archive
            # - Per-day completion notifications (NO per-day start notifications)
            # - One final Slack completion notification for the entire range
            # - If any per-day step fails with SystemExit, the entire range stops
            # - Original range CSV and split files are archived only after ALL days complete successfully
            #
            # Spill handling:
            # - Spill files created during day N are written to uploads/spill/
            # - Spill files are automatically merged when processing day N+1 (if date matches)
            # - Spill files are archived only once they are used (via archive_files())
            logging.info("\n=== RANGE MODE: Processing date range ===")
            logging.info("Range mode is atomic: all days must complete successfully for final archival")
            
            # Compute human-readable company folder name for filesystem paths
            company_dir = company_dir_name(config.display_name)
            logging.info(f"Using company folder: {company_dir}")
            
            if skip_download:
                # SKIP DOWNLOAD MODE: Use existing split files from uploads/range_raw/
                logging.info("\n=== SKIP-DOWNLOAD MODE: Using existing split files ===")
                
                # Check for existing split files in range_raw directory
                # First try exact range folder
                split_dir = repo_root / "uploads" / "range_raw" / company_dir / f"{from_date}_to_{to_date}"
                
                # Also check legacy path (company_key instead of company_dir)
                if not split_dir.exists():
                    legacy_split_dir = repo_root / "uploads" / "range_raw" / company_key / f"{from_date}_to_{to_date}"
                    if legacy_split_dir.exists():
                        split_dir = legacy_split_dir
                        logging.info(f"Using legacy path: {split_dir}")
                
                # If exact range folder doesn't exist, search for any range folder that might contain our dates
                if not split_dir.exists():
                    logging.info(f"Exact range folder not found, searching for files in any range folder...")
                    company_range_dir = repo_root / "uploads" / "range_raw" / company_dir
                    if not company_range_dir.exists():
                        company_range_dir = repo_root / "uploads" / "range_raw" / company_key
                    
                    if company_range_dir.exists():
                        # Search all range folders for this company
                        found_dir = None
                        for range_folder in company_range_dir.iterdir():
                            if range_folder.is_dir():
                                # Check if this folder contains any of our requested date files
                                test_file = range_folder / f"BookKeeping_{from_date}.csv"
                                if test_file.exists():
                                    found_dir = range_folder
                                    logging.info(f"Found files in range folder: {range_folder.name}")
                                    break
                        
                        if found_dir:
                            split_dir = found_dir
                        else:
                            error_msg = f"[ERROR] Skip-download mode: no split files found for date range {from_date} to {to_date}"
                            logging.error(error_msg)
                            logging.error(f"Searched in: {company_range_dir}")
                            raise SystemExit(error_msg)
                    else:
                        error_msg = f"[ERROR] Skip-download mode: range_raw directory not found for company: {company_range_dir}"
                        logging.error(error_msg)
                        raise SystemExit(error_msg)
                
                # Build date_to_file from existing split files
                date_to_file = {}
                future_spill_to_file = {}
                split_stats = {
                    "total_rows": 0,
                    "in_range_rows": 0,
                    "future_rows": 0,
                    "past_rows": 0,
                    "null_rows": 0,
                    "future_spill_details": {}
                }
                
                # Generate date list for checking
                from_dt = datetime.strptime(from_date, "%Y-%m-%d")
                to_dt = datetime.strptime(to_date, "%Y-%m-%d")
                date_list = []
                current = from_dt
                while current <= to_dt:
                    date_list.append(current.strftime("%Y-%m-%d"))
                    current += timedelta(days=1)
                
                # Look for BookKeeping_<date>.csv files
                for day_date in date_list:
                    split_file = split_dir / f"BookKeeping_{day_date}.csv"
                    if split_file.exists():
                        date_to_file[day_date] = str(split_file)
                        # Count rows for stats
                        try:
                            df = pd.read_csv(split_file)
                            split_stats["in_range_rows"] += len(df)
                            split_stats["total_rows"] += len(df)
                            logging.info(f"Found existing split file for {day_date}: {split_file.name} ({len(df)} rows)")
                        except Exception as e:
                            logging.warning(f"Could not read {split_file.name}: {e}")
                    else:
                        logging.warning(f"No split file found for {day_date}: {split_file.name}")
                
                # Also check for CombinedRaw files (merged with spill)
                for day_date in date_list:
                    combined_file = split_dir / f"CombinedRaw_{day_date}.csv"
                    if combined_file.exists():
                        # Prefer CombinedRaw over BookKeeping if both exist
                        date_to_file[day_date] = str(combined_file)
                        try:
                            df = pd.read_csv(combined_file)
                            logging.info(f"Found existing combined file for {day_date}: {combined_file.name} ({len(df)} rows)")
                        except Exception as e:
                            logging.warning(f"Could not read {combined_file.name}: {e}")
                
                if not date_to_file:
                    error_msg = f"[ERROR] Skip-download mode: no split files found in {split_dir}"
                    logging.error(error_msg)
                    raise SystemExit(error_msg)
                
                logging.info(f"Found {len(date_to_file)} existing split file(s) for date range")
                downloaded_csv = None  # No downloaded CSV in skip-download mode
                
                # Create minimal trading_day_stats with cutoff info (if trading day mode is enabled)
                # We can't compute per-date counts without the original raw CSV, but we can show the cutoff
                trading_day_stats = None
                if config and config.trading_day_enabled:
                    cutoff_str = f"{config.trading_day_start_hour:02d}:{config.trading_day_start_minute:02d}"
                    trading_day_stats = {
                        "cutoff": cutoff_str,
                        "by_date": {}  # Empty - no per-date stats available in skip-download mode
                    }
                    logging.info(f"Trading-day mode enabled (cutoff={cutoff_str} WAT) - stats unavailable in skip-download mode")
                
            else:
                # NORMAL MODE: Download and split
                # Deterministic range-mode raw CSV detection: snapshot repo-root CSVs before download
                # This ensures we use exactly the file created by the download, not a stale CSV
                before_csvs = {p.resolve() for p in repo_root.glob("*.csv")}
                
                # Phase 1: Download EPOS CSV once for the entire range
                run_step(
                    "Phase 1: Download EPOS CSV (epos_playwright) - Range",
                    "epos_playwright.py",
                    ["--company", company_key, "--from-date", from_date, "--to-date", to_date]
                )
                
                # Snapshot repo-root CSVs after download and detect new files via set difference
                after_csvs = {p.resolve() for p in repo_root.glob("*.csv")}
                new_csvs = sorted(after_csvs - before_csvs, key=lambda p: p.stat().st_mtime)
                
                # Defensive: exclude processed CSVs if any appeared (shouldn't happen, but safety check)
                new_csvs = [
                    p for p in new_csvs
                    if not (p.name.startswith("single_sales_receipts_") or p.name.startswith("gp_sales_receipts_"))
                ]
                
                if not new_csvs:
                    error_msg = "[ERROR] Range mode: no new raw EPOS CSV appeared in repo root after download"
                    logging.error(error_msg)
                    raise SystemExit(error_msg)
                
                if len(new_csvs) > 1:
                    logging.warning(
                        "Range mode: multiple new CSVs detected after EPOS download; using newest. Candidates: %s",
                        ", ".join(p.name for p in new_csvs),
                    )
                
                downloaded_csv = new_csvs[-1]
                logging.info(f"Using raw EPOS file for splitting: {downloaded_csv.name}")
                
                # Split CSV into per-day files
                logging.info("\n=== Splitting CSV into per-day files ===")
                date_to_file, future_spill_to_file, split_stats = split_csv_by_date(downloaded_csv, from_date, to_date, company_dir, repo_root, config)
                
                if not date_to_file:
                    raise SystemExit("No data found in date range after splitting")
                
                # Store trading day stats for later use in summaries
                trading_day_stats = split_stats.get("trading_day_stats")
            
            # Add warnings for future raw spills (for Slack notification)
            if future_spill_to_file:
                logging.info(f"Future raw spill files created for: {', '.join(sorted(future_spill_to_file.keys()))}")
                for spill_date, row_count in split_stats.get("future_spill_details", {}).items():
                    warnings.append(f"Future raw spill: {spill_date} ({row_count} rows)")
                
                # Send watchdog notification for future spills (high-signal)
                if not watchdog_sent:
                    logging.info("\n=== Watchdog: Sending update notification (post-split) ===")
                    watchdog_summary = {
                        "from_date": from_date,
                        "to_date": to_date,
                        "company_key": company_key,
                        "company_name": config.display_name,
                        "phase": "Split",
                        "phase_status": "Completed",
                        "warnings": warnings.copy(),
                    }
                    if trading_day_stats:
                        watchdog_summary["trading_day_stats"] = trading_day_stats
                    notify_pipeline_update(pipeline_name, log_file, watchdog_summary, config.slack_webhook_url)
                    watchdog_sent = True
            
            # Set up spill_raw directory for merging existing raw spills
            spill_raw_dir = repo_root / "uploads" / "spill_raw" / company_dir
            
            # Generate date list for iteration
            from_dt = datetime.strptime(from_date, "%Y-%m-%d")
            to_dt = datetime.strptime(to_date, "%Y-%m-%d")
            date_list = []
            current = from_dt
            while current <= to_dt:
                date_list.append(current.strftime("%Y-%m-%d"))
                current += timedelta(days=1)
            
            # Track per-day results for final summary
            per_day_results = []
            
            # Process each day in the range (atomic: any SystemExit stops the range)
            for day_date in date_list:
                if day_date not in date_to_file:
                    logging.warning(f"No data for {day_date}, skipping")
                    continue
                
                logging.info(f"\n{'='*60}")
                logging.info(f"Processing day: {day_date}")
                logging.info(f"{'='*60}")
                
                day_raw_file = date_to_file[day_date]
                used_raw_spill_for_day = []  # Track raw spills used for this day
                
                # Check for existing raw spill file for this day and merge if present
                raw_spill_path = spill_raw_dir / f"BookKeeping_raw_spill_{day_date}.csv"
                raw_file_to_use = day_raw_file
                
                if raw_spill_path.exists():
                    logging.info(f"Found raw spill file for {day_date}: {raw_spill_path.name}")
                    
                    # Merge day_raw_file + raw_spill into a combined file
                    split_dir = Path(day_raw_file).parent
                    combined_path = split_dir / f"CombinedRaw_{day_date}.csv"
                    
                    merge_stats = merge_raw_csvs(
                        Path(day_raw_file),
                        [raw_spill_path],
                        combined_path
                    )
                    logging.info(f"Merged split ({merge_stats['base_rows']} rows) + raw spill ({merge_stats['extra_rows']} rows) -> final ({merge_stats['total_rows']} rows): {combined_path.name}")
                    
                    raw_file_to_use = str(combined_path)
                    used_raw_spill_for_day.append(raw_spill_path)
                    warnings.append(f"{day_date}: merged target split ({merge_stats['base_rows']} rows) + raw spill ({merge_stats['extra_rows']} rows) -> final ({merge_stats['total_rows']} rows)")
                
                # Phase 2: Transform using raw file (combined or original)
                run_step(
                    f"Phase 2: Transform to single CSV (transform) - {day_date}",
                    "transform.py",
                    ["--company", company_key, "--target-date", day_date, "--raw-file", raw_file_to_use]
                )
                
                # Check for spill files after Phase 2
                metadata_path = repo_root / config.metadata_file
                if metadata_path.exists():
                    try:
                        with open(metadata_path, "r") as f:
                            phase2_metadata = json.load(f)
                        rows_non_target = phase2_metadata.get("rows_non_target", 0)
                        if rows_non_target > 0:
                            warnings.append(f"{day_date}: {rows_non_target} non-target row(s) ignored by transform")
                    except Exception:
                        pass
                
                # Phase 3: Upload to QBO
                qbo_upload_args = ["--company", company_key]
                if config.trading_day_enabled:
                    qbo_upload_args.extend(["--target-date", day_date])
                run_step(
                    f"Phase 3: Upload to QBO (qbo_upload) - {day_date}",
                    "qbo_upload.py",
                    qbo_upload_args
                )
                
                # Check upload stats after Phase 3
                if metadata_path.exists():
                    try:
                        with open(metadata_path, "r") as f:
                            phase3_metadata = json.load(f)
                        upload_stats = phase3_metadata.get("upload_stats")
                        if upload_stats:
                            skipped = upload_stats.get("skipped", 0)
                            failed = upload_stats.get("failed", 0)
                            if skipped > 0:
                                warnings.append(f"{day_date}: {skipped} duplicate receipt(s) skipped")
                            if failed > 0:
                                warnings.append(f"{day_date}: {failed} upload(s) failed")
                    except Exception:
                        pass
                
                # Phase 4: Reconcile EPOS vs QBO totals
                # Reconciliation mismatches are warnings, not fatal (range continues)
                logging.info(f"\n=== Phase 4: Reconciliation - {day_date} ===")
                reconcile_result = None
                try:
                    reconcile_result = reconcile_company(company_key, day_date, config, repo_root)
                    if reconcile_result.get("status") == "MATCH":
                        logging.info(f"[OK] Reconciliation: MATCH (EPOS: ₦{reconcile_result.get('epos_total', 0):,.2f}, QBO: ₦{reconcile_result.get('qbo_total', 0):,.2f})")
                    elif reconcile_result.get("status") == "MISMATCH":
                        diff = reconcile_result.get("difference", 0)
                        logging.warning(f"[WARN] Reconciliation: MISMATCH (Difference: ₦{diff:,.2f})")
                        warnings.append(f"{day_date}: Reconciliation mismatch (₦{diff:,.2f})")
                    else:
                        reason = reconcile_result.get("reason", "unknown")
                        logging.warning(f"[WARN] Reconciliation: NOT RUN ({reason})")
                        warnings.append(f"{day_date}: Reconciliation not run ({reason})")
                except Exception as e:
                    logging.error(f"[ERROR] Phase 4: Reconciliation failed: {e}")
                    logging.warning("Continuing despite reconciliation failure (upload was successful)")
                    reconcile_result = {"status": "NOT RUN", "reason": str(e)[:100]}
                    warnings.append(f"{day_date}: Reconciliation failed ({str(e)[:50]})")
                
                # Phase 5: Archive files
                logging.info(f"\n=== Phase 5: Archive Files - {day_date} ===")
                try:
                    archive_files(repo_root, config)
                    
                    # Archive used raw spill files after successful archive
                    if used_raw_spill_for_day:
                        # Get the archive directory from the normalized_date
                        archive_dir = repo_root / "Uploaded" / day_date
                        archive_dir.mkdir(parents=True, exist_ok=True)
                        
                        for raw_spill_path in used_raw_spill_for_day:
                            if raw_spill_path.exists() and raw_spill_path.is_file():
                                dest_name = f"RAW_SPILL_{raw_spill_path.name}"
                                dest_path = archive_dir / dest_name
                                shutil.move(str(raw_spill_path), str(dest_path))
                                logging.info(f"Archived used raw spill: {raw_spill_path.name} -> Uploaded/{day_date}/{dest_name}")
                            else:
                                logging.warning(f"Raw spill file not found for archiving: {raw_spill_path}")
                except Exception as e:
                    logging.error(f"[ERROR] Phase 5: Archive failed: {e}")
                    logging.warning("Continuing despite archive failure (upload was successful)")
                
                # Send per-day completion notification (but NOT start notification)
                metadata = None
                if metadata_path.exists():
                    try:
                        with open(metadata_path, "r") as f:
                            metadata = json.load(f)
                    except Exception:
                        pass
                
                summary = {
                    "target_date": day_date,
                    "company_key": company_key,
                    "company_name": config.display_name,
                    "is_range_day": True,
                    "range": f"{from_date} to {to_date}"
                }
                if metadata:
                    summary.update(metadata)
                if reconcile_result:
                    summary["reconcile"] = reconcile_result
                
                # Add trading day stats for this day if available
                if trading_day_stats and day_date in trading_day_stats.get("by_date", {}):
                    summary["trading_day_stats"] = {
                        "cutoff": trading_day_stats["cutoff"],
                        "by_date": {day_date: trading_day_stats["by_date"][day_date]}
                    }
                
                # Store per-day result for final summary
                per_day_results.append({
                    "date": day_date,
                    "reconcile": reconcile_result,
                    "warnings": [w for w in warnings if w.startswith(f"{day_date}:")]
                })
                
                notify_pipeline_success(
                    f"{pipeline_name} - {day_date}",
                    log_file,
                    day_date,
                    summary,
                    config.slack_webhook_url
                )
                logging.info(f"Completed processing for {day_date} ✅")
            
            # All per-day loops completed successfully - archive range raw files
            logging.info("\n" + "="*60)
            logging.info("All days processed successfully - archiving range raw files")
            logging.info("="*60)
            
            try:
                archive_range_raw_files(repo_root, company_dir, company_key, from_date, to_date, downloaded_csv)
            except Exception as e:
                logging.error(f"[ERROR] Failed to archive range raw files: {e}")
                logging.warning("Continuing despite archive failure (all days processed successfully)")
                warnings.append(f"Range archive failed: {str(e)[:50]}")
            
            # Final success notification for the entire range
            logging.info("\n" + "="*60)
            logging.info("Range mode completed successfully ✅")
            logging.info("="*60)
            
            # Build final summary with per-day reconciliation results
            final_summary = {
                "from_date": from_date,
                "to_date": to_date,
                "company_key": company_key,
                "company_name": config.display_name,
                "days_processed": len(date_to_file),
                "per_day_results": per_day_results,
                "warnings": warnings
            }
            if trading_day_stats:
                final_summary["trading_day_stats"] = trading_day_stats
            
            # Compute Range Totals by summing per-day reconciliation results
            included_days = 0
            epos_total_sum = 0.0
            qbo_total_sum = 0.0
            epos_count_sum = 0
            qbo_count_sum = 0
            
            for day_result in per_day_results:
                reconcile = day_result.get("reconcile")
                if reconcile and reconcile.get("status") in ("MATCH", "MISMATCH"):
                    included_days += 1
                    epos_total_sum += reconcile.get("epos_total", 0) or 0
                    qbo_total_sum += reconcile.get("qbo_total", 0) or 0
                    epos_count_sum += reconcile.get("epos_count", 0) or 0
                    qbo_count_sum += reconcile.get("qbo_count", 0) or 0
            
            # Only add range_totals if we have at least one day with reconciliation
            if included_days > 0:
                difference_sum = round(epos_total_sum - qbo_total_sum, 2)
                final_summary["range_totals"] = {
                    "included_days": included_days,
                    "total_days": len(date_to_file),
                    "epos_total": round(epos_total_sum, 2),
                    "qbo_total": round(qbo_total_sum, 2),
                    "epos_count": epos_count_sum,
                    "qbo_count": qbo_count_sum,
                    "difference": difference_sum,
                }
            
            notify_pipeline_success(pipeline_name, log_file, date_range_str, final_summary, config.slack_webhook_url)
            return 0
        
        else:
            # SINGLE-DAY MODE: Now uses same deterministic raw file selection as range mode
            # This ensures consistency and prevents data loss from future rows
            logging.info("\n=== SINGLE-DAY MODE: Processing target date ===")
            
            # Compute human-readable company folder name (same as range mode)
            company_dir = company_dir_name(config.display_name)
            logging.info(f"Using company folder: {company_dir}")
            
            # Deterministic CSV detection: snapshot repo-root CSVs before download
            before_csvs = {p.resolve() for p in repo_root.glob("*.csv")}
            
            # Phase 1: Download from EPOS with target_date and company config
            run_step(
                "Phase 1: Download EPOS CSV (epos_playwright)",
                "epos_playwright.py",
                ["--company", company_key, "--target-date", target_date]
            )
            
            # Snapshot repo-root CSVs after download and detect new files via set difference
            after_csvs = {p.resolve() for p in repo_root.glob("*.csv")}
            new_csvs = sorted(after_csvs - before_csvs, key=lambda p: p.stat().st_mtime)
            
            # Defensive: exclude processed CSVs if any appeared
            new_csvs = [
                p for p in new_csvs
                if not (p.name.startswith("single_sales_receipts_") or p.name.startswith("gp_sales_receipts_"))
            ]
            
            if not new_csvs:
                error_msg = "[ERROR] Single-day mode: no new raw EPOS CSV appeared in repo root after download"
                logging.error(error_msg)
                raise SystemExit(error_msg)
            
            if len(new_csvs) > 1:
                logging.warning(
                    "Single-day mode: multiple new CSVs detected after EPOS download; using newest. Candidates: %s",
                    ", ".join(p.name for p in new_csvs),
                )
            
            downloaded_csv = new_csvs[-1]
            logging.info(f"Using raw EPOS file for splitting: {downloaded_csv.name}")
            
            # Split CSV into per-day files (same mechanism as range mode)
            # For single-day, from_date == to_date
            logging.info("\n=== Splitting CSV by date ===")
            date_to_file, future_spill_to_file, split_stats = split_csv_by_date(
                downloaded_csv, target_date, target_date, company_dir, repo_root, config
            )
            
            # Store trading day stats for later use in summaries
            trading_day_stats = split_stats.get("trading_day_stats")
            
            # Check if we have data for the target date
            if target_date not in date_to_file:
                error_msg = f"[ERROR] No rows found for target_date {target_date} after splitting; abort."
                logging.error(error_msg)
                raise SystemExit(error_msg)
            
            day_raw_file = date_to_file[target_date]
            used_raw_spill_for_day = []  # Track raw spills used for this day
            
            # Add warnings for future raw spills (for Slack notification)
            if future_spill_to_file:
                logging.info(f"Future raw spill files created for: {', '.join(sorted(future_spill_to_file.keys()))}")
                for spill_date, row_count in split_stats.get("future_spill_details", {}).items():
                    warnings.append(f"Future raw spill: {spill_date} ({row_count} rows)")
            
            # Set up spill_raw directory for merging existing raw spills
            spill_raw_dir = repo_root / "uploads" / "spill_raw" / company_dir
            
            # Check for existing raw spill file for target_date and merge if present
            raw_spill_path = spill_raw_dir / f"BookKeeping_raw_spill_{target_date}.csv"
            raw_file_to_use = day_raw_file
            
            if raw_spill_path.exists():
                logging.info(f"Found raw spill file for {target_date}: {raw_spill_path.name}")
                
                # Merge day_raw_file + raw_spill into a combined file
                split_dir = Path(day_raw_file).parent
                combined_path = split_dir / f"CombinedRaw_{target_date}.csv"
                
                merge_stats = merge_raw_csvs(
                    Path(day_raw_file),
                    [raw_spill_path],
                    combined_path
                )
                logging.info(f"Merged split ({merge_stats['base_rows']} rows) + raw spill ({merge_stats['extra_rows']} rows) -> final ({merge_stats['total_rows']} rows): {combined_path.name}")
                
                raw_file_to_use = str(combined_path)
                used_raw_spill_for_day.append(raw_spill_path)
                warnings.append(f"{target_date}: merged target split ({merge_stats['base_rows']} rows) + raw spill ({merge_stats['extra_rows']} rows) -> final ({merge_stats['total_rows']} rows)")
            
            # Send watchdog notification if we have future spills or merged spills (high-signal only)
            if (future_spill_to_file or used_raw_spill_for_day) and not watchdog_sent:
                logging.info("\n=== Watchdog: Sending update notification (post-split) ===")
                watchdog_summary = {
                    "target_date": target_date,
                    "company_key": company_key,
                    "company_name": config.display_name,
                    "phase": "Split/Merge",
                    "phase_status": "Completed",
                    "warnings": warnings.copy(),  # Copy current warnings
                }
                if trading_day_stats:
                    watchdog_summary["trading_day_stats"] = trading_day_stats
                notify_pipeline_update(pipeline_name, log_file, watchdog_summary, config.slack_webhook_url)
                watchdog_sent = True
            
            # Phase 2: Transform using raw file (combined or original)
            run_step(
                "Phase 2: Transform to single CSV (transform)",
                "transform.py",
                ["--company", company_key, "--target-date", target_date, "--raw-file", raw_file_to_use]
            )
            
            # Check for spill files after Phase 2 (transformed spill - still logged but we don't rely on it)
            metadata_path = repo_root / config.metadata_file
            if metadata_path.exists():
                try:
                    with open(metadata_path, "r") as f:
                        phase2_metadata = json.load(f)
                    rows_non_target = phase2_metadata.get("rows_non_target", 0)
                    if rows_non_target > 0:
                        warnings.append(f"{rows_non_target} non-target row(s) ignored by transform")
                except Exception:
                    pass  # Ignore metadata read errors

            # Phase 3: Upload to QBO
            qbo_upload_args = ["--company", company_key]
            if config.trading_day_enabled:
                qbo_upload_args.extend(["--target-date", target_date])
            run_step(
                "Phase 3: Upload to QBO (qbo_upload)",
                "qbo_upload.py",
                qbo_upload_args
            )
            
            # Check upload stats after Phase 3 for partial failures
            if metadata_path.exists():
                try:
                    with open(metadata_path, "r") as f:
                        phase3_metadata = json.load(f)
                    upload_stats = phase3_metadata.get("upload_stats")
                    if upload_stats:
                        skipped = upload_stats.get("skipped", 0)
                        failed = upload_stats.get("failed", 0)
                        if skipped > 0:
                            warnings.append(f"{skipped} duplicate receipt(s) skipped")
                        if failed > 0:
                            warnings.append(f"{failed} upload(s) failed (continuing)")
                except Exception:
                    pass  # Ignore metadata read errors

            # Phase 4: Reconcile EPOS vs QBO totals (BEFORE archiving so CSV is still in repo root)
            logging.info("\n=== Phase 4: Reconciliation ===")
            reconcile_result = None
            try:
                reconcile_result = reconcile_company(company_key, target_date, config, repo_root)
                if reconcile_result.get("status") == "MATCH":
                    logging.info(f"[OK] Reconciliation: MATCH (EPOS: ₦{reconcile_result.get('epos_total', 0):,.2f}, QBO: ₦{reconcile_result.get('qbo_total', 0):,.2f})")
                elif reconcile_result.get("status") == "MISMATCH":
                    diff = reconcile_result.get("difference", 0)
                    logging.warning(f"[WARN] Reconciliation: MISMATCH (Difference: ₦{diff:,.2f})")
                else:
                    reason = reconcile_result.get("reason", "unknown")
                    logging.warning(f"[WARN] Reconciliation: NOT RUN ({reason})")
            except Exception as e:
                logging.error(f"[ERROR] Phase 4: Reconciliation failed: {e}")
                logging.warning("Continuing despite reconciliation failure (upload was successful)")
                reconcile_result = {"status": "NOT RUN", "reason": str(e)[:100]}

            # Phase 5: Archive files after successful upload and reconciliation
            logging.info("\n=== Phase 5: Archive Files ===")
            try:
                archive_files(repo_root, config)
                
                # Archive used raw spill files after successful archive
                if used_raw_spill_for_day:
                    # Get the archive directory for this date
                    archive_dir = repo_root / "Uploaded" / target_date
                    archive_dir.mkdir(parents=True, exist_ok=True)
                    
                    for raw_spill_path in used_raw_spill_for_day:
                        if raw_spill_path.exists() and raw_spill_path.is_file():
                            dest_name = f"RAW_SPILL_{raw_spill_path.name}"
                            dest_path = archive_dir / dest_name
                            shutil.move(str(raw_spill_path), str(dest_path))
                            logging.info(f"Archived used raw spill: {raw_spill_path.name} -> Uploaded/{target_date}/{dest_name}")
                        else:
                            logging.warning(f"Raw spill file not found for archiving: {raw_spill_path}")
            except Exception as e:
                logging.error(f"[ERROR] Phase 5: Archive failed: {e}")
                # Don't fail the pipeline if archiving fails - upload already succeeded
                logging.warning("Continuing despite archive failure (upload was successful)")
            
            # Single-day mode cleanup: remove scratch split directory after successful archive
            # Move any remaining files to Uploaded/<target_date>/ before deleting
            split_dir = repo_root / "uploads" / "range_raw" / company_dir / f"{target_date}_to_{target_date}"
            if split_dir.exists():
                logging.info(f"\n=== Cleaning up single-day split directory ===")
                archive_dir = repo_root / "Uploaded" / target_date
                archive_dir.mkdir(parents=True, exist_ok=True)
                
                # Move remaining .csv files with appropriate prefixes
                remaining_files = list(split_dir.glob("*.csv"))
                for csv_file in remaining_files:
                    # Determine prefix based on filename pattern
                    if csv_file.name.startswith("BookKeeping_") and not csv_file.name.startswith("BookKeeping_raw_spill_"):
                        prefix = "RAW_SPLIT_"
                    elif csv_file.name.startswith("CombinedRaw_"):
                        prefix = "RAW_COMBINED_"
                    else:
                        prefix = "RAW_INPUT_"
                    
                    dest_name = f"{prefix}{csv_file.name}"
                    dest_path = archive_dir / dest_name
                    
                    # Don't overwrite if file already exists in archive
                    if not dest_path.exists():
                        shutil.move(str(csv_file), str(dest_path))
                        logging.info(f"Moved remaining split file: {csv_file.name} -> Uploaded/{target_date}/{dest_name}")
                    else:
                        logging.info(f"Skipped (already archived): {csv_file.name}")
                        # Remove the duplicate from split dir
                        csv_file.unlink()
                
                # Remove split_dir if now empty
                try:
                    if split_dir.exists() and not any(split_dir.iterdir()):
                        split_dir.rmdir()
                        logging.info(f"Removed empty split directory: {split_dir.name}")
                except OSError as e:
                    logging.warning(f"Could not remove split directory: {e}")
                
                # Attempt to clean up parent directories if empty
                try:
                    company_range_dir = repo_root / "uploads" / "range_raw" / company_dir
                    if company_range_dir.exists() and not any(company_range_dir.iterdir()):
                        company_range_dir.rmdir()
                        logging.info(f"Removed empty company range directory: {company_range_dir.name}")
                except OSError:
                    pass  # Directory not empty or other error
                
                try:
                    range_raw_dir = repo_root / "uploads" / "range_raw"
                    if range_raw_dir.exists() and not any(range_raw_dir.iterdir()):
                        range_raw_dir.rmdir()
                        logging.info(f"Removed empty range_raw directory")
                except OSError:
                    pass  # Directory not empty or other error
                
                logging.info("[OK] Single-day split directory cleanup complete")
            
            # Archive the original downloaded EPOS CSV from repo root
            if downloaded_csv.exists() and downloaded_csv.is_file():
                archive_dir = repo_root / "Uploaded" / target_date
                archive_dir.mkdir(parents=True, exist_ok=True)
                original_dest = archive_dir / f"ORIGINAL_{downloaded_csv.name}"
                
                if original_dest.exists():
                    # Don't overwrite - suffix with timestamp
                    timestamp = datetime.now().strftime("%H%M%S")
                    original_dest = archive_dir / f"ORIGINAL_{downloaded_csv.stem}_{timestamp}{downloaded_csv.suffix}"
                    logging.warning(f"Archive destination exists, using timestamped name: {original_dest.name}")
                
                shutil.move(str(downloaded_csv), str(original_dest))
                logging.info(f"Archived original EPOS CSV: {downloaded_csv.name} -> Uploaded/{target_date}/{original_dest.name}")
            else:
                logging.warning(f"Original downloaded CSV not found for archival: {downloaded_csv}")

            # Success notification - load metadata for summary
            metadata = None
            metadata_path = repo_root / config.metadata_file
            if metadata_path.exists():
                try:
                    with open(metadata_path, "r") as f:
                        metadata = json.load(f)
                except Exception as e:
                    logging.warning(f"Could not load metadata for notification: {e}")
            
            # Build summary with company info and metadata
            summary = {
                "target_date": target_date,
                "company_key": company_key,
                "company_name": config.display_name
            }
            if metadata:
                summary.update(metadata)
            
            # Add reconciliation results to summary
            if reconcile_result:
                summary["reconcile"] = reconcile_result
            
            # Add trading day stats if available
            if trading_day_stats and target_date in trading_day_stats.get("by_date", {}):
                summary["trading_day_stats"] = {
                    "cutoff": trading_day_stats["cutoff"],
                    "by_date": {target_date: trading_day_stats["by_date"][target_date]}
                }
            
            notify_pipeline_success(pipeline_name, log_file, date_range_str, summary, config.slack_webhook_url)
            logging.info("\nPipeline completed successfully ✅")
            return 0

    except SystemExit as e:
        logging.error("Pipeline failed", exc_info=True)
        
        # In range mode, if we fail during per-day processing, do NOT archive range files
        if is_range_mode:
            logging.warning("Range mode failed - range raw files will NOT be archived")
        
        # Try to load metadata with upload stats for better error reporting
        metadata = None
        metadata_path = repo_root / config.metadata_file
        if metadata_path.exists():
            try:
                with open(metadata_path, "r") as f:
                    metadata = json.load(f)
            except Exception:
                pass
        
        # Build summary with company info and metadata
        summary = {
            "target_date": target_date if not is_range_mode else None,
            "from_date": from_date if is_range_mode else None,
            "to_date": to_date if is_range_mode else None,
            "company_key": company_key,
            "company_name": config.display_name
        }
        if metadata:
            summary.update(metadata)
        
        notify_pipeline_failure(pipeline_name, log_file, str(e), date_range_str, config.slack_webhook_url, summary)
        return 1
    except Exception as e:
        logging.error("Pipeline failed with unexpected error", exc_info=True)
        
        # In range mode, if we fail during per-day processing, do NOT archive range files
        if is_range_mode:
            logging.warning("Range mode failed - range raw files will NOT be archived")
        
        # Try to load metadata with upload stats for better error reporting
        metadata = None
        metadata_path = repo_root / config.metadata_file
        if metadata_path.exists():
            try:
                with open(metadata_path, "r") as f:
                    metadata = json.load(f)
            except Exception:
                pass
        
        # Build summary with company info and metadata
        summary = {
            "target_date": target_date if not is_range_mode else None,
            "from_date": from_date if is_range_mode else None,
            "to_date": to_date if is_range_mode else None,
            "company_key": company_key,
            "company_name": config.display_name
        }
        if metadata:
            summary.update(metadata)
        
        notify_pipeline_failure(pipeline_name, log_file, str(e), date_range_str, config.slack_webhook_url, summary)
        return 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run EPOS -> QuickBooks pipeline for target business date or date range.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Single day (default)
  python3 run_pipeline.py --company company_a --target-date 2025-12-25
  python3 run_pipeline.py --company company_b  # Uses yesterday as target-date
  
  # Date range
  python3 run_pipeline.py --company company_b --from-date 2025-12-02 --to-date 2025-12-04
        """
    )
    parser.add_argument(
        "--company",
        required=True,
        choices=get_available_companies(),
        help="Company identifier (REQUIRED). Available: %(choices)s",
    )
    parser.add_argument(
        "--target-date",
        help="Target business date in YYYY-MM-DD format (default: yesterday, ignored if --from-date and --to-date are provided)",
    )
    parser.add_argument(
        "--from-date",
        help="Start date for range mode in YYYY-MM-DD format (must be used with --to-date)",
    )
    parser.add_argument(
        "--to-date",
        help="End date for range mode in YYYY-MM-DD format (must be used with --from-date)",
    )
    parser.add_argument(
        "--skip-download",
        action="store_true",
        help="Skip EPOS download and use existing split files in uploads/range_raw/ (range mode only)",
    )
    args = parser.parse_args()
    
    if not args.company:
        parser.error("--company is REQUIRED. Available companies: " + ", ".join(get_available_companies()))
    
    # Validation: --from-date and --to-date must be provided together
    if (args.from_date is None) != (args.to_date is None):
        parser.error("--from-date and --to-date must be provided together")
    
    # Validation: --skip-download only works in range mode
    if args.skip_download and (args.from_date is None or args.to_date is None):
        parser.error("--skip-download can only be used with --from-date and --to-date (range mode)")
    
    raise SystemExit(main(args.company, args.target_date, args.from_date, args.to_date, args.skip_download))
</file>

</files>
